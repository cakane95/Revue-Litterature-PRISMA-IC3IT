@inproceedings{dayanFeudalReinforcementLearning1992,
  title = {Feudal {{Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dayan, Peter and Hinton, Geoffrey E},
  year = 1992,
  volume = {5},
  publisher = {Morgan-Kaufmann},
  urldate = {2025-10-30},
  abstract = {One way to speed up reinforcement learning is to enable learning to  happen simultaneously at multiple resolutions in space and time.  This paper shows how to create a Q-Iearning managerial hierarchy  in which high level managers learn how to set tasks to their sub(cid:173) managers who, in turn, learn how to satisfy them.  Sub-managers  need  not initially understand  their managers' commands.  They  simply learn to maximise their reinforcement in the context of the  current command.  We illustrate the system using a simple maze task ..  As the system  learns  how to get around,  satisfying commands at the multiple  levels, it explores more efficiently than standard, flat,  Q-Iearning  and builds a more comprehensive map.},
  file = {C:\Users\Cheikhou\Zotero\storage\DVEHCJY3\Dayan and Hinton - 1992 - Feudal Reinforcement Learning.pdf}
}

@article{haddawayPRISMA2020PackageShiny2022,
  title = {{{PRISMA2020}}: {{An R}} Package and {{Shiny}} App for Producing {{PRISMA}} 2020-Compliant Flow Diagrams, with Interactivity for Optimised Digital Transparency and {{Open Synthesis}}},
  author = {Haddaway, Neal R. and Page, Matthew J. and Pritchard, Chris C. and McGuinness, Luke A.},
  year = 2022,
  month = jun,
  journal = {Campbell Systematic Reviews},
  volume = {18},
  number = {2},
  pages = {e1230},
  publisher = {John Wiley \& Sons, Ltd},
  issn = {1891-1803},
  doi = {10.1002/cl2.1230},
  urldate = {2022-05-06},
  abstract = {Abstract Background Reporting standards, such as PRISMA aim to ensure that the methods and results of systematic reviews are described in sufficient detail to allow full transparency. Flow diagrams in evidence syntheses allow the reader to rapidly understand the core procedures used in a review and examine the attrition of irrelevant records throughout the review process. Recent research suggests that use of flow diagrams in systematic reviews is poor and of low quality and called for standardised templates to facilitate better reporting in flow diagrams. The increasing options for interactivity provided by the Internet gives us an opportunity to support easy-to-use evidence synthesis tools, and here we report on the development of a tool for the production of PRISMA 2020-compliant systematic review flow diagrams. Methods and Findings We developed a free-to-use, Open Source R package and web-based Shiny app to allow users to design PRISMA flow diagrams for their own systematic reviews. Our tool allows users to produce standardised visualisations that transparently document the methods and results of a systematic review process in a variety of formats. In addition, we provide the opportunity to produce interactive, web-based flow diagrams (exported as HTML files), that allow readers to click on boxes of the diagram and navigate to further details on methods, results or data files. We provide an interactive example here; https://prisma-flowdiagram.github.io/. Conclusions We have developed a user-friendly tool for producing PRISMA 2020-compliant flow diagrams for users with coding experience and, importantly, for users without prior experience in coding by making use of Shiny (https://estech.shinyapps.io/prisma\_flowdiagram/). This free-to-use tool will make it easier to produce clear and PRISMA 2020-compliant systematic review flow diagrams. Significantly, users can also produce interactive flow diagrams for the first time, allowing readers of their reviews to smoothly and swiftly explore and navigate to further details of the methods and results of a review. We believe this tool will increase use of PRISMA flow diagrams, improve the compliance and quality of flow diagrams, and facilitate strong science communication of the methods and results of systematic reviews by making use of interactivity. We encourage the systematic review community to make use of the tool, and provide feedback to streamline and improve their usability and efficiency.}
}

@article{kaelblingPlanningActingPartially1998,
  title = {Planning and Acting in Partially Observable Stochastic Domains},
  author = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
  year = 1998,
  month = may,
  journal = {Artificial Intelligence},
  volume = {101},
  number = {1},
  pages = {99--134},
  issn = {0004-3702},
  doi = {10.1016/S0004-3702(98)00023-X},
  urldate = {2025-10-30},
  abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (mdps) and partially observable MDPs (pomdps). We then outline a novel algorithm for solving pomdps off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to pomdps, and of some possibilities for finding approximate solutions.},
  keywords = {Partially observable Markov decision processes,Planning,Uncertainty},
  file = {C\:\\Users\\Cheikhou\\Zotero\\storage\\EKFUM3MU\\Kaelbling et al. - 1998 - Planning and acting in partially observable stochastic domains.pdf;C\:\\Users\\Cheikhou\\Zotero\\storage\\U8YTK257\\S000437029800023X.html}
}

@incollection{littmanMarkovGamesFramework1994,
  title = {Markov Games as a Framework for Multi-Agent Reinforcement Learning},
  booktitle = {Machine {{Learning Proceedings}} 1994},
  author = {Littman, Michael L.},
  editor = {Cohen, William W. and Hirsh, Haym},
  year = 1994,
  month = jan,
  pages = {157--163},
  publisher = {Morgan Kaufmann},
  address = {San Francisco (CA)},
  doi = {10.1016/B978-1-55860-335-6.50027-1},
  urldate = {2025-10-30},
  abstract = {In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsis-tic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic.},
  isbn = {978-1-55860-335-6},
  file = {C:\Users\Cheikhou\Zotero\storage\KWQ6QG37\B9781558603356500271.html}
}

@misc{loweMultiAgentActorCriticMixed2020,
  title = {Multi-{{Agent Actor-Critic}} for {{Mixed Cooperative-Competitive Environments}}},
  author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
  year = 2020,
  month = mar,
  number = {arXiv:1706.02275},
  eprint = {1706.02275},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.02275},
  urldate = {2025-10-30},
  abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\Cheikhou\\Zotero\\storage\\672CB2RB\\Lowe et al. - 2020 - Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.pdf;C\:\\Users\\Cheikhou\\Zotero\\storage\\V3EBBAZP\\1706.html}
}

@article{mnihHumanlevelControlDeep2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = 2015,
  month = feb,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14236},
  urldate = {2025-10-30},
  langid = {english},
  file = {C:\Users\Cheikhou\Zotero\storage\4DFR8TEA\Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf}
}

@inproceedings{perolatMultiagentReinforcementLearning2017,
  title = {A Multi-Agent Reinforcement Learning Model of Common-Pool Resource Appropriation},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {P{\'e}rolat, Julien and Leibo, Joel Z and Zambaldi, Vinicius and Beattie, Charles and Tuyls, Karl and Graepel, Thore},
  year = 2017,
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-10-30},
  abstract = {Humanity faces numerous problems of common-pool resource appropriation. This class of multi-agent social dilemma includes the problems of ensuring sustainable use of fresh water, common fisheries, grazing pastures, and irrigation systems. Abstract models of common-pool resource appropriation based on non-cooperative game theory predict that self-interested agents will generally fail to find socially positive equilibria---a phenomenon called the tragedy of the commons. However, in reality, human societies are sometimes able to discover and implement stable cooperative solutions. Decades of behavioral game theory research have sought to uncover aspects of human behavior that make this possible. Most of that work was based on laboratory experiments where participants only make a single choice: how much to appropriate. Recognizing the importance of spatial and temporal resource dynamics, a recent trend has been toward experiments in more complex real-time video game-like environments. However, standard methods of non-cooperative game theory can no longer be used to generate predictions for this case. Here we show that deep reinforcement learning can be used instead. To that end, we study the emergent behavior of groups of independently learning agents in a partially observed Markov game modeling common-pool resource  appropriation. Our experiments highlight the importance of trial-and-error learning in common-pool resource appropriation and shed light on the relationship between exclusion, sustainability, and inequality.},
  file = {C:\Users\Cheikhou\Zotero\storage\R3SBRA7M\Pérolat et al. - 2017 - A multi-agent reinforcement learning model of common-pool resource appropriation.pdf}
}

@misc{rashidQMIXMonotonicValue2018a,
  title = {{{QMIX}}: {{Monotonic Value Function Factorisation}} for {{Deep Multi-Agent Reinforcement Learning}}},
  shorttitle = {{{QMIX}}},
  author = {Rashid, Tabish and Samvelyan, Mikayel and de Witt, Christian Schroeder and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
  year = 2018,
  month = jun,
  number = {arXiv:1803.11485},
  eprint = {1803.11485},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1803.11485},
  urldate = {2025-10-30},
  abstract = {In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning},
  file = {C\:\\Users\\Cheikhou\\Zotero\\storage\\Y9BI4TPN\\Rashid et al. - 2018 - QMIX Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning.pdf;C\:\\Users\\Cheikhou\\Zotero\\storage\\7ELVF8LC\\1803.html}
}

@article{suttonMDPsSemiMDPsFramework1999,
  title = {Between {{MDPs}} and Semi-{{MDPs}}: {{A}} Framework for Temporal Abstraction in Reinforcement Learning},
  shorttitle = {Between {{MDPs}} and Semi-{{MDPs}}},
  author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
  year = 1999,
  month = aug,
  journal = {Artificial Intelligence},
  volume = {112},
  number = {1},
  pages = {181--211},
  issn = {0004-3702},
  doi = {10.1016/S0004-3702(99)00052-1},
  urldate = {2025-10-30},
  abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options---closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem.},
  keywords = {Hierarchical planning,Intra-option learning,Macroactions,Macros,Markov decision processes,Options,Reinforcement learning,Semi-Markov decision processes,Subgoals,Temporal abstraction},
  file = {C\:\\Users\\Cheikhou\\Zotero\\storage\\MBXJYHPT\\Sutton et al. - 1999 - Between MDPs and semi-MDPs A framework for temporal abstraction in reinforcement learning.pdf;C\:\\Users\\Cheikhou\\Zotero\\storage\\NR8KQH6W\\S0004370299000521.html}
}

@book{suttonReinforcementLearningIntroduction2014,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew},
  year = 2014,
  series = {Adaptive Computation and Machine Learning},
  edition = {Nachdruck},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  isbn = {978-0-262-19398-6},
  langid = {english}
}

@misc{yuSurprisingEffectivenessPPO2022,
  title = {The {{Surprising Effectiveness}} of {{PPO}} in {{Cooperative}}, {{Multi-Agent Games}}},
  author = {Yu, Chao and Velu, Akash and Vinitsky, Eugene and Gao, Jiaxuan and Wang, Yu and Bayen, Alexandre and Wu, Yi},
  year = 2022,
  month = nov,
  number = {arXiv:2103.01955},
  eprint = {2103.01955},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.01955},
  urldate = {2025-10-30},
  abstract = {Proximal Policy Optimization (PPO) is a ubiquitous on-policy reinforcement learning algorithm but is significantly less utilized than off-policy learning algorithms in multi-agent settings. This is often due to the belief that PPO is significantly less sample efficient than off-policy methods in multi-agent systems. In this work, we carefully study the performance of PPO in cooperative multi-agent settings. We show that PPO-based multi-agent algorithms achieve surprisingly strong performance in four popular multi-agent testbeds: the particle-world environments, the StarCraft multi-agent challenge, Google Research Football, and the Hanabi challenge, with minimal hyperparameter tuning and without any domain-specific algorithmic modifications or architectures. Importantly, compared to competitive off-policy methods, PPO often achieves competitive or superior results in both final returns and sample efficiency. Finally, through ablation studies, we analyze implementation and hyperparameter factors that are critical to PPO's empirical performance, and give concrete practical suggestions regarding these factors. Our results show that when using these practices, simple PPO-based methods can be a strong baseline in cooperative multi-agent reinforcement learning. Source code is released at {\textbackslash}url\{https://github.com/marlbenchmark/on-policy\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {C\:\\Users\\Cheikhou\\Zotero\\storage\\CRHN92WZ\\Yu et al. - 2022 - The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games.pdf;C\:\\Users\\Cheikhou\\Zotero\\storage\\789KHG8C\\2103.html}
}
