\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{tabularx}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Reinforcement Learning and Agent Based Modeling for Socio-Ecological Systems Management : A Systematic Review}

\author{\IEEEauthorblockN{Cheikhou Akhmed Kane}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{Samba Diaw}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{Mandicou Ba}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{Alassane Bah}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
}

\maketitle

\begin{abstract}
The sustainable management of complex Socio-Ecological Systems (SES) requires models that can capture both decentralized human behaviors and optimal long-term policies. The coupling of Agent-Based Models (ABM) and Multi-Agent Reinforcement Learning (MARL) is a relevant paradigm, but "full MARL" approaches often face critical barriers in computational cost and scalability. This paper presents a systematic review (PRISMA) to map the state-of-the-art of this coupling, focusing on applications in environmental sustainability.

A multi-faceted query strategy—targeting core technology (Q1), thematic applications (Q2), and hierarchical architectures (Q3)—was deployed across general (e.g., Scopus, Web of Science) and specialized (e.g., IEEE Xplore, ACM) databases, filtered from 2017 to capture the modern Deep MARL era.

Our synthesis of the resulting corpus identifies key research gaps, particularly in model validation and scalability. Based on this analysis, we conclude that hierarchical architectures emerge as a highly promising research direction. These models, which can combine high-level learning agents with simpler behavioral rules, appear particularly suited to overcoming critical barriers in real-world deployments, such as scalability and effective application in data-poor environments.
\end{abstract}

\begin{IEEEkeywords}
Multi-Agent Reinforcement Learning (MARL), Agent-Based Modeling (ABM), Socio-Ecological Systems (SES)
\end{IEEEkeywords}

\section{Introduction}
The modeling and sustainable management of Socio-Ecological Systems (SES) constitute significant scientific challenges. These systems, characterized by the coexistence of biophysical dynamics and human behaviors, exhibit inherent complexity, non-linear interactions, and actor heterogeneity, making the prediction of outcomes and the design of optimal interventions particularly arduous.

In response to this challenge, the bottom-up paradigm is frequently employed over traditional top-down models. Within this paradigm, Agent-Based Modeling (ABM) has become a prevalent simulation tool. While conceptually close to Multi-Agent Systems (MAS), a broader field within computer science, ABMs are specifically valued in social and ecological sciences for their capacity to simulate decentralized interactions and observe the emergence of collective behaviors.

However, traditional ABMs often face limitations in modeling complex and adaptive human decision-making under uncertainty. It is in this context that Reinforcement Learning (RL), and more specifically Multi-Agent Reinforcement Learning (MARL), offers a relevant methodological framework. MARL provides the formal tools for agents to learn decision-making policies through \textit{trial-and-error} interaction with dynamic environments.

This coupling of ABM and MARL introduces new possibilities for managing major environmental issues, such as common-pool resource management or achieving carbon neutrality. However, this fusion also creates new challenges. A "full MARL" approach, where every entity is a complex learner, can lead to a significant increase in computational complexity, state-action space dimensionality, and training time. This may render such models computationally prohibitive for real-world application, especially in data-poor environments.

A potential pathway to mitigate this complexity lies in hybrid or hierarchical control architectures, where a limited number of high-level 'learning' (RL) agents interact with simpler, behavior-driven agents. Yet, an initial survey of the literature suggests this avenue, which is important for scalability and practical applicability, remains relatively unexplored.

\textbf{This paper is organized as follows: Section II presents our motivation and objectives, positioning this review within the context of a doctoral thesis. Section III details the systematic review methodology, including the PICO framework and PRISMA protocol used for our search. Section IV analyzes the selected corpus of papers under the "Related Works" heading. Based on this analysis, Section V discusses promising future research directions. Finally, Section VI concludes the paper.}

\section{Motivation \& Objectives}

Since the mid-20th century, human activities exacerbated by climate change have driven a steady decline of the Sahel’s fragile ecosystems. Under rising pressure on natural resources, pastoral and agro-pastoral communities have adopted new practices to mitigate underproduction risk. Yet these coping strategies have themselves contributed to slow but persistent damage: overgrazing, deforestation, soil erosion, and disruptions to the water and carbon cycles.

Environmental and resource management in this region is inherently complex. Agro-sylvo-pastoral systems (ASPS) in the Sudano-Sahelian zone, where crops, trees, livestock, and people coexist, exemplify this complexity. Their nonlinear dynamics emerge from countless individual decisions interacting with shifting climatic and socioeconomic conditions.

Climate change has only amplified these challenges and inspired large-scale efforts like the Great Green Wall. Yet progress has stalled, largely because:
\begin{itemize}
    \item Genuine community consultation was often missing, leading to resistance and conflict.
    \item Most interventions remain generic and neglect to assess impacts at the territorial scale.
    \item Crucially, formal models capable of forecasting the interactions between ecological dynamics and actor decisions under evolving conditions are largely absent.
\end{itemize}

Addressing these gaps requires an integrated assessment approach. As established in our introduction, the coupling of Agent-Based Modeling (ABM) and Multi-Agent Reinforcement Learning (MARL) is a relevant paradigm to tackle such challenges. However, the methodological landscape for this coupling is nascent and fraught with its own complexities, particularly concerning computational cost, scalability, and the validation of "full MARL" approaches.

Therefore, before a specific model for the Sahel can be designed, a systematic assessment of the existing literature is required. The primary objectives of \textit{this paper} are:
\begin{enumerate}
    \item To systematically identify and map the current state-of-the-art in coupling ABM and MARL for Socio-Ecological Systems.
    \item To critically analyze the \textit{architectures} of these models, with a specific focus on identifying hybrid or hierarchical structures that mitigate complexity.
    \item To identify the key research gaps, particularly regarding model scalability and application in data-poor contexts.
\end{enumerate}

This review serves as the foundational justification for a doctoral thesis, ``An adaptive agent-based optimization model for carbon neutral pathways in agro-sylvo-pastoral systems in Senegal.'' The insights gathered from this analysis will directly inform the design of the novel hierarchical simulation platform proposed in that dissertation, which aims to provide a robust, adaptive decision-support tool for a low-carbon transition in the Sahel.

\section{Background : Sequential Decision-Making}

In this section, we prodide the relevant background on sequential decison-making.

\subsection{Markov Decision Processes (MDPs)}

At the heart of managing complex systems, such as farming in the Sahel, lies the problem of \textbf{sequential decision-making}. During the growing seasons, a farmer must decide when to plant, what to plant, which fertilizers to use, and when to harvest in order to achieve a long-term production target. In such a setting, agents learn to make decisions by interacting with their environment through trial-and-error. This example brings to light two core challenges of sequential decision-making: \textbf{delayed feedback} (the consequences of an action are seen much later) and the \textbf{combinatorial explosion} of choices for the same goal.

The mathematical framework designed to represent this problem is the \textbf{Markov Decision Process (MDP)}. It is founded on the \textbf{Markov Property}, which informally stipulates that, given the present state, the future is independent of the past history of states and actions.

More formally, an MDP is defined as a quintuple $\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$ \cite{suttonReinforcementLearningIntroduction2014}, where:

\begin{itemize}
    \item $\mathcal{S}$ is the set of possible \textbf{states},information about all aspects
of the past agent–environment interaction that makes a difference for the future of the system.
    \item $\mathcal{A}$ is the set of possible \textbf{actions} that the agent can take.
    \item $\mathcal{P}(s'|s, a)$ is the \textbf{transition function}: the probability of transitioning from state $s$ to state $s'$ after executing action $a$.
    \item $\mathcal{R}(s, a)$ is the \textbf{reward function}: the immediate scalar reward received after executing action $a$ in state $s$.
    \item $\gamma \in [0, 1]$ is the \textbf{discount factor}, which balances the importance of immediate versus future rewards.
\end{itemize}

Solving an MDP involves finding an optimal \textbf{policy} $\pi^{*}(a|s)$, which is a mapping from state to action that provides the agent with a strategy to maximize its \textbf{expected long-term return} (the discounted cumulative reward).

The fundamental questions of "How do we compare policies?" and "How do we find an optimal policy?" are addressed by the concept of the \textbf{value function}, which is formalized by the \textbf{Bellman Equations} \cite{suttonReinforcementLearningIntroduction2014}. These recursive equations relate the value of a state (or a state-action pair) to the expected value of its successor states, providing the basis for iterative solution methods.

\subsubsection{Bellman Equations}

\textbf{State-Value Function ($V^{\pi}$):}
$$V^{\pi}(s) = \mathbb{E}_{\pi} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s \right]$$

\textbf{Action-Value Function ($Q^{\pi}$):}
$$Q^{\pi}(s, a) = \mathcal{R}(s, a) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}(s'|s, a) \sum_{a' \in \mathcal{A}} \pi(a'|s') Q^{\pi}(s', a')$$

\noindent The optimal policy $\pi^{*}$ is found by maximizing the action-value function: $\pi^{*}(s) = \arg \max_{a \in \mathcal{A}} Q^{*}(s, a)$.

\vspace{0.5em}
\noindent As seen, the MDP provides a robust framework to represent the sequential decision-making problem. However, its core assumption of \textbf{full observability} of the system is often violated in real-world scenarios, such as our small-scale farmer example, where crucial information is hidden. This limitation leads to the generalization addressed by \textbf{Partially Observable Markov Decision Processes (POMDPs)}.

\subsection{Partially Observable Markov Decision Processes (POMDPs) and Reinforcement Learning}

The MDP framework's core assumption of full observability is often violated in practice. In many real-world complex systems—such as autonomous vehicles with limited sensor range or smart grids with incomplete metering—agents receive only a noisy and partial view of the environment. This scenario requires the generalization of the MDP to the \textbf{Partially Observable Markov Decision Process (POMDP)}.

The POMDP extends the MDP tuple by adding an observation set $\Omega$ and an observation function $\mathcal{O}$: $\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \Omega, \mathcal{O}, \gamma \rangle$. Since the agent cannot condition its actions on the true state $S \in \mathcal{S}$, it must maintain a \textbf{belief state} $b(s)$. This belief is a probability distribution over the set of real states, representing the agent's uncertainty based on its entire action-observation history. Solving a POMDP involves finding a policy over this continuous belief space \cite{kaelblingPlanningActingPartially1998}.

\subsubsection{Reinforcement Learning (RL)}

Both MDPs and POMDPs can be solved using \textbf{Reinforcement Learning (RL)}. RL is a computational paradigm where an agent learns an optimal policy $\pi$ by maximizing its long-term expected reward through direct interaction (\textit{trial-and-error}) with the environment.

RL algorithms are fundamentally based on the concept of \textbf{Temporal Difference (TD) Learning}, which involves updating the estimate of a value function toward a target value that includes a primary reward ($R_{t+1}$) and the value of the succeeding state ($V(S_{t+1})$). This bootstrapping approach allows learning without waiting for the final outcome.

Key families of RL algorithms include:
\begin{itemize}
    \item \textbf{Value-Based Methods:} These methods focus on estimating the optimal action-value function, $Q^*(s, a)$. Examples include **Q-Learning** (off-policy) and **SARSA** (on-policy). When combined with function approximators like neural networks (leading to **Deep Q-Networks (DQN)** \cite{mnihHumanlevelControlDeep2015}), these methods can tackle large state spaces.
    \item \textbf{Policy Gradient and Actor-Critic Methods:} These methods directly learn the policy $\pi$ (the \textbf{Actor}) and simultaneously learn a value function (the \textbf{Critic}) to estimate the advantage of actions, guiding the policy update. Examples include **Proximal Policy Optimization (PPO)** and **Soft Actor-Critic (SAC)**, which are known for stability and performance in continuous control tasks.
\end{itemize}

\subsection{Multi-Agent Reinforcement Learning (MARL): Core Challenges and Paradigms}

The increasing prevalence of complex systems—such as robotic swarms, large-scale traffic networks, and distributed smart grids—necessitates extending the RL framework to environments featuring multiple interacting decision-makers. \textbf{Multi-Agent Reinforcement Learning (MARL)} formalizes the challenge of concurrent decision-making, where a set of agents $\mathcal{N} = \{1, \ldots, N\}$ interacts with a shared environment.

This problem is typically formalized as a \textbf{Stochastic Game} (also known as a \textbf{Markov Game}) \cite{littmanMarkovGamesFramework1994}.

\subsubsection{The Challenges: Non-Stationarity and Credit Assignment}

The principal challenge in MARL stems from \textbf{environmental non-stationarity} \cite{perolatMultiagentReinforcementLearning2017}. From the perspective of any individual agent $i$, the environment is comprised of the physical world \textit{and} the collective policies of all other learning agents $\pi_{-i}$. Since these policies are constantly being updated, the environment perceived by agent $i$ is non-stationary, violating the fundamental Markovian assumption required for standard single-agent RL convergence.

A secondary but crucial challenge in cooperative MARL is the \textbf{credit assignment problem}: determining how the scalar joint reward $R_{\text{total}}$ should be attributed to the individual actions of each agent.

\subsubsection{The CTDE Paradigm and Key Frameworks}

The dominant \textbf{framework} addressing both non-stationarity and credit assignment in cooperative MARL is the \textbf{Centralized Training, Decentralized Execution (CTDE) paradigm} \cite{loweMultiAgentActorCriticMixed2020}.

The CTDE framework leverages the following structure:
\begin{itemize}
    \item \textbf{Centralized Training:} The critic leverages the complete system information (joint state $\mathbf{s}$, joint action $\mathbf{a}$) to stabilize learning and mitigate non-stationarity.
    \item \textbf{Decentralized Execution:} Each agent's actor relies solely on its local observation $o_i$ to select actions $a_i$ during deployment, ensuring scalability and autonomy.
\end{itemize}

Key MARL frameworks built upon the CTDE paradigm include:
\begin{itemize}
    \item \textbf{QMIX} (Q-Learning extensions): This framework \cite{rashidQMIXMonotonicValue2018a} addresses credit assignment by approximating the global joint action-value function $Q_{\text{tot}}$ as a monotonic combination of individual action-value functions $Q_i$. This ensures that maximizing $Q_{\text{tot}}$ is equivalent to greedily maximizing each $Q_i$, simplifying coordination.
    \item \textbf{Multi-Agent PPO (MAPPO)}: A highly effective framework \cite{yuSurprisingEffectivenessPPO2022} that extends the robust single-agent PPO algorithm to the MARL setting, typically utilizing a centralized value function (Critic) to guide decentralized policy updates (Actors) in a CTDE manner.
\end{itemize}

The success of these frameworks has shifted the focus toward more complex issues, leading to the development of sophisticated architectural extensions like \textbf{Hierarchical Reinforcement Learning (HRL)} and \textbf{Hybrid Frameworks} (discussed next).

\subsubsection{Architectural Extensions: Hierarchical RL for Sparse Rewards}

While CTDE frameworks effectively manage non-stationarity and instantaneous credit assignment, they often struggle with \textbf{sparse and temporally extended rewards}. In real-world resource management problems, such as annual agricultural planning or long-cycle infrastructure maintenance, the agent receives a significant reward only after many thousands of time steps (e.g., the final harvest or the successful completion of a complex mission). This long horizon hinders effective learning, as attributing the final reward to initial actions becomes extremely difficult.

\textbf{Hierarchical Reinforcement Learning (HRL)} is the most suitable framework to address this long-horizon challenge, as it decomposes the complex, single task into a sequence of smaller, manageable sub-tasks.

\paragraph{The Options Framework}

The formal mathematical foundation for task decomposition is the \textbf{Options Framework} \cite{suttonMDPsSemiMDPsFramework1999}. An option is a temporally extended course of action, allowing the high-level policy (the Manager) to operate in a Semi-MDP (SMDP) space—making decisions over longer time intervals—while the low-level policy (the Worker) executes primitive actions. An option $o$ is formally defined by the triplet $\langle I, \pi, \beta \rangle$:
\begin{itemize}
    \item $I \subseteq \mathcal{S}$ : The set of states where the option can be \textbf{initiated}.
    \item $\pi : \mathcal{S} \times \mathcal{A} \to [0, 1]$ : The option's internal \textbf{policy}, dictating primitive actions until termination.
    \item $\beta : \mathcal{S} \to [0, 1]$ : The \textbf{termination} function, the probability that the option ceases execution in a given state.
\end{itemize}

\paragraph{Feudal Reinforcement Learning (FRL)}

\textbf{Feudal Reinforcement Learning (FRL)} \cite{dayanFeudalReinforcementLearning1992} is a foundational and influential HRL paradigm specifically designed to leverage this hierarchy for goal-seeking behavior. It employs a two-level control structure:
\begin{itemize}
    \item \textbf{The Manager (High-Level):} Operates on a slow timescale, selecting abstract goals (\textit{subgoals}) or directions for the Worker. Its reward is the \textit{global, sparse reward} (e.g., maximizing the final output of the system).
    \item \textbf{The Worker (Low-Level):} Operates on a fast timescale, executing primitive actions to achieve the Manager's assigned subgoal. Its reward is \textit{intrinsic} and immediate: based on its success in reaching the subgoal.
\end{itemize}
This decoupling generates dense, local reward signals for the Worker, effectively solving the temporal credit assignment problem and facilitating the discovery of long-term strategic behaviors necessary for complex resource planning. This section provided the necessary technical foundation to analyze the collected literature, leading us to our systematic review methodology.

\section{Methodology}

This review follows the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines. Our methodology is structured into three phases: (1) a systematic search strategy to build the initial corpus, (2) a rigorous selection process using the PICO framework to filter the corpus, and (3) a structured data extraction grid to analyze the final set of papers.

\subsection{Search Strategy and Sources}

A multi-faceted search strategy was designed to capture the interdisciplinary nature of the topic, targeting both fundamental computer science literature and domain-specific applications. The strategy relies on three query streams executed across Scopus, Web of Science, IEEE Xplore, ACM Digital Library, ArXiv, and HAL.

\begin{itemize}
    \item \textbf{Core Query (Q1):} Targets the direct intersection of the two technologies. \\
    \textit{(("multi-agent reinforcement learning" OR "MARL") AND ("agent-based model" OR "ABM" OR "multi-agent system"))}

    \item \textbf{Thematic Query (Q2):} Refines Q1 with socio-ecological application terms to be run on generalist databases. \\
    \textit{(Q1) AND ("socio-ecological system" OR "environmental management" OR "natural resource management" OR "sustainability" OR "common-pool resource" OR "carbon" OR "land use" OR "agriculture" OR "forestry")}

    \item \textbf{Architectural Query (Q3):} Identifies works on hierarchical control architectures, run on specialist databases. \\
    \textit{(("hierarchical reinforcement learning" OR "governing agent" OR "macro-micro" OR "CTDE" OR "centralized training decentralized execution") AND ("agent-based model" OR "multi-agent system"))}
\end{itemize}

The following filters were applied to all searches:
\begin{itemize}
    \item \textbf{Time Filter:} 2017--Present. This date marks the publication of key Deep MARL algorithms (e.g., MADDPG) and the start of the modern era of the field.
    \item \textbf{Language Filter:} English and French.
    \item \textbf{Document Type Filter:} Conference Papers, Proceedings, Journal Articles, and Reviews.
\end{itemize}

Finally, a \textit{snowballing} process (forward and backward) was performed on key identified papers to ensure corpus completeness.

\subsection{Selection Criteria: The PICO Framework}

To filter the initial search pool, we employed the PICO framework (Population, Intervention, Comparison, Outcome) to define precise inclusion and exclusion criteria. Table \ref{tab:pico} details these criteria. An article was required to meet all inclusion criteria to be retained for final analysis.

\begin{table*}[htbp]
\caption{PICO Framework for Study Selection}
\begin{center}
\begin{tabular}{|l|p{7.5cm}|p{7.5cm}|}
\hline
\textbf{Component} & \textbf{Inclusion Criteria (Must Include)} & \textbf{Exclusion Criteria (Must Not Include)} \\
\hline
\textbf{P}opulation & 
Studies modeling a Multi-Agent System (MAS) or Agent-Based Model (ABM). \newline
Environment represents a complex system (e.g., socio-ecological, socio-technical, resource management).
& 
Single-agent systems. \newline
Non-agent-based RL (e.g., pure NLP, image classification). \newline
Game applications (e.g., Chess, Go, Atari) without an explicit link to resource management or social dilemmas.
\\
\hline
\textbf{I}ntervention & 
Use of a Reinforcement Learning (RL) algorithm to train one or more agents. \newline
Approach is multi-agent (MARL) or hierarchical (e.g., high-level RL agent guiding low-level agents).
& 
Models with fixed, non-learning agent rules. \newline
Exclusive use of other AI techniques (e.g., genetic algorithms, expert systems) without RL.
\\
\hline
\textbf{C}omparison & 
Includes quantitative comparison against at least one baseline. \newline
Baseline is a non-learning model (heuristic), a simpler RL model (e.g., single-agent RL), or another MARL algorithm.
& 
Purely descriptive or theoretical studies without experimental comparison. \newline
Models evaluated in isolation without a baseline.
\\
\hline
\textbf{O}utcome & 
Reports quantitative performance metrics. \newline
Metrics relate to system-level goals (e.g., resource yield, sustainability, economic cost) OR algorithmic performance (e.g., cumulative reward, convergence speed).
& 
Studies with purely qualitative or anecdotal results.
\\
\hline
\end{tabular}
\label{tab:pico}
\end{center}
\end{table*}

\subsection{Data Extraction and Analysis Grid}

After filtering, the final set of papers was analyzed using a structured data extraction grid. This process allowed for a systematic synthesis and comparison of the literature, and the identification of research gaps. The following key categories, detailed in Table \ref{tab:grid}, were extracted from each paper.

\begin{table}[htbp]
\caption{Data Extraction Grid Categories}
\begin{center}
\begin{tabularx}{\columnwidth}{|l|X|}
\hline
\textbf{Category} & \textbf{Key Elements Extracted} \\
\hline
\textbf{1. Model Architecture} & 
Problem formalization (e.g., MDP, POMDP) \newline
RL/MARL Algorithm (e.g., QMIX, MAPPO) \newline
Training paradigm (e.g., CTDE, DTDE) \newline
Agent design (e.g., BDI, hybrid)
\\
\hline
\textbf{2. Experimental Framework} & 
Application domain (e.g., Agriculture, Energy) \newline
Simulation platform (e.g., GAMA, NetLogo) \newline
Nature of data (synthetic vs. real) \newline
Scale and complexity (e.g., number of agents)
\\
\hline
\textbf{3. Performance Metrics} & 
Reward function definition (individual, global) \newline
Algorithmic metrics (e.g., convergence) \newline
System-level metrics (e.g., sustainability)
\\
\hline
\textbf{4. Applicability} & 
Computational cost (training time, resources) \newline
Scalability analysis \newline
Robustness / Generalization \newline
Safety / Equity considerations
\\
\hline
\end{tabularx}
\label{tab:grid}
\end{center}
\end{table}

\textbf{The results of this analysis are presented in Section IV.}

\section{Related Works: Analysis of Selected Studies}

% --- Diagramme de flux PRISMA ---
\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{prisma_flow.png}}
\caption{PRISMA flow diagram of the study selection process. Generated using the tool described in \cite{haddawayPRISMA2020PackageShiny2022}.}
\label{fig:prisma}
\end{figure}
% -----------------------------------------------------------------

Following the application of our PRISMA protocol and PICO selection criteria, a final corpus of [N] articles was retained for analysis (see Fig.~\ref{fig:prisma}). This section presents the results of this systematic review. 

We analyze this corpus according to the four main categories defined in our Data Extraction Grid (Table \ref{tab:grid}). Our analysis focuses on: (1) the model architectures employed, (2) the experimental frameworks and application domains, (3) the performance metrics used, and (4) the reported applicability and scalability of the proposed solutions. This structured analysis allows us to synthesize the current state-of-the-art and, most importantly, to identify the critical gaps that motivate our research.

\subsection{Analysis of Application Domains and Frameworks}
% Dans cette sous-section, vous analyserez les résultats de la Catégorie 2 de votre Grille d'Analyse (Tableau 2).
% Par exemple : "Our review shows that the primary application domain for ABM-MARL coupling is traffic simulation (N articles) and energy markets (N articles). 
% Applications to socio-ecological systems, particularly in agriculture or forestry, are notably rare (N articles)..."
% Vous parlerez aussi des plateformes (GAMA, NetLogo...) et de l'usage de données synthétiques vs. réelles.
In this subsection, we 

\subsection{Analysis of Model Architectures}
% C'est le cœur de votre analyse, basé sur la Catégorie 1.
% Par exemple : "The dominant MARL paradigm observed is Centralized Training with Decentralized Execution (CTDE), appearing in N\% of studies. 
% The most common algorithms are QMIX (N articles) and MADDPG (N articles)..."
% C'est ici que vous placez votre découverte clé :
% "...However, we found that hierarchical or hybrid architectures are critically under-explored, with only [X] studies 
% explicitly detailing such a design..."
In this subsection, we 

\subsection{Analysis of Performance and Applicability}
% C'est ici que vous analysez les Catégories 3 et 4.
% Par exemple : "A significant finding is that N\% of papers measure performance via algorithmic metrics (e.g., cumulative reward) 
% rather than system-level metrics (e.g., resource sustainability)..."
% Et votre deuxième lacune :
% "...Furthermore, we identified a critical gap regarding applicability: less than N\% of papers provide any data on computational cost 
% or training time, and formal scalability analysis (i.e., performance with a growing number of agents) is largely absent..."
In this subsection, we 

\section{Research Direction}

\textbf{RAJOUTER l'image de l'Architecture Fonctionnelle}

Our analysis of the recent literature reveals several structural trends. The strategic positioning of our doctoral research is articulated around these dynamics, which justify our focus on a hierarchical, constraint-based model.

First, the dominant paradigm for coordination in MARL is Centralized Training for Decentralized Execution (CTDE). This approach offers a pragmatic compromise: during the training phase, a centralized module leverages global information (observations and actions of all agents) to overcome non-stationarity and facilitate coordinated learning. During execution, each agent acts autonomously based solely on its local observations, which is a realistic constraint for distributed systems. The macro-decision-maker (RL) / micro-agent (ABM) architecture we propose can be interpreted as a sophisticated, structured instance of this CTDE paradigm.

Second, we observe a significant "Domain Maturity Gap" in MARL applications. Domains rooted in engineering, such as telecommunication networks, swarm robotics, or urban traffic control, are relatively advanced, with established benchmarks. In contrast, applications to complex Socio-Ecological Systems (SES)---particularly in agriculture, forestry, and land-use management---are more nascent and less standardized. This gap in applying advanced MARL to socio-ecological challenges represents a major research opportunity, which this work aims to address.

Furthermore, this domain gap exists partly because the scale and complexity of SES expose the limits of monolithic MARL architectures, where every entity is a complex learner. In response, hierarchical and hybrid models are gaining traction. These approaches combine one or more high-level (strategic) learning agents with low-level agents whose behavior is simpler (e.g., rule-based, heuristics, or cognitive architectures like BDI). This research direction is highly active as it directly addresses the need to reduce learning complexity and state-space dimensionality while maintaining behavioral plausibility.

Finally, we note a growing trend away from simple scalar reward maximization and towards constrained optimization. The "Safe RL" community is developing formal tools, such as Lagrangian optimization, to ensure agents avoid dangerous states. These concepts are directly translatable to sustainability challenges, where "avoiding a dangerous state" can be defined as "not depleting a critical resource" or "not exceeding a carbon emissions threshold." Similarly, questions of equity in the distribution of costs and benefits are becoming distinct optimization objectives.

These identified trends---the prevalence of CTDE, the nascent state of SES applications, the necessity of hierarchical models to manage complexity, and the move towards constrained optimization---form the central justification for the model proposed in our doctoral research. Our work is positioned at the intersection of these four active research frontiers.

\section{Conclusion}

\textbf{Rappeler la problematique et les obecjtifs de l'article}

\textbf{Quel a été la contribution de cet article?}

\textbf{Les limites et les travaux futures.}

In this paper, we conducted a systematic review of the literature at the intersection of Agent-Based Modeling and Multi-Agent Reinforcement Learning, guided by the PRISMA protocol. Our analysis revealed that while the field is active, its application to complex Socio-Ecological Systems remains nascent. We identified critical gaps in the dominant "full MARL" paradigm, namely a lack of scalability and a significant under-reporting of computational costs, which hinder practical application. In response, our analysis concludes that hierarchical and hybrid architectures, which combine high-level learning agents with simpler behavioral models, represent the most promising and scalable research direction. This approach, particularly when integrated with "Safe RL" concepts to enforce sustainability constraints, offers a viable pathway to developing the robust decision-support tools needed for real-world environmental management.

\section*{Acknowledgment}

The preferred spelling of the word ``acknowledgment'' in America is without 
an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
acknowledgments in the unnumbered footnote on the first page.


% Charge le guide de style IEEE (comment formater les refs)
\bibliographystyle{IEEEtran}

% Appelle votre base de données de citations
\bibliography{references}


\end{document}
