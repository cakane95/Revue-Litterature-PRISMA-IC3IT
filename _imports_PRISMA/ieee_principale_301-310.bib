@ARTICLE{10433682,
  author={Li, Ziqi and Zhang, Heli and Li, Xi and Ji, Hong and Leung, Victor C.M.},
  journal={IEEE Transactions on Vehicular Technology}, 
  title={Distributed Task Scheduling for MEC-Assisted Virtual Reality: A Fully-Cooperative Multiagent Perspective}, 
  year={2024},
  volume={73},
  number={7},
  pages={10572-10586},
  abstract={In the upcoming 6th generation of mobile networks, wireless virtual reality will become an essential application. Due to the heterogeneity of rendering requirements and edge servers, how to provide adaptable task scheduling capabilities in edge networks becomes an important issue. However, most related studies regard edge servers as self-interested nodes that make scheduling decisions independently, ignoring the potential power of cooperation among edge servers. In this article, we propose a fully-cooperative task scheduling scheme for virtual reality, taking into account parallel multitasking, the randomness of task arrival and leaving, as well as network load balance. By sharing a common objective among distributed schedulers, tasks can be offloaded to proper edge servers in a cooperative way. We first formulate an optimization problem to maximize the number of delay-satisfied tasks in the entire edge network. Next, we treat the edge network as a fully-cooperative multi-agent system and transform the problem into a decentralized partially observed Markov decision process (Dec-POMDP). Finally, we propose a cooperative task scheduling scheme based on multi-agent proximal policy optimization to solve the Dec-POMDP. Simulation results show that the cooperation among edge nodes brings the network improvement in terms of offloading success rate, delay compliance rate, and load balance.},
  keywords={Servers;Peer-to-peer computing;Rendering (computer graphics);Processor scheduling;Wireless communication;Optimization;6G mobile communication;Multiaccess communication;Multi-agent systems;6G;virtual reality;multi-access edge computing;distributed collaboration;multi-agent PPO},
  doi={10.1109/TVT.2024.3365476},
  ISSN={1939-9359},
  month={July},}@ARTICLE{10896652,
  author={Shyaa, Methaq A. and Ibrahim, Noor Farizah and Zainol, Zurinahni Binti and Abdullah, Rosni and Anbar, Mohammed},
  journal={IEEE Access}, 
  title={Reinforcement Learning-Based Voting for Feature Drift-Aware Intrusion Detection: An Incremental Learning Framework}, 
  year={2025},
  volume={13},
  number={},
  pages={37872-37903},
  abstract={In Intrusion Detection Systems (IDS), stream data classification faces significant challenges due to concept drifts and feature evolution, where traditional methods struggle to maintain accuracy over time. One critical challenge is feature drift, which refers to changes in the relevance of features over time, directly impacting the model’s classification accuracy. This paper introduces the Incremental Feature Drift-Aware Genetic Programming Combiner (IFDA-GPC), which integrates a Voting Enhanced Deep Q-Network Multi-Agent Feature Selection (VE-DQN-MAFS) mechanism to address these challenges. The framework extends the existing IGPC architecture by incorporating dynamic feature selection and employing a multi-agent system with voting-based aggregation. This approach enhances feature selection decisions, especially in cases where agents provide conflicting assessments of feature relevance. By reconciling these variations, the framework ensures consistency and reliability in real-time classification tasks. The framework was evaluated using benchmark datasets, including KDD Cup ’99, CICIDS-2017, HIKARI-2021, and ISCX2012, under both evolving and non-evolving scenarios. Results demonstrate that GPC-KOS-DFS, derived from IFDA-GPC, significantly outperformed existing methods in accuracy, F1-score, recall, and AUC metrics. Notably, it achieved an accuracy of 93% on the CICIDS-2017 dataset, showcasing its effectiveness in handling feature drifts while maintaining high classification performance. These findings establish IFDA-GPC as a robust solution for managing evolving data streams in intrusion detection systems.},
  keywords={Feature extraction;Heuristic algorithms;Accuracy;Intrusion detection;Adaptation models;Faces;Genetic programming;Classification algorithms;Tuning;Sensitivity;Reinforcement learning;feature drift;concept drift;stream data classification;intrusion detection;dynamic feature selection},
  doi={10.1109/ACCESS.2025.3544221},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9092353,
  author={Ramaswamy, Arunselvan},
  booktitle={2020 28th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)}, 
  title={DSPG: Decentralized Simultaneous Perturbations Gradient Descent Scheme}, 
  year={2020},
  volume={},
  number={},
  pages={54-62},
  abstract={Distributed descent-based methods are an essential toolset to solving optimization problems in multi-agent system scenarios. Here the agents seek to optimize a global objective function through mutual cooperation. Oftentimes, cooperation is achieved over a wireless communication network that is prone to delays and errors. There are many scenarios wherein the objective function is either non-differentiable or merely observable. In this paper, we present a cross-entropy based distributed stochastic approximation algorithm (SA) that finds a minimum of the objective, using only samples. We call this algorithm Decentralized Simultaneous Perturbation Stochastic Gradient, with Constant Sensitivity Parameters (DSPG). This algorithm is a two fold improvement over the classic Simultaneous Perturbation Stochastic Approximations (SPSA) algorithm. Specifically, DSPG allows for (i) the use of old information from other agents and (ii) easy implementation through the use simple hyper-parameter choices. We analyze the biases and variances that arise due to these two allowances. We show that the biases due to communication delays can be countered by a careful choice of algorithm hyper-parameters. The variance of the gradient estimator and its effect on the rate of convergence is studied. We present numerical results supporting our theory. Finally, we discuss an application to the stochastic consensus problem.},
  keywords={Sensitivity;Approximation algorithms;Delays;Linear programming;Convergence;Gradient methods;distributed optimization;distributed approximate-gradient method;cross-entropy based gradient approximation;optimization over networks with unbounded delays},
  doi={10.1109/PDP50117.2020.00015},
  ISSN={2377-5750},
  month={March},}@INPROCEEDINGS{8999048,
  author={Erduran, Ömer Ibrahim and Minor, Mirjam and Hedrich, Lars and Tarraf, Ahmad and Ruehl, Frederik and Schroth, Hans},
  booktitle={2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA)}, 
  title={Multi-agent Learning for Energy-Aware Placement of Autonomous Vehicles}, 
  year={2019},
  volume={},
  number={},
  pages={1671-1678},
  abstract={Mobility gets an increasing amount of meaning and significance in the modern society. In this paper, we introduce a multi-agent learning application for a multi-agent system in e-mobility. In particular, we propose a geospatial model for free-floating and autonomously driving e-trikes and demonstrate a calculation method of positioning e-trikes on a given area by using different methods of cluster analysis. The solution of the cluster analysis contains cluster centers which represent a positioning for the e-trikes. The solution is then evaluateded by a simulation model with more sophisticated parameters. This research field opens different opportunities of application scenarios, which are discussed in the conclusion.},
  keywords={Batteries;Geospatial analysis;Geology;Surveillance;Machine learning;Autonomous vehicles;Multi-agent systems;Machine learning;multi-agent systems;multi-agent learning;Unmanned autonomous vehicle;Clustering methods},
  doi={10.1109/ICMLA.2019.00273},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10934696,
  author={Chen, Boyang and Ding, Maomao and Yan, Jiamei and Cheng, Shiyao and Cai, Zhongwei},
  booktitle={2025 International Conference on Intelligent Systems and Computational Networks (ICISCN)}, 
  title={Decision Optimization Method for Intelligent Electricity Market Assistance based on Deep Reinforcement Learning Algorithm}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={The design of electricity market mechanisms is of great significance for responding to the "dual carbon" goals and the requirements of building new power systems. This article proposes an intelligent power market decision optimization method based on deep reinforcement learning algorithm. Firstly, this article uses Deep Q-Network (DQN) to model the complex environment in the electricity market. Furthermore, this article enhances the exploration capability of the algorithm by introducing the policy gradient method. Then, this article combines Long ShortTerm Memory (LSTM) networks to predict electricity loads and prices. Finally, this article utilizes a multi-agent reinforcement learning framework for collaborative optimization among multiple agents. The experimental results show that with a renewable energy penetration rate of 80%, the scheduling efficiency of deep reinforcement learning methods has increased by 29.4% compared to traditional methods, the system operating cost has been reduced by 16%, and the average utilization rate of renewable energy has reached 93%. In the above data conclusions, the research strategy can effectively cope with the complex and ever-changing electricity market environment, and improve the intelligence level of auxiliary decision-making.},
  keywords={Renewable energy sources;Processor scheduling;Power system stability;Electricity supply industry;Deep reinforcement learning;Prediction algorithms;Scheduling;Power markets;Long short term memory;Load modeling;intelligent power dispatching;deep reinforcement learning algorithm;electricity market;DQN algorithm;decision optimization},
  doi={10.1109/ICISCN64258.2025.10934696},
  ISSN={},
  month={Jan},}@ARTICLE{9205569,
  author={Kaur, Amandeep and Kumar, Krishan},
  journal={IEEE Transactions on Mobile Computing}, 
  title={Imperfect CSI Based Intelligent Dynamic Spectrum Management Using Cooperative Reinforcement Learning Framework in Cognitive Radio Networks}, 
  year={2022},
  volume={21},
  number={5},
  pages={1672-1683},
  abstract={The rapid development of wireless traffic pushed the wireless community to research different solutions towards the efficient utilization of the available radio spectrum. However, a recent study shows that most of the dynamically allocated spectrum bands (radio frequency resources), experience significant underutilization as cognitive radio (CR) technology still lacks intelligence. An intelligence in CRs can be incorporated with machine learning algorithms. Further, the perfect channel state information (CSI) is hardly obtained and CSI imperfections play a crucial role in Dynamic Spectrum Management. Thus, for efficient utilization of available spectrum, a decentralized Multi-Agent Reinforcement Learning based resource allocation scheme has been proposed. A robust resource allocation scheme is proposed which integrates machine learning and CR technology into a sophisticated multi-agent system (MAS). Moreover, assisted with cloud computing which provides a huge amount of storage space, reduces operating expenditures, and provides wider flexibility of cooperation. Hence, to foster the performance of the proposed scheme, a cooperative framework in MAS is introduced which enhances the performance of the proposed scheme in terms of network capacity, outage probability, and convergence speed. Numerical results verify the effectiveness of the proposed scheme and show the non-negligible impact of imperfect CSI, thus highlighting the importance of robust designs that maintains users’ QoS in practical wireless networks.},
  keywords={Resource management;Cloud computing;Mobile computing;Interference;Wireless communication;Channel estimation;Machine learning;Cognitive radio (CR) networks;cloud computing;cooperative multi-agent system;reinforcement learning;spectrum management;imperfect CSI},
  doi={10.1109/TMC.2020.3026415},
  ISSN={1558-0660},
  month={May},}@INPROCEEDINGS{10807448,
  author={Takakuwa, Naohiro and Arai, Sachiyo},
  booktitle={2024 IEEE International Conference on Agents (ICA)}, 
  title={Action-Integrated QAttn: Introducing Action Values into Value Decomposition for Effective Cooperation among Heterogenous Agents}, 
  year={2024},
  volume={},
  number={},
  pages={31-32},
  abstract={Multi-agent deep reinforcement learning (MARL) approaches, such as VDN, QMIX, and Qatten, decompose the global value $Q_{t o t}$ into individual action values $Q_{i}$, promoting cooperation among agents. However, these methods typically assume that agents perform homogeneous tasks or have predefined roles. This paper addresses environments where agents must autonomously allocate heterogeneous tasks, such as in the preypredator problem, without explicitly defined roles. To tackle this challenge, we propose a novel method that uses action values as agent features, combined with attention mechanisms. Our approach aims to enable effective learning of task allocation strategies in environments where roles are not explicitly assigned.},
  keywords={Attention mechanisms;Neural networks;Deep reinforcement learning;Resource management;Multi-Agent System;Reinforcement learning;Graph Neural Network},
  doi={10.1109/ICA63002.2024.00014},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10976107,
  author={Gervino, Francesco and Eirale, Andrea and Chiaberge, Marcello and Sacco, Alessio and Marchetto, Guido and Casetti, Claudio},
  booktitle={2025 IEEE 22nd Consumer Communications & Networking Conference (CCNC)}, 
  title={MARS: Multi-Agent Deep Reinforcement Learning for Complex Environment Exploration}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Autonomous exploration of complex, unknown environments is a cutting-edge task not entirely solved by the scientific community. When an agent needs to explore a maze without any a priori information about the environment, the lack of proper destinations and explicit task objectives make traditional navigation policies inappropriate. While the literature presents some sporadic deterministic systems able to face the tasks, learning approaches still need an adequate investigation which could prove them to be more suitable and versatile for this purpose. In this paper, we present MARS, a path planner that exploits swarms of robots to optimize the exploration of complex unknown environments, such as mazes. To make the solution scalable, the proposed method exploits two cooperating modules: local and global planners. The local planner is modeled as a Markov Decision Process (MDP) and trained as a Reinforcement Learning (RL) multi-agent system. Each agent has access to image representations of a section of the global map, always centered in the robot reference frame, and decides the next navigation goal to complete the local exploration. The global planner is a deterministic system that recovers the navigation when a local solution is unavailable. The robots share the explored section with peers when they meet in a rendez-vous. We compared our approach to a single deterministic agent, a single RL agent and a close-to-optimal deterministic approach which deploys five greedy agents. The simulation results demonstrate MARS' efficiency, reaching near-optimal levels in significantly less time.},
  keywords={Mars;Navigation;Simulation;Markov decision processes;Image representation;Deep reinforcement learning;Robots;Faces;Multi-agent systems;multi-agent reinforcement learning;robots swarm;environment exploration;machine learning},
  doi={10.1109/CCNC54725.2025.10976107},
  ISSN={2331-9860},
  month={Jan},}@INPROCEEDINGS{11180603,
  author={Guern–Dall’o, Eloann Le and Féraud, Raphaël and Camilleri, Guy and Maillé, Patrick and Petit, Fabien and Zorgati, Riadh and Ahmed, Hamid Ben and Blavette, Anne},
  booktitle={2025 IEEE Kiel PowerTech}, 
  title={Multi-Agent Multi-Armed Bandits: application to EVs smart charging with grid constraints}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Electrification of energy uses with renewable sources is a major lever of decarbonization. To maximize the use of renewable energy, and tackle its variability and uncertainty, flexible entities such as electric vehicles can be used. This paper presents a scalable, decentralized multi-agent system, where each electric vehicle (EV) seeks the best instants to charge to satisfy its mobility needs, while favoring photovoltaic energy and avoiding congesting the electric network. Each EV makes autonomous decisions using information from its environment, using contextual multi-armed bandit algorithms based on Thompson sampling. Four types of bandit-based algorithms are compared to a baseline through simulations with 55 homogeneous EV agents, achieving a distance of only 14% from an optimal omniscient centralized algorithm. The system’s real-world practicality is demonstrated through minimal computational requirements (200 µs per EV per timestep) and limited information sharing, maintaining user privacy while effectively managing grid congestion.},
  keywords={Photovoltaic systems;Renewable energy sources;Privacy;Uncertainty;Reinforcement learning;Electric vehicles;Low carbon economy;Smart grids;Smart charging;Multi-agent systems;smart grid;multi-agent;reinforcement learning;bandits;electric vehicle smart charging},
  doi={10.1109/PowerTech59965.2025.11180603},
  ISSN={},
  month={June},}@INPROCEEDINGS{11097641,
  author={Ratnabala, Lavanya and Fedoseev, Aleksey and Peter, Robinroy and Tsetserukou, Dzmitry},
  booktitle={2025 IEEE Intelligent Vehicles Symposium (IV)}, 
  title={MAGNNET: Multi-Agent Graph Neural Network-Based Efficient Task Allocation for Autonomous Vehicles with Deep Reinforcement Learning}, 
  year={2025},
  volume={},
  number={},
  pages={970-975},
  abstract={This paper addresses the challenge of decentralized task allocation within heterogeneous multiagent systems operating under communication constraints. We introduce a novel framework that integrates Graph Neural Networks (GNNs) with a centralized training and decentralized execution (CTDE) paradigm, further enhanced by a tailored Proximal Policy Optimization (PPO) algorithm for multi-agent deep reinforcement learning (MARL). Our approach enables unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) to dynamically allocate tasks efficiently without necessitating central coordination in a 3D grid environment. The framework minimizes total travel time while simultaneously avoiding conflicts in task assignments. For the cost calculation and routing, we employ reservation-based $A^{*}$ and $R^{*}$ path planners. Experimental results revealed that our method achieves a high 92.5% conflict-free success rate, with only a 7.49% performance gap compared to the centralized Hungarian method, while outperforming the heuristic decentralized baseline based on a greedy approach. Additionally, the framework exhibits scalability with up to 20 agents with allocation processing of 2.8 s and robustness in responding to dynamically generated tasks, underscoring its potential for real-world applications in complex multi-agent scenarios.},
  keywords={Training;Three-dimensional displays;Scalability;Deep reinforcement learning;Routing;Graph neural networks;Robustness;Resource management;Optimization;Multi-agent systems;Multi-agent system;Task Allocation;Multi-agent Deep Reinforcement Learning;Graph Neural Network;CTDE;Scalability},
  doi={10.1109/IV64158.2025.11097641},
  ISSN={2642-7214},
  month={June},}
