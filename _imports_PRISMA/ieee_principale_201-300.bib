@INPROCEEDINGS{9544385,
  author={Zhan, Mengying and Chen, Jinchao and Du, Chenglie and Xu, Yongqiang},
  booktitle={2021 IEEE 4th International Conference on Computer and Communication Engineering Technology (CCET)}, 
  title={Dueling Network Architecture for Multi-Agent Deep Deterministic Policy Gradient}, 
  year={2021},
  volume={},
  number={},
  pages={163-168},
  abstract={Recently, reinforcement learning has made remarkable achievements in the fields of natural science, engineering, medicine and operational research. Reinforcement learning addresses sequence problems and considers long-term returns. This long-term view of reinforcement learning is critical to find the optimal solution of many problems. The existing multi-agent reinforcement learning methods usually update the value function of state action slowly, and the reward value of agents is low. This paper presents a Dueling Multi-Agent Deep Deterministic Policy Gradient (MADDPG) method based on MADDPG, which modifies critic's network structure. The main work is to add two subnetworks behind the critic network of the traditional MADDPG method. This method allows the critic network to update its parameters faster and receive higher rewards. Finally, in order to verify the validity of the network structure, the improved framework is compared with the traditional MADDPG, DQN and DDPG methods in the simulation environment.},
  keywords={Training;Conferences;Reinforcement learning;Network architecture;Convergence;Dueling network;Reinforcement learning;Deep learning;neural networks;multi-agent system},
  doi={10.1109/CCET52649.2021.9544385},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{8618746,
  author={Muniraj, Devaprakash and Vamvoudakis, Kyriakos G. and Farhood, Mazen},
  booktitle={2018 IEEE Conference on Decision and Control (CDC)}, 
  title={Enforcing Signal Temporal Logic Specifications in Multi-Agent Adversarial Environments: A Deep Q-Learning Approach}, 
  year={2018},
  volume={},
  number={},
  pages={4141-4146},
  abstract={This work addresses the problem of learning optimal control policies for a multi-agent system in an adversarial environment. Specifically, we focus on multi-agent systems where the mission objectives are expressed as signal temporal logic (STL) specifications. The agents are classified as either defensive or adversarial. The defensive agents are maximizers, namely, they maximize an objective function that enforces the STL specification; the adversarial agents, on the other hand, are minimizers. The interaction among the agents is modeled as a finite-state team stochastic game with an unknown transition probability function. The synthesis objective is to determine optimal control policies for the defensive agents that implement the STL specification against the best responses of the adversarial agents. A multi-agent deep Q-learning algorithm, which is an extension of the minimax Q-learning algorithm, is then proposed to learn the optimal policies. The effectiveness of the proposed approach is illustrated through a simulation case study.},
  keywords={Games;Multi-agent systems;Robustness;Linear programming;Markov processes;US Government;multi-agent system;signal temporal logic;deep Q-learning},
  doi={10.1109/CDC.2018.8618746},
  ISSN={2576-2370},
  month={Dec},}@INPROCEEDINGS{9703029,
  author={Rahim, Kashif and Khaliq, Hassaan},
  booktitle={2021 International Conference on Cyber Warfare and Security (ICCWS)}, 
  title={Modeling and Simulation Challenges for Cyber Physical Systems from Operational Security Perspective}, 
  year={2021},
  volume={},
  number={},
  pages={63-69},
  abstract={As Critical Infrastructures become more dependent on Cyber Physical Systems, their design and deployment in reliable, secure and safe manner has become more important. Such systems are bridging the gap between operational and information technologies as physical systems are interconnected and dependent upon underlying computational and communication infrastructure. The operational and information level vulnerabilities can cause physical damage and destruction in the event of system compromise. Appropriately verified modeling and simulation frameworks are therefore essential that may be incorporated from design till deployment stage of software, firmware, hardware and underlying connectivity fabric of physical systems or System of Systems. However, the heterogeneous nature of CPS limits full scale modeling and simulation in a single framework. In this study, state of art in CPS modeling and simulation is introduced for designing resilient systems with new methods and paradigms. Most relevant platforms are then analyzed for modeling CPS from mathematical, system theory, process control, interoperability and resiliency aspects. Lastly, we discuss challenges to CPS modeling and propose game theoretic approaches for formulation of operational security research through scenario building.},
  keywords={Computational modeling;Process control;Reliability engineering;Mathematical models;Software;Security;Interoperability;cyber physical systems;game theory;INTO-CPS;modeling and simulation;ptolemy},
  doi={10.1109/ICCWS53234.2021.9703029},
  ISSN={},
  month={Nov},}@ARTICLE{11000322,
  author={Alamdar, Khawaja G. and Petrović, Tamara},
  journal={IEEE Access}, 
  title={Decentralized Battery-Aware Connectivity Maintenance for Multi-UAV Missions}, 
  year={2025},
  volume={13},
  number={},
  pages={83738-83751},
  abstract={This study presents a novel battery-aware approach for connectivity maintenance for multi-agent systems. It extends a classical connectivity controller by incorporating battery states into the computation of control inputs, enhancing the network’s transition response in the event of agent removals by anticipating battery depletion. Additionally, strategies for agent addition are introduced to maintain the desired level of connectivity within the multi-agent network. The proposed approach is thoroughly tested in the Gymnasium (previously OpenAI Gym) environment using a specific mission scenario. The results demonstrate the controller’s ability to anticipate agent removals and maintain robust performance during transitional stages. The approach is further validated with Crazyflie UAVs in both simulation and real-world environments, with a fleet management system. The system is designed to be modular and easily adaptable to different flight controllers, demonstrating the applicability of the proposed algorithms to real-world multi-robot missions.},
  keywords={Robots;Maintenance;Batteries;Robot kinematics;Autonomous aerial vehicles;Base stations;Robustness;Heuristic algorithms;Multi-agent systems;Multi-robot systems;Connectivity control;connectivity maintenance;fleet management;multi-robot systems},
  doi={10.1109/ACCESS.2025.3569206},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9359223,
  author={Luo, Canhui and Liu, Xuan and Chen, Xinning and Luo, Juan},
  booktitle={2020 IEEE 26th International Conference on Parallel and Distributed Systems (ICPADS)}, 
  title={Multi-agent Fault-tolerant Reinforcement Learning with Noisy Environments}, 
  year={2020},
  volume={},
  number={},
  pages={164-171},
  abstract={Multi-agent reinforcement learning system is used to solve the problem that agents achieve specific goals in the interaction with the environment through learning policies. Almost all existing multi-agent reinforcement learning methods assume that the observation of the agents is accurate during the training process. It does not take into account that the observation may be wrong due to the complexity of the actual environment or the existence of dishonest agents, which will make the agent training difficult to succeed. In this paper, considering the limitations of the traditional multi-agent algorithm framework in noisy environments, we propose a multi-agent fault-tolerant reinforcement learning (MAFTRL) algorithm. Our main idea is to establish the agent's own error detection mechanism and design the information communication medium between agents. The error detection mechanism is based on the autoencoder, which calculates the credibility of each agent's observation and effectively reduces the environmental noise. The communication medium based on the attention mechanism can significantly improve the ability of agents to extract effective information. Experimental results show that our approach accurately detects the error observation of the agent, which has good performance and strong robustness in both the traditional reliable environment and the noisy environment. Moreover, MAFTRL significantly outperforms the traditional methods in the noisy environment.},
  keywords={Training;Fault tolerance;Working environment noise;Fault tolerant systems;Reinforcement learning;Robustness;Noise measurement;multi-agent system;reinforcement learning;noisy environment;fault-tolerant;error detection},
  doi={10.1109/ICPADS51040.2020.00031},
  ISSN={2690-5965},
  month={Dec},}@ARTICLE{10982217,
  author={Rodoshi, Rehenuma Tasnim and Nazib, Rezoan Ahmed and Chun, Chanjun and Choi, Wooyeol},
  journal={IEEE Access}, 
  title={Combined Multi-Agent and Centralized Resource Allocation in Cloud Radio Access Networks}, 
  year={2025},
  volume={13},
  number={},
  pages={79098-79106},
  abstract={The fifth generation (5G) mobile network is designed to facilitate high data rates with massive connectivity with the benefit of small cell technology. The cloud radio access network (C-RAN) is a promising mobile network architecture that can meet the ever-increasing resource demand of a growing number of users. In C-RAN, base station functionalities are separated into baseband units (BBUs) and remote radio heads (RRHs), with BBUs centralized and virtualized via cloud computing. However, this architecture introduces new challenges in efficiently allocating resources to dynamic users. This paper aims to design a resource allocation scheme that improves system efficiency and satisfies dynamic user demands in C-RAN. We propose a hybrid resource allocation approach that combines centralized control and multi-agent-based decision-making. The centralized controller, located within the BBU pool, collaborates with virtual base stations (VBSs) acting as multi-agent system (MAS) agents. The resource allocation solution is derived by jointly considering the real-time resource requests from agents and the historical demand estimates generated by the centralized controller. Through simulation-based evaluation, we compare our proposed scheme with conventional random and fixed resource allocation methods. The results demonstrate improved performance in terms of resource utilization, reduced unfulfilled demand, and fairness among VBS agents. The proposed combined resource allocation strategy effectively meets dynamic user requirements while maintaining system efficiency in C-RAN. Our work highlights the importance of integrating historical demand trends with real-time agent requests for improved long-term resource planning.},
  keywords={Resource management;Dynamic scheduling;Computer architecture;Cloud radio access networks;Base stations;Multi-agent systems;Quality of service;Network architecture;Microprocessors;Market research;Cloud radio access network;multi-agent system;resource allocation},
  doi={10.1109/ACCESS.2025.3566389},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10314177,
  author={Julien, Jean Jimmy and Nuannimnoi, Sirapop and Huang, Ching-Yao},
  booktitle={2023 10th International Conference on Dependable Systems and Their Applications (DSA)}, 
  title={Automated Pricing-based Provisioning of SDN/NFV Services in Distributed Multi-access Edge Computing using Cooperative Multi-Agent Deep Reinforcement Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1-12},
  abstract={The disruption caused by Software Defined Network and Network Function Virtualization (SDN/NFV) technologies will have many impacts on the telecom network. Specifically, the network architecture based on ETSI MANO comprising Virtual Infrastructure Manager (VIM), Virtual Network Function Manager (VNFM), and NFV Orchestrator (NFVO) will significantly change how we operate and manage the telecom network. These impacts on the network architecture will be made gradually. Thus, the migration from non-virtualized networks to all virtualized networks will happen step by step. By introducing new actors into the telecom ecosystem, NFV-MANO will bring in new business models. It is envisaged that these new actors/models will promote competition hence the demand for more flexible charging models with real-time charging. In this research, we will address the architectural realization of the SDN/NFV charging model under the business model of MANO (MANagement and Orchestration) when the service provider (SP) uses the Network Function Virtualization Infrastructure (NFVI) from the NFVIaaS Provider in a distributed multi-access edge computing (MEC) environment. To optimize the Quality of Service (QoS) of MEC resource allocation for incoming services and maximize the overall operating profit of the service provider adopting our charging model, we also propose a new cooperative multi-agent actor-critic based deep reinforcement learning (MADRL) method trained with proximal policy optimization algorithm, namely Coop MAPPO. The results of our experiments showcase the superiority of the Coop-MAPPO multi-agent system over alternative decision-making approaches, with its potential for enhancing operational efficiency and profitability while minimizing failure rates.},
  keywords={Deep learning;Multi-access edge computing;Biological system modeling;Computational modeling;Reinforcement learning;Quality of service;Telecommunications;Multi-Agent Deep Reinforcement Learning;Distributed Edge Computing;Software Defined Network;Network Function Virtualization;Service Provisioning;Charging Factors;Charging Models},
  doi={10.1109/DSA59317.2023.00027},
  ISSN={2767-6684},
  month={Aug},}@INPROCEEDINGS{11193251,
  author={Hu, Wenjie},
  booktitle={2025 5th International Conference on Computer Vision, Application and Algorithm (CVAA)}, 
  title={Analysis of Collaborative Policy Learning Mechanisms in the Multi-Agent PPO-Comm Framework for Game}, 
  year={2025},
  volume={},
  number={},
  pages={398-402},
  abstract={Effective collaboration among agents in Multi-Agent Systems (MAS) is a crucial challenge for accomplishing complex tasks. However, existing methods often suffer from insufficient collaboration efficiency in dynamic environments, and their communication mechanisms are vulnerable to noise interference. To address these issues, this paper proposes a communication mechanism framework named PPO-Comm, based on Multi-Agent Proximal Policy Optimization (PPO), aiming to enhance communication efficiency and collaborative performance among agents through collaborative policy learning. The specific methods include: 1) Designing a Graph Neural Network (GNN)-based communication module where agents achieve efficient information sharing through message passing; 2) Introducing an Attention Mechanism to enhance the ability to extract key information; 3) Combining a multi-agent credit assignment mechanism to optimize agent behavior through a weighted reward function; 4) Adopting a distributed training architecture to improve model training efficiency and convergence speed. Experiments are conducted on platforms such as StarCraft II and MPE. The results show that PPO-Comm achieves a task success rate of $\mathbf{9 5 \%}$ in collaborative tasks, reduces communication costs to $\mathbf{4. 5 M B}$, and demonstrates stronger robustness in dynamic environments. The proposed method effectively solves communication and efficiency problems in multi-agent collaboration, providing new insights for the design of collaborative strategies in MAS.},
  keywords={Training;Scalability;Collaboration;Interference;Reinforcement learning;Robustness;Graph neural networks;Stability analysis;Optimization;Multi-agent systems;Multi-Agent System;PPO-Comm;Collaborative Policy Learning;Communication Mechanism;Graph Neural Network},
  doi={10.1109/CVAA66438.2025.11193251},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10489458,
  author={Zhang, Kai and Xu, Yang and Liu, Chaolun and Zhai, Yangyang},
  booktitle={2023 5th International Conference on Robotics, Intelligent Control and Artificial Intelligence (RICAI)}, 
  title={Intelligent Air Combat Maneuvering Decision Method of Multi-UAV System Based on TA-MASAC}, 
  year={2023},
  volume={},
  number={},
  pages={299-303},
  abstract={To meet the cooperation needs of multiple UAVs in the three-dimensional air combat environment, a multi-agent soft actor-critic algorithm based on the maximum entropy and target allocation named TA-MASAC is proposed. Firstly, this paper expands the air combat environment from two-dimensional to three-dimensional and constructs a multi-UAV air combat mission model and decision-making model in the three-dimensional environment. Then, a multi-UAV countermeasure target allocation based on the Hungarian algorithm is proposed, which is introduced into the MASAC algorithm to explore strategies under the condition of effective cluster coordination. Finally, a 2vs2 multi-UA V air combat simulation is carried out, and the proposed algorithm has proved its feasibility and effectiveness.},
  keywords={Training;Solid modeling;Atmospheric modeling;Robot kinematics;Decision making;Clustering algorithms;Learning (artificial intelligence);UAV;air combat;multi-agent system;reinforcement learning},
  doi={10.1109/RICAI60863.2023.10489458},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9650047,
  author={Park, Bumjin and Kang, Cheongwoong and Choi, Jaesik},
  booktitle={2021 21st International Conference on Control, Automation and Systems (ICCAS)}, 
  title={Generating Multi-agent Patrol Areas by Reinforcement Learning}, 
  year={2021},
  volume={},
  number={},
  pages={104-107},
  abstract={In this paper, we designed reinforcement learning environment for distributed patrolling agents. In the partially observable environment, the agents take actions for each one's interest and the non-stationary problem in multi-agent setting encourages the agents not to invade other agent's region. In our environment, the patrolling routes for the agents are generated implicitly. We suggested different types of the environments and evaluated with different initial positions of the agents. We also show how the reinforcement learning algorithm changes the distribution of agents as training time goes.},
  keywords={Training;Automation;Reinforcement learning;Control systems;Entropy;Task analysis;Multi-agent system;Reinforcement learning;Patrol Task},
  doi={10.23919/ICCAS52745.2021.9650047},
  ISSN={2642-3901},
  month={Oct},}@ARTICLE{9982611,
  author={Khan, Danyal Afgan and Arshad, Ammar and Lehtonen, Matti and Mahmoud, Karar},
  journal={IEEE Access}, 
  title={Combined DR Pricing and Voltage Control Using Reinforcement Learning Based Multi-Agents and Load Forecasting}, 
  year={2022},
  volume={10},
  number={},
  pages={130839-130849},
  abstract={The demand for energy around the world continues to increase at a very high rate. To sufficiently supply this high demand, it is imperative to employ efficient methods so that the total costs for fulfilling such high demand in energy are minimized. To achieve this ambitious goal, this paper proposes a multi-agent reinforcement learning system for time of use pricing based combined demand response and voltage control. For this purpose, a long short term memory network is employed for day-ahead load forecasting in order to remove future uncertainties. The Q-learning algorithm is used which is a model free algorithm and hence, doesn’t require the agent(s) to have prior knowledge of the environment. The role of reinforcement learning in this work is very important since it allows the agent(s) to determine their respective optimal behavior(s) autonomously without explicit training by the end user. To allow effective cooperation among multiple agents, each household is controlled by its own agent, whereas all the household agents are directed by a master agent or service provider. Accordingly, the voltage control agent serves the purpose of checking voltage level violations in the system and removing them through optimal decision making. The proposed system yields very good results, whereby, not only is the overall cost of electricity reduced, but voltage level violations are also removed from the entire system. The implementation of this mechanism reduces the total average aggregated load demand from 5.23 kW to 3.86 kW, while reducing the total aggregated average cost from 94.01 Rs to 60.80 Rs, thanks to the proposed effective multi-agent based system.},
  keywords={Home appliances;Pricing;Voltage control;Costs;Load modeling;Reinforcement learning;Load forecasting;Reinforcement learning;long short term memory;demand response;multi-agent system;voltage control},
  doi={10.1109/ACCESS.2022.3228836},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10650505,
  author={Li, Jianping and Tan, Guozhen},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Signed Safety Field Reinforcement Learning with Multi-agent System in Competitive and Cooperative Environments}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Through mean-field optimization, mean-field reinforcement learning provides an applicable method for the environment of many agents. However, mean-field reinforcement learning uses only the mean action of the population as a basis, which may result in the conformity effect for individual decision-making and is challenging to implement in the complex real world with multi-agent systems. The main purpose of the study is to reduce the influence of the conformity effect using the signed safety field model to describe the population’s state. The comprehensive understanding of the information available to the population is aimed at enhancing individual decision-making. The state of the population guides the actions of local agents. The signed safety framework was also established, including the signed safety field Q and the AC learning algorithm. In addition, we demonstrated that these two algorithms could converge to the point of Nash equilibrium. The experiments in three situations were conducted to show the signed safety field method outperforms other baseline algorithms.},
  keywords={Decision making;Neural networks;Buildings;Reinforcement learning;Nash equilibrium;Safety;Computational complexity;mean field optimization;mean field reinforcement learning;multi-agent systems;signed safety field},
  doi={10.1109/IJCNN60899.2024.10650505},
  ISSN={2161-4407},
  month={June},}@ARTICLE{10433866,
  author={Liu, Da and Zong, Qun and Zhang, Xiuyun and Zhang, Ruilong and Dou, Liqian and Tian, Bailing},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, 
  title={Game of Drones: Intelligent Online Decision Making of Multi-UAV Confrontation}, 
  year={2024},
  volume={8},
  number={2},
  pages={2086-2100},
  abstract={Due to the characteristics of the small size and low cost of unmanned aerial vehicles (UAVs), Multi-UAV confrontation will play an important role in future wars. The Multi-UAV confrontation game in the air combat environment is investigated in this paper. To truly deduce the confrontation scene, a physics engine is established based on the Multi-UAV Confrontation Scenario (MCS) framework, enabling the real-time interaction between the agent and environment while making the learned strategies more realistic. To form an effective confrontation strategy, the Graph Attention Multi-agent Soft Actor Critic Reinforcement Learning with Target Predicting Network (GA-MASAC-TP Net) is firstly proposed for Multi-UAV confrontation game. The merits lie in that the Multi-UAV trajectory prediction, considering interactions among targets, is incorporated innovatively into the Multi-agent reinforcement learning (MARL), enabling Multi-UAVs to make decisions more accurately based on situation prediction. Specifically, the Soft Actor Critic (SAC) algorithm is extended to the Multi-agent domain and embed with the graph attention neural network into the Actor, Critic network, so the UAV could aggregate the information of the spatial neighbor teammates based on the attention mechanism for better collaboration. The comparative experiment and ablation study demonstrate the effectiveness of the proposed algorithm and the state-of-art performance in the MCS.},
  keywords={Games;Autonomous aerial vehicles;Reinforcement learning;Heuristic algorithms;Trajectory;Real-time systems;Prediction algorithms;Multi-UAV confrontation;multi-agent system;graph attention neural network;multi-UAV trajectory prediction},
  doi={10.1109/TETCI.2024.3360282},
  ISSN={2471-285X},
  month={April},}@INPROCEEDINGS{9376433,
  author={Lan, Xi and Qiao, Yuansong and Lee, Brian},
  booktitle={2021 7th International Conference on Automation, Robotics and Applications (ICARA)}, 
  title={Towards Pick and Place Multi Robot Coordination Using Multi-agent Deep Reinforcement Learning}, 
  year={2021},
  volume={},
  number={},
  pages={85-89},
  abstract={Recent advances in deep reinforcement learning are enabling the creation and use of powerful multi-agent systems in complex areas such as multi-robot coordination. These show great promise to help solve many of the difficult challenges of rapidly growing domains such as smart manufacturing. In this position paper we describe our early-stage work on the use of multi-agent deep reinforcement learning to optimise coordination in a multi-robot pick and place system. Our goal is to evaluate the feasibility of this new approach in a manufacturing environment. We propose to adopt a decentralised partially observable Markov Decision Process approach and to extend an existing cooperative game work to suitably formulate the problem as a multiagent system. We describe the centralised training/decentralised execution multi-agent learning approach which allows a group of agents to be trained simultaneously but to exercise decentralised control based on their local observations. We identify potential learning algorithms and architectures that we will investigate as a base for our implementation and we outline our open research questions. Finally we identify next steps in our research program.},
  keywords={Robot kinematics;Reinforcement learning;Games;Markov processes;Optimization;Multi-agent systems;Smart manufacturing;multi-agent system;pick and place;deep reinforcement learning;multi-robot system;Dec-POMDP},
  doi={10.1109/ICARA51699.2021.9376433},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{9892225,
  author={Chen, Hao and Yang, Guangkai and Zhang, Junge and Yin, Qiyue and Huang, Kaiqi},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)}, 
  title={RACA: Relation-Aware Credit Assignment for Ad-Hoc Cooperation in Multi-Agent Deep Reinforcement Learning}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={In recent years, reinforcement learning has faced several challenges in the multi-agent domain, such as the credit assignment issue. Value function factorization emerges as a promising way to handle the credit assignment issue under the centralized training with decentralized execution (CTDE) paradigm. However, existing value function factorization methods cannot deal with ad-hoc cooperation, that is, adapting to new configurations of teammates at test time. Specifically, these methods do not explicitly utilize the relationship between agents and cannot adapt to different sizes of inputs. To address these limitations, we propose a novel method, called Relation-Aware Credit Assignment (RACA), which achieves zero-shot generalization in ad-hoc cooperation scenarios. RACA takes advantage of a graph-based relation encoder to encode the topological structure between agents. Furthermore, RACA utilizes an attention-based observation abstraction mechanism that can generalize to an arbitrary number of teammates with a fixed number of parameters. Experiments demonstrate that our method outperforms baseline methods on the StarCraftII micromanagement benchmark and ad-hoc cooperation scenarios.},
  keywords={Training;Neural networks;Reinforcement learning;Benchmark testing;Ad hoc networks;Multi-Agent System;Deep Reinforcement Learning;Ad-Hoc Cooperation},
  doi={10.1109/IJCNN55064.2022.9892225},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{9817168,
  author={Ornatelli, Antonio and Giuseppi, Alessandro and Tortorelli, Andrea},
  booktitle={2022 IEEE World AI IoT Congress (AIIoT)}, 
  title={A Distributed Average Cost Reinforcement Learning approach for Power Control in Wireless 5G Networks}, 
  year={2022},
  volume={},
  number={},
  pages={393-399},
  abstract={This paper deals with the transmission power control problem in wireless networks. Such a problem represents a well known and relevant issue as it allows to efficiently manage the network's required energy and the interference experienced by end-users. With the widespread diffusion of smart devices, the relevance of this aspect further increased and has been identified as such also in 5G standards. The problem has been formalized as a Multi-Agent Reinforcement Learning approach (MARL) to guarantee scalability and robustness. These two aspects also drove the development of an original Distributed Average-Cost Temporal-Difference (TD) Learning algorithm. To adopt such an algorithm, a Markov Game formulation of the power control problem has also been derived. The effectiveness of the proposed distributed framework in reducing the total network's transmission power has been proved by means of simulations in a specific case study.},
  keywords={Costs;5G mobile communication;Wireless networks;Scalability;Power control;Reinforcement learning;Robustness;Distributed Reinforcement Learning;Power Control;Average Cost TD Learning;Dynamic Consensus;Net- worked Multi-Agent System},
  doi={10.1109/AIIoT54504.2022.9817168},
  ISSN={},
  month={June},}@INPROCEEDINGS{10286796,
  author={Saker, Wiam and Benatia, Mohamed Amin and Sahnoun, M'hammed and Mouss, Nadia Kinza},
  booktitle={2023 International Conference on Decision Aid Sciences and Applications (DASA)}, 
  title={Distributed Dynamics Scheduling Based Reinforcement Learning: Importance and Challenges}, 
  year={2023},
  volume={},
  number={},
  pages={314-319},
  abstract={When it comes to scheduling choices inside complex industrial systems, the dynamic job shop scheduling problem (DJSSP) poses substantial difficulties. Deep learning, artificial intelligence (AI), and reinforcement learning approaches have all shown promising solutions in recent years to enhance the effectiveness and performance of DJSSP systems. This study provides a detailed analysis of the DJSSP literature, with an emphasis on these cutting-edge methods. The review adopts a rational methodology that includes an exhaustive search of many databases from 1995 to 2023. Articles were chosen based on predetermined qualifying criteria, taking into account their applicability to the DJSSP and the methodology used. This paper also examines how software design affects research, how industry influences research, and the types of research outputs that are disclosed. The findings provide valuable insights into the current state of research and offer guidance for future advancements in optimizing dynamic job-shop scheduling using advanced learning techniques.},
  keywords={Industries;Deep learning;Job shop scheduling;Software design;Databases;Reinforcement learning;Dynamic scheduling;Dynamic scheduling;Job-shop problem;distributed control and supervision;multi-agent system;reinforcement learning},
  doi={10.1109/DASA59624.2023.10286796},
  ISSN={},
  month={Sep.},}@ARTICLE{9757149,
  author={Omoniwa, Babatunji and Galkin, Boris and Dusparic, Ivana},
  journal={IEEE Wireless Communications Letters}, 
  title={Optimizing Energy Efficiency in UAV-Assisted Networks Using Deep Reinforcement Learning}, 
  year={2022},
  volume={11},
  number={8},
  pages={1590-1594},
  abstract={In this letter, we study the energy efficiency (EE) optimization of unmanned aerial vehicles (UAVs) providing wireless coverage to static and mobile ground users. Recent multi-agent reinforcement learning approaches optimise the system’s EE using a 2D trajectory design, neglecting interference from nearby UAV cells. We aim to maximize the system’s EE by jointly optimizing each UAV’s 3D trajectory, number of connected users, and the energy consumed, while accounting for interference. Thus, we propose a cooperative Multi-Agent Decentralized Double Deep Q-Network (MAD-DDQN) approach. Our approach outperforms existing baselines in terms of EE by as much as 55– 80%.},
  keywords={Autonomous aerial vehicles;Interference;Trajectory;Wireless communication;Three-dimensional displays;Optimization;Signal to noise ratio;Energy efficiency;UAV base stations;deep reinforcement learning;multi-agent system},
  doi={10.1109/LWC.2022.3167568},
  ISSN={2162-2345},
  month={Aug},}@INPROCEEDINGS{9313387,
  author={Wang, Hanchao and Tang, Hongyao and Hao, Jianye and Hao, Xiaotian and Fu, Yue and Ma, Yi},
  booktitle={2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Large Scale Deep Reinforcement Learning in War-games}, 
  year={2020},
  volume={},
  number={},
  pages={1693-1699},
  abstract={War-game is a type of multi-agent real-time strategy game, with challenges of the large-scale decision-making space and the flexible and changeable battlefield situation. In addition to the military field, it has played a role in fields including epidemic prevention and pest control. In recent years, more and more learning algorithms have tried to solve this kind of game. However, the existing methods have not yet given a satisfactory solution for the war-game, especially when preparation time is limited. In this background, we try to solve a traditional war-game based on hexagon grids. We propose a hierarchical multi-agent reinforcement learning framework to rapidly training an AI model for the war-game. The higher-level network in our hierarchical framework is used for task decision, it solves the credit assignment problem between agents through cooperative training. The lower-level network is mainly used for route planning, and it can be reused through parameter sharing for all the agents and all the maps. To deal with various opponents, we improve the robustness of the model through a grouped self-play approach. In experiments, we get encouraging results which show that the hierarchical structure allows agents to learn their strategies effectively. Our final AI model demonstrates that our methods can effectively deal with the challenges in the war-game.},
  keywords={Task analysis;Games;Training;Artificial intelligence;Reinforcement learning;Markov processes;Decision making;Multi-agent system;Reinforcement Learning;War-game;Self-play},
  doi={10.1109/BIBM49941.2020.9313387},
  ISSN={},
  month={Dec},}@ARTICLE{9763486,
  author={Wang, Jianrui and Hong, Yitian and Wang, Jiali and Xu, Jiapeng and Tang, Yang and Han, Qing-Long and Kurths, Jürgen},
  journal={IEEE/CAA Journal of Automatica Sinica}, 
  title={Cooperative and Competitive Multi-Agent Systems: From Optimization to Games}, 
  year={2022},
  volume={9},
  number={5},
  pages={763-783},
  abstract={Multi-agent systems can solve scientific issues related to complex systems that are difficult or impossible for a single agent to solve through mutual collaboration and cooperation optimization. In a multi-agent system, agents with a certain degree of autonomy generate complex interactions due to the correlation and coordination, which is manifested as cooperative/competitive behavior. This survey focuses on multi-agent cooperative optimization and cooperative/non-cooperative games. Starting from cooperative optimization, the studies on distributed optimization and federated optimization are summarized. The survey mainly focuses on distributed online optimization and its application in privacy protection, and overviews federated optimization from the perspective of privacy protection mechanisms. Then, cooperative games and non-cooperative games are introduced to expand the cooperative optimization problems from two aspects of minimizing global costs and minimizing individual costs, respectively. Multi-agent cooperative and non-cooperative behaviors are modeled by games from both static and dynamic aspects, according to whether each player can make decisions based on the information of other players. Finally, future directions for cooperative optimization, cooperative/non-cooperative games, and their applications are discussed.},
  keywords={Privacy;Costs;Correlation;Decision making;Focusing;Collaboration;Games;Cooperative games;counterfactual regret minimization;distributed optimization;federated optimization;fictitious self-play;mean field games;multi-agent reinforcement learning;non-cooperative games},
  doi={10.1109/JAS.2022.105506},
  ISSN={2329-9274},
  month={May},}@ARTICLE{9712375,
  author={Dai, Zhaojun and Zhang, Yan and Zhang, Wancheng and Luo, Xinran and He, Zunwen},
  journal={IEEE Transactions on Signal and Information Processing over Networks}, 
  title={A Multi-Agent Collaborative Environment Learning Method for UAV Deployment and Resource Allocation}, 
  year={2022},
  volume={8},
  number={},
  pages={120-130},
  abstract={The dynamic position deployment and resource allocation of the unmanned aerial vehicle (UAV) communication networks has great significance in terms of interference management, coverage enhancement, and capacity improvement. Since the transmission power and energy resources of the UAVs are limited and the actual communication environment is complex and time-varying, it is challenging for the multiple UAVs to dynamically make decisions to ensure the communication performance of the system. Meanwhile, the centralized architecture may generate a certain degree of communication delay and affect communication efficiency. Facing this challenge, a resource allocation algorithm for the UAV networks based on multi-agent collaborative environment learning is proposed. This method is based on a distributed architecture. Each UAV is modeled as an independent agent, which improves the utility of the UAV networks through the dynamic selection decisions of its deployment position, transmission power, and occupied sub-channels. Each UAV learns the mapping of the network information to the position deployment and resource selection decisions based on the reinforcement learning algorithm according to partial of the state information it can observe. For the overall network, a multi-agent reinforcement learning method based on federated learning is designed on the purpose of realizing information interaction and combined dispatching of the UAVs. In the multi-agent system, the framework of federated learning is introduced to realize the sharing of non-privacy data among the UAVs. Simulation results indicate that the proposed method can effectively improve the network utility compared with the multi-agent deep reinforcement learning algorithm without information interaction.},
  keywords={Resource management;Autonomous aerial vehicles;Communication networks;Reinforcement learning;Information processing;Machine learning algorithms;Heuristic algorithms;Federated learning;location deployment;reinforcement learning;resource allocation;unmanned aerial vehicle networks},
  doi={10.1109/TSIPN.2022.3150911},
  ISSN={2373-776X},
  month={},}@INPROCEEDINGS{10556841,
  author={Cao, Jiawei and Leong, Wai Lun and Huat Teo, Rodney Swee},
  booktitle={2024 International Conference on Unmanned Aircraft Systems (ICUAS)}, 
  title={A Highly Scalable, Robust and Decentralized Approach for Multi-UAV Persistent Surveillance}, 
  year={2024},
  volume={},
  number={},
  pages={971-977},
  abstract={Multi-robot patrolling is known to be challenging, especially in a decentralized manner. The state-of-the-art de-centralized approaches are either suboptimal or usually require exchange of information that would potentially limit their scalability. This paper presents a novel decentralized approach of high scalability and robustness to multi- UAV persistent surveillance. Our solution decentralizes a cyclic strategy while considering communication constraints. We give a theoretical derivation of the decentralized algorithm with convergence analysis. In addition, we consider practical issues such as motion constraints and potential livelocks in our implementation. The proposed approach is extensively tested and analyzed in a medium-fidelity swarm simulator to minimize the gap between simulation and real experiments.},
  keywords={Surveillance;Scalability;Autonomous aerial vehicles;Robustness;Aircraft;Convergence},
  doi={10.1109/ICUAS60882.2024.10556841},
  ISSN={2575-7296},
  month={June},}@INPROCEEDINGS{10978593,
  author={Liao, Wensong and Mao, Jiahui and Tan, Chong and Zheng, Min},
  booktitle={2025 IEEE Wireless Communications and Networking Conference (WCNC)}, 
  title={Deep Learning-Based Dispatching Conflict Avoider in MEC Systems}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Multi-access Edge Computing (MEC) is a promising computing paradigm that brings cloud services closer to mobile devices. In contrast to cloud computing, MEC dispatches tasks to edge servers, offering lower transmission/serving latency. Distributed dispatch algorithms are commonly employed in large-scale MEC systems to reduce information-sharing latency. However, even in small local area networks (LAN), the Round-Trip Time (RTT) required for information sharing is in the millisecond range. When multiple devices generate tasks within a single RTT, similar environmental states can result in identical dispatching decisions, leading to a sudden surge in the load on a specific server. This phenomenon is known as a dispatching conflict. To address this, we propose DCA (Dispatching Conflicts Avoider), a distributed intelligent agent designed to mitigate dispatching conflicts by dynamically regulating the task pools of edge servers in real-time. Simulation results demonstrate that DCA significantly reduces the weighted response time (WRT) compared to baseline distributed heuristic algorithms, particularly under high-concurrency scenarios.},
  keywords={Cloud computing;Heuristic algorithms;Simulation;Neural networks;Dispatching;Real-time systems;Servers;Time factors;Surges;Local area networks;MEC;dispatching conflict;deep learning},
  doi={10.1109/WCNC61545.2025.10978593},
  ISSN={1558-2612},
  month={March},}@INPROCEEDINGS{10191422,
  author={Fu, Qingxu and Qiu, Tenghai and Pu, Zhiqiang and Yi, Jianqiang and Ai, Xiaolin and Yuan, Wanmai},
  booktitle={2023 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Learning Superior Cooperative Policy in Adversarial Multi-Team Reinforcement Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={Multi-agent Reinforcement Learning (MARL) has become a powerful tool for addressing multi-agent challenges. Existing studies have explored numerous models to use MARL to solve single-team cooperation (competition) problems and adversarial problems with opponents controlled by static knowledge-based policies. However, most studies in the literature often ignore adversarial multi-team problems involving dynamically evolving opponents. We investigate adversarial multi-team problems where all participating teams use MARL learners to learn policies against each other. Two objectives are achieved in this study. Firstly, we design an adversarial team-versus-team learning framework to generate cooperative multi-agent policies to compete against opponents without preprogrammed opponent partners or any supervision. Secondly, we explore the key factors to achieve win-rate superiority during dynamic competitions. Then we put forward a novel FeedBack MARL (FBMARL) algorithm that takes advantage of feedback loops to adjust optimizer hyper-parameters based on real-time game statistics. Finally, the effectiveness of our FBMARL model is tested in a benchmark environment named Multi-Team Decentralized Collective Assault (MT-DCA). The results demonstrate that our feedback MARL model can achieve superior performance over baseline competitor MARL learners in 2-team and 3-team dynamic competitions.},
  keywords={Training;Feedback loop;Heuristic algorithms;Neural networks;Knowledge based systems;Reinforcement learning;Games;Multi-agent system;reinforcement learning;competitive RL},
  doi={10.1109/IJCNN54540.2023.10191422},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{9242822,
  author={Varshney, Niharika and Dewan, Lillie and Bhusan, Shashi},
  booktitle={2020 First IEEE International Conference on Measurement, Instrumentation, Control and Automation (ICMICA)}, 
  title={A Comprehensive Survey on Event Triggered Control for State Feedback and State Observer Based Closed Loop Systems}, 
  year={2020},
  volume={},
  number={},
  pages={1-5},
  abstract={This paper presents a comparison between state feedback and state observer based system and application of event triggered based control techniques on various systems. Event triggered mechanism is constructed with the help of sensors and actuators to detect any error occurred and to bring back the system to its stability. Event triggered control can be feedback based or observer based, advantages and limitations of both the methods has been discussed. This paper shows how closed loop system is being controlled by applying event-based control law to build its stability and can be cost effective. This paper also discusses future trends of event triggered control in various systems.},
  keywords={State feedback;Uncertainty;Observers;Stability analysis;Sensors;Closed loop systems;Velocity measurement;event trigger;state feedback;state observer;network system;multi-agent system},
  doi={10.1109/ICMICA48462.2020.9242822},
  ISSN={},
  month={June},}@INPROCEEDINGS{9331136,
  author={Xin, Yu and Bao, Xiaohong},
  booktitle={2020 7th International Conference on Dependable Systems and Their Applications (DSA)}, 
  title={System Vulnerability Oriented to Strategies}, 
  year={2020},
  volume={},
  number={},
  pages={493-499},
  abstract={System vulnerability has received more and more attention. However, traditional methods of vulnerability are difficult to apply to multi-agent systems with complex game relations with constraints, and research on competition and cooperative relationships rarely considers its system background and constraints. This paper summarizes the existing research and uses reinforcement learning technology to abstract the complex relationship with constraints into a multi-agent model. Through simulation analysis, the impact of three different strategies on system vulnerability is obtained, and provides solutions to improve the vulnerability of the system.},
  keywords={Analytical models;Games;Reinforcement learning;Multi-agent systems;component;System Vulnerability;Game Theory;Reinforcement Learning;Multi-Agent System;Strategic Analysis},
  doi={10.1109/DSA51864.2020.00085},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9257684,
  author={Zemzem, Wiem and Hosni, Ines},
  booktitle={2020 2nd International Conference on Computer and Information Sciences (ICCIS)}, 
  title={Improving Multi-Agent Cooperation Using Directed Exploration}, 
  year={2020},
  volume={},
  number={},
  pages={1-5},
  abstract={In this work, we are addressing the problem of fully cooperative multi-agent system (MASs) with the same common goal for all agents. Coordination question is the main focus in such systems: how to ensure that the agents' own decisions contribute to the group's jointly optimal decisions? To solve this, a new multi-agent reinforcement learning algorithm, named TM LRVS Qlearning, is introduced and tested. The usefulness of this new method is shown using a simulated hunting game.},
  keywords={Reinforcement learning;Multi-agent systems;Games;Mathematical model;Convergence;Testing;Task analysis;cooperative;multi-agent systems;coordination;reinforcement learning},
  doi={10.1109/ICCIS49240.2020.9257684},
  ISSN={},
  month={Oct},}@ARTICLE{10538064,
  author={Yin, Baocai and Weng, Haoen and Hu, Yongli and Xi, Jiayang and Ding, Pinggang and Liu, Jia},
  journal={IEEE Transactions on Power Systems}, 
  title={Multi-Agent Deep Reinforcement Learning for Simulating Centralized Double-Sided Auction Electricity Market}, 
  year={2025},
  volume={40},
  number={1},
  pages={518-529},
  abstract={Developing optimal bidding strategies for the market participants plays a crucial role in increasing the profit of the electricity market. For the complex double-sided auction market featured with partial observability, uncertainty and dynamic nature in the bidding transaction, current methods like Reinforcement Learning (RL) based on single agent framework are difficult to model the strategic bidding behavior of both supply side and demand side with elastic demand. For this purpose, this paper proposes a multi-agent system based on Multi-Agent Deep Deterministic Policy Gradient (MADDPG), combined with Prioritized Experience Replay (PER) mechanism, namely MADDPG-PER, to simulate the complex market environment. The multi-agent system based on MADDPG-PER is not only able to deal with the sparse reward gradient phenomenon for auction-based electricity market, but also can find superior bidding strategies for the market participants. The MADDPG-PER algorithm is evaluated on 9-bus and 30-bus congested network, where both supply and demand sides are modeled as RL agents. The results demonstrate that MADDPG-PER outperforms the state-of-the-art methods under different level of system uncertainty.},
  keywords={Electricity supply industry;Electricity;Heuristic algorithms;Optimization;Aerospace electronics;Q-learning;Companies;Electricity market;double-sided auction;bidding strategy;agent-based simulation (ABS);deep reinforcement learning (DRL);multi-agent deep deterministic policy gradient (MADDPG);prioritized experience replay (PER)},
  doi={10.1109/TPWRS.2024.3404472},
  ISSN={1558-0679},
  month={Jan},}@INPROCEEDINGS{8525896,
  author={Rezzai, Maha and Dachry, Wafaa and Moutaouakkil, Fouad and Medromi, Hicham},
  booktitle={2018 6th International Conference on Multimedia Computing and Systems (ICMCS)}, 
  title={Design and realization of a new architecture based on multi-agent systems and reinforcement learning for traffic signal control}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={Increasing the number of cars in cities creates traffic congestion. This is due to static management of traffic lights. Reinforcement Learning RL algorithm is an artificial intelligence approach that enables adaptive real-time control at intersections. In this research paper, we purpose a new architecture based on multi-agent systems and RL algorithm in order to make the signal control system more autonomous, able to learn from its environment and make decisions to optimize road traffic.},
  keywords={Multi-agent systems;Traffic control;Heuristic algorithms;Real-time systems;Systems architecture;Reinforcement Learning;Multi-Agent System;AUML;Q-learning},
  doi={10.1109/ICMCS.2018.8525896},
  ISSN={2472-7652},
  month={May},}@ARTICLE{10144471,
  author={Guo, Jiaying and Cheng, Long and Wang, Shen},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={CoTV: Cooperative Control for Traffic Light Signals and Connected Autonomous Vehicles Using Deep Reinforcement Learning}, 
  year={2023},
  volume={24},
  number={10},
  pages={10501-10512},
  abstract={The target of reducing travel time only is insufficient to support the development of future smart transportation systems. To align with the United Nations Sustainable Development Goals (UN-SDG), a further reduction of fuel and emissions, improvements of traffic safety, and the ease of infrastructure deployment and maintenance should also be considered. Different from existing work focusing on optimizing the control in either traffic light signal (to improve the intersection throughput), or vehicle speed (to stabilize the traffic), this paper presents a multi-agent Deep Reinforcement Learning (DRL) system called CoTV, which Cooperatively controls both Traffic light signals and Connected Autonomous Vehicles (CAV). Therefore, our CoTV can well balance the reduction of travel time, fuel, and emissions. CoTV is also scalable to complex urban scenarios by cooperating with only one CAV that is nearest to the traffic light controller on each incoming road. This avoids costly coordination between traffic light controllers and all possible CAVs, thus leading to the stable convergence of training CoTV under the large-scale multi-agent scenario. We describe the system design of CoTV and demonstrate its effectiveness in a simulation study using SUMO under various grid maps and realistic urban scenarios with mixed-autonomy traffic.},
  keywords={Fuels;Training;Roads;Traffic control;Throughput;Safety;Velocity control;Deep reinforcement learning;multi-agent system;connected autonomous vehicles;mixed-autonomy traffic},
  doi={10.1109/TITS.2023.3276416},
  ISSN={1558-0016},
  month={Oct},}@ARTICLE{9917497,
  author={Liu, Shuai and Han, Siyuan and Zhu, Shanying},
  journal={IEEE Transactions on Smart Grid}, 
  title={Reinforcement Learning-Based Energy Trading and Management of Regional Interconnected Microgrids}, 
  year={2023},
  volume={14},
  number={3},
  pages={2047-2059},
  abstract={In this paper, we present a Value-Decomposition Deep Deterministic Policy Gradients (V3DPG) based Reinforcement Learning (RL) method for energy trading and management of regional interconnected microgrids (MGs). In practice, the state of an MG is time-varying and the traded energy flows continuously, which is generally neglected in researches. To address this problem, an Actor-Critic framework is adopted. Each MG has to make energy trading decision based on local observation and has no access to any knowledge of other MGs. We bring in the idea of value-decomposition in the training process to ensure the generation of feasible cooperative policies while maintaining MGs’ privacy and autonomous decision-making ability. Furthermore, in light of the uncertainty and fluctuation of renewable energy generation and users’ demand, a recurrent neural network (RNN) with Burn-In initialization is combined with critic network to achieve implicit predictions. Meanwhile, we also take Energy Storage System (ESS) with operational constraints into consideration and deem it as a virtual market innovatively. Experiments have been carried out under real-world data to verify the merit of the proposed method, compared to existing RL-based works.},
  keywords={Renewable energy sources;State of charge;Uncertainty;Microgrids;Energy management;Generators;Fluctuations;Multi-agent system;regional interconnected microgrids;energy trading and management;reinforcement learning;RNN},
  doi={10.1109/TSG.2022.3214202},
  ISSN={1949-3061},
  month={May},}@INPROCEEDINGS{9679143,
  author={Li, Xujia and Shen, Yanyan and Chen, Lei},
  booktitle={2021 IEEE International Conference on Data Mining (ICDM)}, 
  title={Mcore: Multi-Agent Collaborative Learning for Knowledge-Graph-Enhanced Recommendation}, 
  year={2021},
  volume={},
  number={},
  pages={330-339},
  abstract={Recently, knowledge-graph-enhanced recommendation systems have attracted much attention, since knowledge graph (KG) can help improving the dataset quality and offering rich semantics for explainable recommendation. However, current KG-enhanced solutions focus on analyzing user behaviors on the product level and lack effective approaches to extract user preference towards product category, which is essential for better recommendation because users shopping online normally have strong preference towards distinctive product categories, not merely on products, according to various user studies. Moreover, the existing pure embedding-based recommendation methods can only utilize KGs with a limited size, which is not adaptable to many real-world applications. In this paper, we generalize the recommendation problem with preference mining as a compound knowledge reasoning task and propose a novel multi-agent system, called Mcore, which can promote model performance by mining users’ high-level interests and is adaptable to large KGs. Specifically, we split the overall problem and allocate sub-task to each agent: Coordinate Agent takes charge of recognizing the product-category preference of current user, while Relation Agent and Entity Agent perform KG reasoning cooperatively from a user node towards the preferred categories and terminate at a product node as recommendation. To train this heterogeneous multi-agent system, where agents own various functionalities, we propose an asynchronous reinforcement training pipeline, called Multi-agent Collaborative Learning. The extensive experiments on real datasets demonstrate the effectiveness and adaptability of Mcore on recommendation tasks.},
  keywords={Training;Navigation;Robot kinematics;Pipelines;Semantics;Collaborative work;Cognition;Knowledge Graph Reasoning;Multi-agent Reinforcement Learning;Explainable Recommendation},
  doi={10.1109/ICDM51629.2021.00044},
  ISSN={2374-8486},
  month={Dec},}@INPROCEEDINGS{9238802,
  author={Mai, Tianle and Yao, Haipeng and Zhang, Xing and Xiong, Zehui and Niyato, Dusit},
  booktitle={2020 IEEE/CIC International Conference on Communications in China (ICCC)}, 
  title={A Distributed Reinforcement Learning Approach to In-network Congestion Control}, 
  year={2020},
  volume={},
  number={},
  pages={817-822},
  abstract={Due to network traffic volatility, congestion control has been a challenging problem faced by network operators. The current network is often over-provisioned to accommodate the worst-case congestion conditions (e.g. links running at only around 30% capacity). Effectively congestion control schemes can enhance network utilization and lower operators cost. Nowadays, the most commonly used congestion control technique is end-host based solutions (e.g., additive increase/multiplicative decrease (AIMD)), which use the feedback signal from the network (e.g., explicit congestion notification (ECN) or round trip time (RTT)) to adjust the transmission rates. However, these solutions are hard to detect and respond to the millisecond microburst traffics in the current network. In this paper, we focus on the volatility that occurs on the timescales of 10 to 100 milliseconds, which are still large enough to cause congestion to occur. We propose a reinforcement learning aided in-network congestion control scheme. The congestion control algorithm is directly implemented inside the switches to quickly adapt to traffic volatility. Besides, to enhance the network-scale cooperative control among distributed switches, we adopt the centralized training with decentralized execution framework, where a centralized critic is introduced to ease the training process of distributed switches. The extensive simulations are performed on Omnet++ to evaluate our proposed algorithm in comparison to state-of-the-art schemes.},
  keywords={Training;Process control;Reinforcement learning;Telecommunication traffic;Performance gain;Control systems;Optimization;Multi-agent system;Reinforcement learning;Traffic Engineering;Congestion control},
  doi={10.1109/ICCC49849.2020.9238802},
  ISSN={2377-8644},
  month={Aug},}@INPROCEEDINGS{10839941,
  author={Yao, Jietong and Gong, Xin},
  booktitle={2024 IEEE International Conference on Unmanned Systems (ICUS)}, 
  title={Communication-Efficient and Resilient Distributed Deep Reinforcement Learning for Multi-Agent Systems}, 
  year={2024},
  volume={},
  number={},
  pages={1521-1526},
  abstract={In this paper, we address the challenge of developing a resilient Deep Q-Network (DQN) algorithm for multi-agent systems (MASs) under Byzantine attacks. Traditional Q-learning methods suffer from significant communication overhead and limited capacity to handle high-dimensional state spaces, which restricts their scalability and effectiveness in large-scale MASs. To mitigate these issues, we propose an event-triggered resilient DQN (ET-RDQN) algorithm that reduces unnecessary communication while maintaining robustness under the environment exists adversarial agents. Our approach extends the DQN algorithm to MASs and employs the Mean-Subsequence Reduced (MSR) algorithm to filter out extreme values, ensuring that normal agents can still approximately estimate the optimal state-action values and achieve consensus even in the presence of malicious agents. The algorithm incorporates an event-triggered mechanism that triggers communication only when the triggering conditions are met, thereby reducing communication overhead. Theoretical analysis and extensive simulations demonstrate that our ET-RDQN algorithm significantly improves the resilience and efficiency of MASs in the presence of Byzantine attacks, providing a scalable and effective solution for distributed reinforcement learning in adversarial environments. Finally, we conclude this paper and outline potential directions for future work.},
  keywords={Q-learning;Event detection;Network topology;Scalability;Filtering algorithms;Approximation algorithms;Deep reinforcement learning;Robustness;Multi-agent systems;Resilience;Byzantine-resilient;multi-agent system;eventtriggered communication;Deep Q-Network},
  doi={10.1109/ICUS61736.2024.10839941},
  ISSN={2771-7372},
  month={Oct},}@ARTICLE{10681512,
  author={Peng, Zhe and Lu, Zhifeng and Mao, Xiao and Ye, Feng and Huang, Kuihua and Wu, Guohua and Wang, Ling},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, 
  title={Multi-Ship Dynamic Weapon-Target Assignment via Cooperative Distributional Reinforcement Learning With Dynamic Reward}, 
  year={2025},
  volume={9},
  number={2},
  pages={1843-1859},
  abstract={In fleet air defense, the efficient coordination of multiple ships to complete weapon-target assignment has always been a critical challenge, primarily due to the varying combat capabilities and duties associated with each ship. Consequently, the traditional “weapon-target” assignment mode has turned into a “ship-weapon-target” assignment mode in the multi-ship dynamic weapon-target assignment (MS-DWTA) problem we proposed, with a larger solution space. In this problem, different ships possess distinct attributes, such as defense duties, weapon types, and loaded missile quantities. To solve this problem, we proposed an Attention enhanced multi-agent Distributional reinforcement learning method with Dynamic Reward (ADDR). Different from standard reinforcement learning method, ADDR learns to estimate the distribution, as opposed to only the expectation of future return, enabling better adaptation to air defense scenarios with significant randomness. The multi-head attention network integrates both the ship situation and the target situation to appropriately adjust the output of each agent, which explicitly considers the agent-level impact of ships to the whole fleet. Moreover, due to the missile fight time, ships may not immediately receive rewards after executing actions. To address this delayed phenomenon, we designed a dynamic reward mechanism to accurately adjust the delayed rewards. Through extensive simulation experiments, ADDR has demonstrated superior performance over multiple evaluation metrics.},
  keywords={Marine vehicles;Weapons;Discrete wavelet transforms;Heuristic algorithms;Atmospheric modeling;Optimization;Missiles;Multi-ship dynamic weapon-target assignment (MS-DWTA);multi-agent system;distributional reinforcement learning;dynamic reward},
  doi={10.1109/TETCI.2024.3451338},
  ISSN={2471-285X},
  month={April},}@INPROCEEDINGS{10437553,
  author={Hammami, Nessrine and Nguyen, Kim Khoa and Purmehdi, Hakimeh},
  booktitle={GLOBECOM 2023 - 2023 IEEE Global Communications Conference}, 
  title={Attentional Communication for Multi-Agent Distributed Resource Allocation in V2X Networks}, 
  year={2023},
  volume={},
  number={},
  pages={5653-5658},
  abstract={Cooperative multi-agent reinforcement learning (MARL) is a promising solution for many large-scale multi-agent system (MAS) scenarios. A MARL framework is usually based on a decentralized scheme that enables communication between all agents in a given architecture. The agents exchange information to maximize their average reward and increase the overall system performance. However, this decentralized information sharing results in high communication costs, which is a critical issue for environments with limited communication bandwidth. On the other hand, a predefined inter-agent communication architecture may limit potential cooperation. This paper addresses such issues in a vehicle-to-everything (V2X) network, a typical example of MAS with strict Quality of Service (QoS) requirements. For efficient utilization of limited network resources, a solution to the resource-sharing problem between Vehicle to Infrastructure (V2I) and Vehicle to Vehicle (V2V) links is required. We propose a POST-Attentional Communication Actor-Critic (POST-2AC) model that learns when communication is needed and how to integrate shared information for cooperative decision-making. Our learning method uses an attention approach combined with the critic-network to label the agents local information based on its importance so that each agent learns to trade off its performance and communication cost. The simulation results show that the proposed model achieves better performance than the state-of-the-art baselines.},
  keywords={Training;Costs;Vehicle-to-infrastructure;Vehicular ad hoc networks;Quality of service;Computer architecture;Resource management;MARL;Communication;Attention;Resource allocation;V2X},
  doi={10.1109/GLOBECOM54140.2023.10437553},
  ISSN={2576-6813},
  month={Dec},}@INPROCEEDINGS{10480189,
  author={Lu, Ziqing and Liu, Guanlin and Lai, Lifeng and Xu, Weiyu},
  booktitle={2024 58th Annual Conference on Information Sciences and Systems (CISS)}, 
  title={Optimal Cost Constrained Adversarial Attacks for Multiple Agent Systems}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Since many security-related applications use multi-agent reinforcement learning as their underlying algorithms, the study on the adversarial attacks against mutli-agent reinforcement learning systems receives a lot of attention. Finding optimal adversarial attack strategies is an important topic in adversarial attacks on reinforcement learning and the Markov decision process. Previous studies usually assume one all-knowing coordinator (attacker) for whom attacking different recipient (victim) agents incurs uniform costs. However, in important real-world applications, instead of coming from one limitless central attacker, the attacks often need to be performed by distributed attack agents. We formulate the new problem of performing optimal adversarial agent-to-agent attacks using distributed attack agents, in which we impose distinct cost constraints on each different attacker-victim pair. We propose a novel method integrating within-time-step static attack-resource allocation optimization and between-time-step dynamic programming to achieve the optimal adversarial attack in a multi-agent system. Our numerical results show that the proposed attacks can significantly reduce the rewards received by the attacked agents.},
  keywords={Costs;Markov decision processes;Reinforcement learning;Programming;Dynamic programming;Resource management;Optimization;Reinforcement Learning;Markov decision process;adversarial attack;dynamic programming},
  doi={10.1109/CISS59072.2024.10480189},
  ISSN={2837-178X},
  month={March},}@ARTICLE{10787272,
  author={Redondo, Jeffrey and Aslam, Nauman and Zhang, Juan and Yuan, Zhenhui},
  journal={IEEE Transactions on Network Science and Engineering}, 
  title={Multi-Agent Assessment With QoS Enhancement for HD Map Updates in a Vehicular Network and Multi-Service Environment}, 
  year={2025},
  volume={12},
  number={2},
  pages={738-749},
  abstract={Reinforcement Learning (RL) algorithms have been increasingly applied to tackle the complex challenges of offloading in vehicular ad hoc networks (VANETs), particularly in high-density and high-mobility scenarios where network congestion leads to significant latency issues. These challenges are further exacerbated by the introduction of low-latency applications, such as high-definition (HD) Maps, which are compromised in the current IEEE 802.11p standard due to their low-priority classification. In our previous work, we developed a novel coverage-aware Q-learning algorithm using a single-agent approach to address these concerns. However, a key question remains: how does this solution perform when scaled to a larger, more complex environment using a multi-agent system? To address this, our current study evaluates the scalability and effectiveness of the previously developed single-agent Q-learning solution within a distributed multi-agent environment. This multi-agent approach is designed to enhance network performance by leveraging a smaller state and action space across multiple agents. We conduct extensive evaluations through various test cases, considering factors such as reward functions for individual and overall network performance, the number of agents, and comparisons between centralized and distributed learning. The experimental results show that our proposed multi-agent solution significantly reduces time latency in voice, video, HD Map, and best-effort cases by 40.4%, 36%, 43%, and 12%, respectively, compared to the single-agent approach. These findings demonstrate the potential of our solution to effectively manage the challenges of VANETs in dynamic and large-scale environments.},
  keywords={Quality of service;Standards;Vehicle dynamics;Vehicular ad hoc networks;Q-learning;Scalability;Wireless networks;Throughput;Multi-agent systems;Space vehicles;Access category;contention window;high definition map;latency;LiDAR;prioritization},
  doi={10.1109/TNSE.2024.3514744},
  ISSN={2327-4697},
  month={March},}@ARTICLE{9932003,
  author={Zhang, Jiawei and Chang, Cheng and Zeng, Xianlin and Li, Li},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Multi-Agent DRL-Based Lane Change With Right-of-Way Collaboration Awareness}, 
  year={2023},
  volume={24},
  number={1},
  pages={854-869},
  abstract={Lane change is a common-yet-challenging driving behavior for automated vehicles. To improve the safety and efficiency of automated vehicles, researchers have proposed various lane-change decision models. However, most of the existing models consider lane-change behavior as a one-player decision-making problem, ignoring the essential multi-agent properties when vehicles are driving in traffic. Such models lead to deficiencies in interaction and collaboration between vehicles, which results in hazardous driving behaviors and overall traffic inefficiency. In this paper, we revisit the lane-change problem and propose a bi-level lane-change behavior planning strategy, where the upper level is a novel multi-agent deep reinforcement learning (DRL) based lane-change decision model and the lower level is a negotiation based right-of-way assignment model. We promote the collaboration performance of the upper-level lane-change decision model from three crucial aspects. First, we formulate the lane-change decision problem with a novel multi-agent reinforcement learning model, which provides a more appropriate paradigm for collaboration than the single-agent model. Second, we encode the driving intentions of surrounding vehicles into the observation space, which can empower multiple vehicles to implicitly negotiate the right-of-way in decision-making and enable the model to determine the right-of-way in a collaborative manner. Third, an ingenious reward function is designed to allow the vehicles to consider not only ego benefits but also the impact of changing lanes on traffic, which will guide the multi-agent system to learn excellent coordination performance. With the upper-level lane-change decisions, the lower-level right-of-way assignment model is used to guarantee the safety of lane-change behaviors. The experiments show that the proposed approaches can lead to safe, efficient, and harmonious lane-change behaviors, which boosts the collaboration between vehicles and in turn improves the safety and efficiency of the overall traffic. Moreover, the proposed approaches promote the microscopic synchronization of vehicles, which can lead to the macroscopic synchronization of traffic flow.},
  keywords={Behavioral sciences;Safety;Collaboration;Planning;Vehicles;Trajectory;Reinforcement learning;Automated vehicle;lane change;multi-agent deep reinforcement learning;right-of-way collaboration;driving intention},
  doi={10.1109/TITS.2022.3216288},
  ISSN={1558-0016},
  month={Jan},}@INPROCEEDINGS{9429052,
  author={Wafa, Hani'ah and Santoso, Judhi},
  booktitle={2020 7th International Conference on Advance Informatics: Concepts, Theory and Applications (ICAICTA)}, 
  title={Multiagent Simulation On Hide and Seek Games Using Policy Gradient Trust Region Policy Optimization}, 
  year={2020},
  volume={},
  number={},
  pages={1-5},
  abstract={The hide and seek game is a game that implements a multi-agent system so that it will be solved by using multi-agent reinforcement learning. In this research, we examine how to apply policy gradient method, Trust Region Policy Optimization (TRPO) to solve the hide and seek game environment. We also examine the configuration of TRPO algorithm that gives the best performance and its comparison with the Vanilla Policy Gradient (VPG) algorithm. From the results of experiments conducted, we found that in general the VPG algorithm gives a better performance than the TRPO algorithm when tested in the same environment as the training environment. However, on the contrary, the TRPO algorithm gives better performance when tested in a different environment than that in the training environment.},
  keywords={Training;Gradient methods;Games;Reinforcement learning;Informatics;Optimization;Multi-agent systems;reinforcement learning;multi-agent;TRPO;hide and seek},
  doi={10.1109/ICAICTA49861.2020.9429052},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9926587,
  author={Camacho-Gonzalez, Gerardo and D’Avella, Salvatore and Avizzano, Carlo A. and Tripicchio, Paolo},
  booktitle={2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)}, 
  title={A Reinforcement Learning Decentralized Multi-Agent Control Approach exploiting Cognitive Cooperation on Continuous Environments}, 
  year={2022},
  volume={},
  number={},
  pages={1557-1562},
  abstract={Multi-agent system control is a research topic that has broad applications ranging from multi-robot cooperation to distributed sensor networks. Reinforcement learning is shown to be promising as a control strategy in cases where the dynamics of the agents are non-linear, complex, and highly uncertain since it can learn policies from samples without using much model information. The presented manuscript proposes a multi-agent decentralized control approach based on a new multi-agent reinforcement learning setting in which two virtual agents, sharing the same environment, control a single avatar but have access to complementary details necessary to finish the task. Each of them is responsible for solving a portion of the problem, and in order to efficiently solve it, a collaboration should emerge among the virtual agents not to compete but to focus on the final goal. Each virtual agent, performing individually, is not fully autonomous since it does not have a complete vision of the scene and needs the other one to properly command the avatar. The proposed approach proved to be able to solve efficiently constrained navigation problems in two different simulated setups. An actor-critic architecture with a Proximal Policy Optimization (PPO) algorithm has been employed in continuous action and state spaces. The training and the testing have been done in a maze-like environment designed using the StarCraft II Learning Environment.},
  keywords={Training;Navigation;Avatars;Reinforcement learning;Aerospace electronics;Robot sensing systems;Task analysis},
  doi={10.1109/CASE49997.2022.9926587},
  ISSN={2161-8089},
  month={Aug},}@ARTICLE{9195858,
  author={Hao, Jun and Gao, David Wenzhong and Zhang, Jun Jason},
  journal={IEEE Open Access Journal of Power and Energy}, 
  title={Reinforcement Learning for Building Energy Optimization Through Controlling of Central HVAC System}, 
  year={2020},
  volume={7},
  number={},
  pages={320-328},
  abstract={This paper presents a novel methodology to control HVAC system and minimize energy cost on the premise of satisfying power system constraints. A multi-agent architecture based on game theory and reinforcement learning is developed so as to reduce the cost and computational complexity of the microgrid. The multi-agent architecture comprising agents, state variables, action variables, reward function and cost game is formulated. The paper fills the gap between multi-agent HVAC systems control and power system optimization and planning. The results and analysis indicate that the proposed algorithm is beneficial to deal with the problem of “curse of dimensionality” for multi-agent microgrid HVAC system control and speed up learning of unknown power system conditions.},
  keywords={Control systems;Buildings;Open Access;Tin;Microgrids;Game theory;reinforcement learning;multi-agent system;HVAC control;cost minimization},
  doi={10.1109/OAJPE.2020.3023916},
  ISSN={2687-7910},
  month={},}@ARTICLE{9146854,
  author={Sun, Yu and Lai, Jun and Cao, Lei and Chen, Xiliang and Xu, Zhixiong and Xu, Yue},
  journal={IEEE Access}, 
  title={A Novel Multi-Agent Parallel-Critic Network Architecture for Cooperative-Competitive Reinforcement Learning}, 
  year={2020},
  volume={8},
  number={},
  pages={135605-135616},
  abstract={Multi-agent deep reinforcement learning (MDRL) is an emerging research hotspot and application direction in the field of machine learning and artificial intelligence. MDRL covers many algorithms, rules and frameworks, it is currently researched in swarm system, energy allocation optimization, stocking analysis, sequential social dilemma, and with extremely bright future. In this paper, a parallel-critic method based on classic MDRL algorithm MADDPG is proposed to alleviate the training instability problem in cooperative-competitive multi-agent environment. Furthermore, a policy smoothing technique is introduced to our proposed method to decrease the variance of learning policies. The suggested method is evaluated in three different scenarios of authoritative multi-agent particle environment (MPE). Multiple statistical data of experimental results show that our method significantly improves the training stability and performance compared to vanilla MADDPG.},
  keywords={Training;Task analysis;Machine learning;Stability analysis;Games;Network architecture;Learning (artificial intelligence);Multi-agent system;deep reinforcement learning;parallel-critic architecture;training stability},
  doi={10.1109/ACCESS.2020.3011670},
  ISSN={2169-3536},
  month={},}@ARTICLE{8769909,
  author={Nguyen, Duong D. and Rajagopalan, Arvind and Kim, Jijoong and Lim, Cheng-Chew},
  journal={IEEE Access}, 
  title={Adaptive Regret Minimization for Learning Complex Team-Based Tactics}, 
  year={2019},
  volume={7},
  number={},
  pages={103019-103030},
  abstract={This paper presents an approach and analysis for performing decentralized cooperative control of a team of decoys to achieve the Honeypot Ambush tactic. In this tactic, the threats are successfully lured into a designated region where they can be easily defeated. The decoys learn to cooperate by incorporating a game-theory-based online-learning method, known as regret minimization, to maximize the team's global reward. The decoy agents are assumed to have physical limitations and to be subject to certain stringent range constraints required for deceiving the networked threats. By employing an efficient coordination mechanism, the agents learn to be less greedy and allow weaker agents to catch up on their rewards to improve team performance. Such a coordination solution corresponds to achieving convergence to coarse correlated equilibrium. The numerical results verify the effectiveness of the proposed solution to achieve a global satisfaction outcome and to adapt to a wide spectrum of scenarios.},
  keywords={Minimization;Convergence;Task analysis;Reinforcement learning;Training;Bandwidth;Australia;Multi-agent system;cooperative control;online learning;regret minimization},
  doi={10.1109/ACCESS.2019.2930640},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10927107,
  author={Owosho, Yetutude and Olusanya, Olamide and Elegbede, Adedayo},
  booktitle={2024 IEEE 5th International Conference on Electro-Computing Technologies for Humanity (NIGERCON)}, 
  title={Integrating Reinforcement Learning and Multi-Agent Systems for Autonomous Traffic Management in Smart Cities: A Review}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={The recent development in information technology including artificial intelligence, and internet of things is affecting every aspect of human existence. This development has given birth to multi-agent deep learning systems. This article takes a broad look at reinforcement learning in multi-agent systems for autonomous traffic management. Reinforcement learning has found application in intelligent traffic management in areas such as traffic flow prediction, traffic management using internet of things and the operation of autonomous vehicles. Spatio-Temporal Graph Convolutional Networks (STGCN) and Spatio-Temporal Graph Neural Network (STGNN) are deep learning models that have researched for traffic prediction, and Vehicles Ad-hoc Networks (VANETs) for traffic management. Also Convolution Neural Network (CNN) has been researched in the operation of autonomous vehicles. However, there are challenges associated with reinforcement learning and multi-agent systems in autonomous traffic management. These include but not limited to quality of data, assumption usually made during simulations which are not valid in real-life situation and lack of definite safety and intelligibility affordances in current algorithm.},
  keywords={Deep learning;Smart cities;Reviews;Reinforcement learning;Predictive models;Safety;Internet of Things;Information technology;Autonomous vehicles;Multi-agent systems;reinforcement learning;multi-agent system;traffic management;autonomous vehicle},
  doi={10.1109/NIGERCON62786.2024.10927107},
  ISSN={2377-2697},
  month={Nov},}@ARTICLE{9766186,
  author={Zhang, Zilin and Wan, Yanni and Qin, Jiahu and Fu, Weiming and Kang, Yu},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={A Deep RL-Based Algorithm for Coordinated Charging of Electric Vehicles}, 
  year={2022},
  volume={23},
  number={10},
  pages={18774-18784},
  abstract={The development of electric vehicle (EV) industry is facing a series of issues, among which the efficient charging of multiple EVs needs solving desperately. This paper investigates the coordinated charging of multiple EVs with the aim of reducing the charging cost, ensuring a high battery state of charge (SoC), and avoiding the transformer overload. To this end, we first formulate the EV coordinated charging problem with the above multiple objectives as a Markov Decision Process (MDP) and then propose a multi-agent deep reinforcement learning (DRL)-based algorithm. In the proposed algorithm, a novel interaction model, i.e., communication neural network (CommNet) model, is adopted to realize the distributed computation of global information (namely the electricity price, the transformer load, and the total charging cost of multiple EVs). Moreover, different from the most existing works which make specific constraints on the size, the location, or the topology of the distribution network, what we need in the proposed method is only the transformer load. Besides, due to the use of long and short-term memory (LSTM) for price prediction, the proposed algorithm can flexibly deal with various uncertain price mechanisms. Finally, simulations are presented to verify the effectiveness and practicability of the proposed algorithm in a residential charging area.},
  keywords={Transformers;Electric vehicle charging;Costs;Batteries;Power grids;Load modeling;Scalability;Electric vehicle (EV);multi-agent system (MAS);deep reinforcement learning (DRL);coordinated charging scheduling;multi-objective;price prediction},
  doi={10.1109/TITS.2022.3170000},
  ISSN={1558-0016},
  month={Oct},}@INPROCEEDINGS{10240070,
  author={Zheng, Yifan and Xin, Bin and Jiao, Keming and Zhao, Zhixin and Wang, Yuyang and Zhao, Yunming},
  booktitle={2023 42nd Chinese Control Conference (CCC)}, 
  title={Enhanced Multi-Agent Proximal Policy Optimization for Multi-UAV Target Offensive-Defensive Decision}, 
  year={2023},
  volume={},
  number={},
  pages={5986-5991},
  abstract={Autonomous collaborative decision-making is the key technology to achieve large-scale unmanned combat. Focus on the problem of multiple unmanned aerial vehicles' cooperative decision in target offensive and defensive combat, a multi-agent deep reinforcement learning (MADRL) based decision framework is proposed in this paper. Firstly, the simulation environment with a high-fidelity fixed-wing motion model is built. Secondly, to address the issue of high-dimension state space and credit assignment under a multi-agent environment, an enhanced multi-agent proximal policy optimization with mean-field counterfactual advantage (MAPPO _ MFCOA) is proposed. Finally, the results of simulation experiments will verify the performance of the proposed approach.},
  keywords={Training;Deep learning;Decision making;Training data;Collaboration;Reinforcement learning;Autonomous aerial vehicles;multi-UAV combat;multi-agent system;deep reinforcement learning;proximal policy optimization},
  doi={10.23919/CCC58697.2023.10240070},
  ISSN={1934-1768},
  month={July},}@INPROCEEDINGS{9892794,
  author={Chen, Yiqun and Yang, Wei and Zhang, Tianle and Wu, Shiguang and Chang, Hongxing},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Commander-Soldiers Reinforcement Learning for Cooperative Multi-Agent Systems}, 
  year={2022},
  volume={},
  number={},
  pages={1-7},
  abstract={In ball sports, such as basketball, the coach can guide players to better offend and defend from a holistic perspective to win the game. Inspired by such scenarios, we introduce a coach-like concept into the decision-making process of cooperative multi-agent systems. We propose a new framework Commander-Soldiers Reinforcement Learning (CSRL), for Multi-Agent systems. Specifically, we introduce a virtual role, Commander, which can obtain and encode global information every T steps and send the encoded global guidance to Soldiers (real agents). Furthermore, we propose Policy Guidance Network (PGN), which can customize the encoded global guidance from Commander based on observations for each Soldier, providing each Soldier with specified guidance to the decision-making process. The Soldier takes into account not only the local action-observation histories but also the specified guidance from PGN when making decisions. We validate CSRL on the challenging StarCraft II micromanagement benchmark, proving that our approach can take advantage of intermittent global information to improve collaborative performance.},
  keywords={Decision making;Neural networks;Collaboration;Reinforcement learning;Games;Benchmark testing;History;Reinforcement Learning;Multi-Agent System;Multi-Agent Cooperation},
  doi={10.1109/IJCNN55064.2022.9892794},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{9892747,
  author={Meng, Linghui and Ruan, Jingqing and Xing, Dengpeng and Xu, Bo},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Learning in Bi-level Markov Games}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Although multi-agent reinforcement learning (MARL) has demonstrated remarkable progress in tackling sophisticated cooperative tasks, the assumption that agents take simultaneous actions still limits the applicability of MARL for many real-world problems. In this work, we relax the assumption by proposing the framework of the bi-level Markov game (BMG). BMG breaks the simultaneity by assigning two players with a leader-follower relationship in which the leader considers the policy of the follower who is taking the best response based on the leader's actions. We propose two provably convergent algorithms to solve BMG: BMG-1 and BMG-2. The former uses the standard Q-learning, while the latter relieves solving the local Stackelberg equilibrium in BMG-1 with the further two-step transition to estimate the state value. For both methods, we consider temporal difference learning techniques with both tabular and neural network representations. To verify the effectiveness of our BMG framework, we test on a series of games, including Seeker, Cooperative Navigation, and Football, that are challenging to existing MARL solvers find challenging to solve: Seeker, Cooperative Navigation, and Football. Experimental results show that our BMG methods achieve competitive advantages in terms of better performance and lower variance.},
  keywords={Q-learning;Navigation;Neural networks;Games;Markov processes;Nash equilibrium;Task analysis;Reinforcement Learning;Multi-Agent System;Leader-Follower},
  doi={10.1109/IJCNN55064.2022.9892747},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{11033623,
  author={Chen, Shaobin and Ma, Shaocong and Wang, Liang and Tao, Xianping and Hu, Hao},
  booktitle={2025 28th International Conference on Computer Supported Cooperative Work in Design (CSCWD)}, 
  title={Time-Critical Cooperative Delivery with Unknown Demands}, 
  year={2025},
  volume={},
  number={},
  pages={1575-1580},
  abstract={This paper studies the Time-Critical Cooperative Delivery with Unknown Demands (TCDUD) problem, developed from the well-studied Capacitated Vehicle Routing Problem with Stochastic Demands (CVRPSD), that corresponds to emergency situations requiring 1) time-critical delivery; 2) unknown demands; and 3) cooperative delivery. We formulate the problem as a multi-agent sequential decision problem with a Collaborative Semi-Markov Decision Process (CSMDP) model in which timecritical delivery is urged by the reward function that takes both the amount and the time of fulfilled demands into account. Unlike traditional CVRPSD, we do not include a priori information about a customer's demand in problem definition, and require vehicles to visit a customer before its demand is revealed. A Transformer-based multi-agent reinforcement learning approach, namely MAODN, is devised to learn online policies that direct the vehicles to visit the customers, and perform timely delivery in a cooperative manner. MAODN utilizes a demand updater to accommodate online updates about customers' demands from the vehicles during delivery. Vehicles then make cooperative decisions via individual policy networks, leveraging the fleet state provided by the state aggregator. Experiment results suggest that our approach outperforms the baseline by at least 27.8% and demonstrates better robustness.},
  keywords={Federated learning;Markov decision processes;Vehicle routing;Stochastic processes;Reinforcement learning;Transformers;Robustness;Real-time systems;Time factors;Multi-agent systems;multi-agent system;Markov Decision Process},
  doi={10.1109/CSCWD64889.2025.11033623},
  ISSN={2768-1904},
  month={May},}@INPROCEEDINGS{10191313,
  author={Du, Yunfei and Wang, Yin and Cong, Ya and Jiang, Weihao and Pu, Shiliang},
  booktitle={2023 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Evolution Strategies Enhanced Complex Multiagent Coordination}, 
  year={2023},
  volume={},
  number={},
  pages={1-9},
  abstract={Multi-agent coordination involves both the individual reward and team reward, where the former guides the agent to learn basic skills and the latter measures how well such a team cooperatively completes final tasks. However, in many complex scenarios, these two aspects can be contradictory, due to that one agent excessively pursuing its own profits may suppress the performance of other teammates and lead to the reduction of overall profits. Besides, such dual rewards are generally entangled which make the learning swing between optimizing either the former or latter, which further leads to the sub-optimal and unstable solutions. Moreover, the sparse reward problem commonly encountered in the multi-agent system would further exacerbate this contradiction. In the present work, we address these challenges by proposing CEMARL, a novel framework combining cross-entropy method (CEM) and off-policy multi-agent reinforcement learning (MARL). CEM is gradient-free and learns from the whole episode, whereas MARL is gradient-based and learns from the experiences of agents. The core idea behind CEMARL is that it explicitly decomposes the individual reward and team reward, and deals with them through gradient-based learning and gradient-free evolution, respectively. By means of this, it can simultaneously maximize the individual and team reward, and reconciles the contradiction between individual and team as well as the sparse reward problem. CEMARL shows both conciseness in framework and stability in training, and achieves significantly better performances than state-of-the-art baselines on a range of complex tasks.},
  keywords={Training;Sociology;Neural networks;Reinforcement learning;Task analysis;Statistics;Multi-agent systems;Multi-agent coordination;Reinforcement Learning;Evolution Strategies},
  doi={10.1109/IJCNN54540.2023.10191313},
  ISSN={2161-4407},
  month={June},}@ARTICLE{9580469,
  author={Ibrahim, Mostafa and Hashmi, Umair Sajid and Nabeel, Muhammad and Imran, Ali and Ekin, Sabit},
  journal={IEEE Transactions on Network and Service Management}, 
  title={Embracing Complexity: Agent-Based Modeling for HetNets Design and Optimization via Concurrent Reinforcement Learning Algorithms}, 
  year={2021},
  volume={18},
  number={4},
  pages={4042-4062},
  abstract={Complexity is an inherent property in wireless heterogeneous networks (HetNets). In this paper, we investigate the application of the agent-based modeling (ABM) tool for optimization of complex and dynamic HetNets. The proposed framework contains a diversity of game-theoretic, machine learning, and rule-based algorithms within the same model. We present and analyze a HetNet ABM model that runs parallel reinforcement learning (RL) algorithms for spectrum deployment, interference management, resource allocation, and load balancing at both micro and macrocell levels. In our proposed model, two RL-based algorithms work jointly to manage the co-tier and cross-tier interferences. The macrocell runs the first algorithm to control the transmission power of the small cells. The second RL algorithm is run by small cells to assign the users to the sub-bands with less interference levels. Simultaneously, the user association is decided by the users depending on the available resources at the cells and user preferences. The model is then evaluated under various network load conditions to deduce relationships between the cell loads, aggregate bit rate, latency, and user association. Moreover, the system is assessed in a dynamic network scenario with moving users and is confirmed to possess the ability to attain convergence with sufficient performance levels.},
  keywords={Interference;Load modeling;Resource management;Macrocell networks;Analytical models;Adaptation models;Optimization;HetNets;complexity;agent-based-modeling;multi-agent-systems;5G and beyond},
  doi={10.1109/TNSM.2021.3121282},
  ISSN={1932-4537},
  month={Dec},}@INPROCEEDINGS{10963160,
  author={Chen, Bo and Ding, Shiqi and Lin, Yukun and Chen, Guohong},
  booktitle={2025 IEEE 17th International Conference on Computer Research and Development (ICCRD)}, 
  title={Multi-agent Simulation of Complex Economic Systems Driven by Deep Reinforcement Learning}, 
  year={2025},
  volume={},
  number={},
  pages={47-54},
  abstract={In economics, the study of macroeconomic laws often encounters challenges such as difficulties in data collection and low data quality. A complex economic system (CES) simulation model based on Multi-agent Simulation (MAS) was designed and proposed to provide reliable, high-quality virtual data support for economic research to address these issues. To enhance the intelligence of agent behaviors in the simulation model, an economic agent decision-making model based on the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm was developed, and experimental results demonstrate the effectiveness of the TD3 algorithm in CES simulations. Additionally, the model's decision-making capability in complex economic environments was further improved by incorporating Long Short-term Memory (LSTM) networks to handle temporal relationships. The research presented herein explores the application effects of deep reinforcement learning in economic research, providing a reliable source of virtual economic data for theoretical economic research and offering new perspectives and methods for economic system simulation research.},
  keywords={Economics;Computational modeling;Decision making;Reliability theory;Deep reinforcement learning;Reliability engineering;Data models;Systems simulation;Long short term memory;Research and development;deep reinforcement learning;TD3;LSTM;Complex economic system simulation;Multi-agent reinforcement learning},
  doi={10.1109/ICCRD64588.2025.10963160},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{9992346,
  author={Heredia, Paulo and George, Jemin and Mou, Shaoshuai},
  booktitle={2022 IEEE 61st Conference on Decision and Control (CDC)}, 
  title={Distributed Offline Reinforcement Learning}, 
  year={2022},
  volume={},
  number={},
  pages={4621-4626},
  abstract={In this work, we explore the problem of offline reinforcement learning for a multi-agent system. Offline reinforcement learning differs from classical online and off-policy reinforcement learning settings in that agents must learn from a fixed and finite dataset. We consider a scenario where there exists a large dataset produced by interactions between an agent an its environment. We suppose the dataset is too large to be efficiently processed by an agent with limited resources, and so we consider a multi-agent network that cooperatively learns a control policy. We present a distributed reinforcement learning algorithm based on Q-learning and an approach called offline regularization. The main result of this work shows that the proposed algorithm converges in the sense that the norm squared error is asymptotically bounded by a constant, which is determined by the number of samples in the dataset. In the simulation, we have implemented the proposed algorithm to train agents to control both a linear system and a nonlinear system, namely the well-known cartpole system. We provide simulation results showing the performance of the trained agents.},
  keywords={Linear systems;Q-learning;Simulation;Process control;Safety;Nonlinear systems;Time-varying systems},
  doi={10.1109/CDC51059.2022.9992346},
  ISSN={2576-2370},
  month={Dec},}@INPROCEEDINGS{10128078,
  author={Gao, Zhenkun and Dai, Xiaoyan and Yao, Meibao and Xiao, Xueming},
  booktitle={2023 IEEE 6th International Conference on Industrial Cyber-Physical Systems (ICPS)}, 
  title={A Data Enhancement Strategy for Multi-Agent Cooperative Hunting based on Deep Reinforcement Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Cooperative hunting is a typical and significant scene to study multi-agent behaviors, where conventional control strategies are difficult to cope with, due to its high dimensionality of state space and locality of communication. Reinforcement learning provides a framework and a set of tools for this issue by trial-and-error interactions with the environment. Though promising, it often requires a large number of empirical sample data to learn effective hunting strategies, leading to low sample efficiency, understood as the training episodes required for the agent to learn effective behavior strategies. To improve the sampling efficiency, we propose a data enhancement strategy integrated in the execution (CTDE) training framework to train the multi-agent system. The data enhancement strategy is based on a state transfer dynamics model to generate additional predicted data, which we called dynamic prediction model, combined with the empirical data by interacting with the environment, for higher sample efficiency. The simulation results on the Webots platform show that our method outperforms some state-of-the-art methods, such as MAPPO, with high data sample efficiency.},
  keywords={Training;Service robots;Simulation;Training data;Reinforcement learning;Predictive models;Data models;Reinforcement learning;Multi robot systems;Data enhancement;Multi robot hunting},
  doi={10.1109/ICPS58381.2023.10128078},
  ISSN={2769-3899},
  month={May},}@ARTICLE{10720855,
  author={Shi, Juan and Liu, Chen and Liu, Jinzhuo},
  journal={IEEE Transactions on Network Science and Engineering}, 
  title={Hypergraph-Based Model for Modeling Multi-Agent Q-Learning Dynamics in Public Goods Games}, 
  year={2024},
  volume={11},
  number={6},
  pages={6169-6179},
  abstract={Modeling the learning dynamic of multi-agent systems has long been a crucial issue for understanding the emergence of collective behavior. In public goods games, agents interact in multiple larger groups. While previous studies have primarily focused on infinite populations that only allow pairwise interactions, we aim to investigate the learning dynamics of agents in a public goods game with higher-order interactions. With a novel use of hypergraphs for encoding higher-order interactions, we develop a formal model (a Fokker-Planck equation) to describe the temporal evolution of the distribution function of Q-values. Noting that early research focused on replicator models to predict system dynamics failed to accurately capture the impact of hyperdegree in hypergraphs, our model effectively maps its influence. Through experiments, we demonstrate that our theoretical findings are consistent with the agent-based simulation results. We demonstrated that as the number of groups an agent is involved in reaches a certain scale, the learning dynamics of the system evolve to resemble those of a well-mixed population. Furthermore, we demonstrate that our model offers insights into algorithmic parameters, such as the Boltzmann temperature, facilitating parameter tuning.},
  keywords={Q-learning;Mathematical models;Heuristic algorithms;Biological system modeling;Multi-agent systems;Game theory;Standards;Topology;System dynamics;Game theory;hypergraph;multi-agent system;Q-learning},
  doi={10.1109/TNSE.2024.3473941},
  ISSN={2327-4697},
  month={Nov},}@INPROCEEDINGS{11161906,
  author={Yuan, Guoling and Lin, Mingduo and Zhao, Bo},
  booktitle={2025 IEEE International Annual Conference on Complex Systems and Intelligent Science (CSIS-IAC)}, 
  title={Adaptive Optimal Tracking Control for Large-Scale Multi-Agent Systems with Input Constraints: A Federated Learning Approach}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper investigates optimal tracking control problem of large-scale multi-agent systems (LSMAS) with input constraints via federated learning-based adaptive dynamic programming (FL-ADP) from the perspective of mean-field games (MFG). To solve difficulty of “curse of dimensionality” caused by the increasing number of agents, the MFG is introduced. For dealing with the input constraints, a discounted cost function that includes the tracking error, the mean-field coupling term and a non-quadratic term of the control input, is designed. By developing a critic-mass algorithm with two time-varying neural networks (NNs), the optimal cost function and the probability density function (PDF) of each agent are estimated to obtain the optimal control policy. To alleviate the negative effects of the estimation deviations caused by using local information only, a FL-ADP method is proposed by adding suitable communication, which improves the tracking control performance. Finally, simulation example illustrates the effectiveness of the developed method.},
  keywords={Federated learning;Optimal control;Estimation;Artificial neural networks;Games;Probability density function;Cost function;Mathematical models;Dynamic programming;Multi-agent systems;Adaptive dynamic programming;multi-agent system;optimal control;tracking control;mean-field games;federated learning},
  doi={10.1109/CSIS-IAC65538.2025.11161906},
  ISSN={},
  month={May},}@ARTICLE{10620210,
  author={Karaki, Anas and Al-Fagih, Luluwah},
  journal={IEEE Access}, 
  title={Evolutionary Game Theory as a Catalyst in Smart Grids: From Theoretical Insights to Practical Strategies}, 
  year={2024},
  volume={12},
  number={},
  pages={186926-186940},
  abstract={The increase in the installation of distributed energy resources (DERs) globally has led to a remarkable transformation in the structure of smart grids due to the growing number of energy participants. Recently, electricity markets (EM) have received substantial attention as a viable solution for the complex issue of managing DERs. Modeling the power grid as a complex system of interacting components facilitates investigating the interaction among electricity producers and consumers to maintain the total generation and demand at a balance. In this work, we present a review of the recent advances in adopting evolutionary game theory (EGT), to mitigate challenges in the emerging smart grid, as a decision-making framework for trading dynamics and considering large populations. It includes a taxonomy of various EGT applications in energy trading dynamics, DER management, and policy and infrastructure development. Finally, the linkage between multi-agent reinforcement learning (MARL) and EGT is provided, highlighting their mathematical parallels in the context of smart grid applications.},
  keywords={Statistics;Social factors;Smart grids;Reviews;Power system dynamics;Games;Game theory;Electricity supply industry;Market research;Reinforcement learning;Multi-agent systems;Game theory;evolutionary games;electricity markets;reinforcement learning;multi-agent system;dynamic populations},
  doi={10.1109/ACCESS.2024.3436935},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{8899448,
  author={Liang, Chao and Shanmugam, Bharanidharan and Azam, Sami and Jonkman, Mirjam and Boer, Friso De and Narayansamy, Ganthan},
  booktitle={2019 International Conference on Vision Towards Emerging Trends in Communication and Networking (ViTECoN)}, 
  title={Intrusion Detection System for Internet of Things based on a Machine Learning approach}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={With the application of Internet of Things technology to every aspect of life, the potential damage caused by Internet of things attacks is more serious than for traditional network attacks. Traditional intrusion detection systems do not serve the network environment of the IoT very well, so it is important to study intrusion detection systems suitable for the network environment of the Internet of Things. Researchers have found that the combination of machine learning technologies with an intrusion detection system is an effective way to resolve the drawbacks traditional IDSs have when they are used for IoT. This research involves the design of a novel intrusion detection system and the implementation and evaluation of its analysis model. This new intrusion detection system uses a hybrid placement strategy based on a multi-agent system. The new system consists of a data collection module, a data management module, an analysis module and a response module. For the implementation of the analysis module, this research applies a deep neural network algorithm for intrusion detection. The results demonstrate the efficiency of deep learning algorithms for detecting attacks from the transport layer. Compared with traditional detection methods used in IDSs, the analysis indicates that deep learning algorithms are more suitable for intrusion detection in an IoT network environment.},
  keywords={Intrusion detection;Internet of Things;Blockchain;Multi-agent systems;Reinforcement learning;Component;Intrusion Detection System;IoT;Machine Learning;Multi-agent system;Blockchain;Cybersecurity},
  doi={10.1109/ViTECoN.2019.8899448},
  ISSN={},
  month={March},}@INPROCEEDINGS{8078952,
  author={Kristensen, Terje and Ezeora, Nnamdi Johnson},
  booktitle={2017 IEEE International Conference on Information and Automation (ICIA)}, 
  title={Simulation of intelligent traffic control for autonomous vehicles}, 
  year={2017},
  volume={},
  number={},
  pages={459-465},
  abstract={Urban cities are getting more congested with vehicular traffic and most of the traffic control systems are not smart to detect and give priority to emergency vehicles. The effect results to inadequate services delivered by the public emergency agencies, and unnecessary traffic congestion to other road users at intersection points. In this paper we present an effective reinforced road traffic control policy that reduces the waiting time of emergency vehicles at road intersection points, and also reduce the travel time of other vehicles. The work involved simulation of traffic control at an intersection using a multi-agent system development framework (JADE) and an agent-based traffic simulator (SUMO). In order to exploit the full potential of SUMO, a third tool (TraSMAPI) was used to connect JADE and SUMO and also, to provide a higher abstraction of SUMO. In this way, the simulation is not restricted to only what SUMO can offer, but also permits us to control and manipulate the behaviour of the simulation runs. The result shows that intelligent traffic control fulfils its objectives by significantly reducing the travel time of emergency vehicles or other normal vehicles. It is shown in the paper that intelligent traffic control is at least 96% better in reducing the waiting time of emergency vehicles at intersections than nonintelligent fixed traffic control.},
  keywords={Roads;Multi-agent systems;Learning (artificial intelligence);Protocols;Intelligent agents;Green products;Heuristic algorithms;SUMO;intersection;traffic light;multi-agent system;emergency vehicles},
  doi={10.1109/ICInfA.2017.8078952},
  ISSN={},
  month={July},}@INPROCEEDINGS{8946043,
  author={Xiao, Yanbing and Zhang, Yingzhou and Sun, Yuxin and Qian, Junyan},
  booktitle={2019 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery (CyberC)}, 
  title={Multi-UAV Formation Transformation Based on Improved Heuristically-Accelerated Reinforcement Learning}, 
  year={2019},
  volume={},
  number={},
  pages={341-347},
  abstract={Addressing the shortcomings of the classical reinforcement learning algorithms in solving the problem of multi-UAV formation transformation, such as large consumption of computing resources and slow convergence speed, this paper introduces heuristics and uses back propagation algorithms to construct a heuristic function required for multi-UAV formation convergence. When designing the action selection function of reinforcement learning algorithms, the idea of simulated annealing is also introduced, which enables the algorithm to fully explore the combination of convergence actions in the early stage so as to jump out of the local optimal trap, while ensuring the convergence of the algorithm in the later stage. The data of the final simulation experiment show that the improved heuristically-accelerated reinforcement learning algorithms can effectively reduce the consumption of UAV computing resources, improve the convergence speed the convergence quality when dealing with the problem of multi-UAV formation transformation.},
  keywords={Handheld computers;Conferences;Distributed computing;Knowledge discovery;UAV;multi-agent system;reinforcement learning;heuristic function;simulated annealing},
  doi={10.1109/CyberC.2019.00065},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10154436,
  author={Ghnaya, Imed and Mosbah, Mohamed and Aniss, Hasnaâ and Ahmed, Toufik},
  booktitle={NOMS 2023-2023 IEEE/IFIP Network Operations and Management Symposium}, 
  title={Multi-Agent Advantage Actor-Critic Learning For Message Content Selection in Cooperative Perception Networks}, 
  year={2023},
  volume={},
  number={},
  pages={1-9},
  abstract={Recent advancements in autonomous vehicle perception haveexposed limitations of onboard sensors such as radar, lidar, and cameras, which road obstacles and adverse weather conditions can impede. Connected and Autonomous Vehicles (CAVs) are leveraging wireless communications to share perception information through a process called Cooperative Perception (CP), aiming to provide a more comprehensive understanding of their environment. However, this can result in excessive redundant and useless information in the network, as the same road objects may be detected and exchanged simultaneously by multiple CAVs. This not only consumes more network resources but also may overload the communication channel, reducing the delivery of perception information to CAVs and ultimately decreasing the overall CP awareness in the network. This paper introduces MCORM, a multi-agent learning method based on the advantage actor-critic algorithm to maximize object usefulness and reduce redundancy in the network. Our evaluations demonstrate that through this method, CAVs learn optimal CP message content selection policies that maximize usefulness. Further more, our proposal proves to be more effective in mitigating object redundancy and improving network reliability in comparison to existing approaches.},
  keywords={Learning systems;Wireless communication;Meteorological radar;Roads;Redundancy;Sensors;Proposals;connected and autonomous vehicles;cooperative perception;redundancy mitigation;multi-agent system;reinforcement learning;advantage actor-critic},
  doi={10.1109/NOMS56928.2023.10154436},
  ISSN={2374-9709},
  month={May},}@INPROCEEDINGS{8080423,
  author={Grossi, Gina and Ross, Brian},
  booktitle={2017 IEEE Conference on Computational Intelligence and Games (CIG)}, 
  title={Evolved communication strategies and emergent behaviour of multi-agents in pursuit domains}, 
  year={2017},
  volume={},
  number={},
  pages={110-117},
  abstract={This study investigates how genetic programs can be effectively used in a multi-agent system to allow agents to learn to communicate. Using the pursuit domain and a co-operative learning strategy, communication protocols are compared as multiple predator agents learn the meaning of commands in order to achieve their common goal of first finding and then tracking prey. The outcome of this study reveals a general synchronization behaviour emerging from simple message passing among agents. An additional outcome shows a learned behaviour in the best result which resembles the behaviour of guards and reinforcements that can be found in popular stealth video games.},
  keywords={Protocols;Games;Message passing;Communication channels;Robots;Conferences;Computational intelligence;multi-agent system;pursuit domain;communication;co-operative learning;emergent behaviour;video games},
  doi={10.1109/CIG.2017.8080423},
  ISSN={2325-4289},
  month={Aug},}@INPROCEEDINGS{8260034,
  author={Khalil, Khaled M. and Abdel-Aziz, M. and Nazmy, Taymour T. and Salem, Abdel-Badeeh M.},
  booktitle={2017 Eighth International Conference on Intelligent Computing and Information Systems (ICICIS)}, 
  title={Multiply-sectioned Bayesian network for multi-agent learning based meta resources scheduling in CloudSim}, 
  year={2017},
  volume={},
  number={},
  pages={185-191},
  abstract={The interest in Cloud Computing is growing significantly. This is for the application requirements to quickly access more resources to expand as needed. Collecting real time data about Cloud resources and finding the best fit resources to user requests are time and space consuming operation. Resources can be added/removed at any time and Cloud Computing System should utilize available resources and schedule user requests within specific time for execution. Considering such dynamic and complex domain, it is important to setup global policy among resources. The global policy can be applied through resources meta-scheduling. Multiply-Sectioned Bayesian Network (MSBN) provides a natural framework for such metaphor. We propose using adaptive Multi-Agent System (MAS) based on Multiply-Sectioned Bayesian Network (MSBN) for the function of meta-scheduling in Cloud Computing Systems. Random variables are representing the uncertainties in workload and resource's attributes. Each set of identical hosts in the Cloud System are grouped as subdomains, where set of agents have partial knowledge about the global system and local observations of hosts in the subdomain. Then, agents work to estimate the state of the local hosts and act accordingly to provide probability of assigning available resources to the user request. In addition, agents need to communicate their beliefs to other agents to improve the overall meta-scheduling performance. Agent with the strongest belief will get the request assigned for local resources scheduling and execution. If the agent failed to satisfy the request needs then the agent with the next strongest belief is assigned and so on. We present our proposed model by extending CloudSim simulation. Validation of the proposed model and improvement in scheduling of Virtual Machines are presented.},
  keywords={Conferences;Information systems;Multi-Agent System;Multiply-Sectioned Bayesian Network;Meta-Scheduling;Agent-Based Learning;CloudSim},
  doi={10.1109/INTELCIS.2017.8260034},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10001584,
  author={Jia, Jiekai and Tahir, Anam and Koeppl, Heinz},
  booktitle={GLOBECOM 2022 - 2022 IEEE Global Communications Conference}, 
  title={Decentralized Coordination in Partially Observable Queueing Networks}, 
  year={2022},
  volume={},
  number={},
  pages={1491-1496},
  abstract={We consider communication in a fully cooperative multi-agent system, where the agents have partial observation of the environment and must act jointly to maximize the overall reward. We have a discrete-time queueing network where agents route packets to queues based only on the partial information of the current queue lengths. The queues have limited buffer capacity, so packet drops happen when they are sent to a full queue. In this work, we implemented a communication channel for the agents to share their information in order to reduce the packet drop rate. For efficient information sharing we use an attention-based communication model, called ATVC, to select informative messages from other agents. The agents then infer the state of queues using a combination of the variational autoencoder, VAE, and product-of-experts, PoE, model. Ultimately, the agents learn what they need to communicate and with whom, instead of communicating all the time with everyone. We also show empirically that ATVC is able to infer the true state of the queues and leads to a policy which outperforms existing baselines.},
  keywords={Training;Heating systems;Scalability;Information sharing;Communication channels;Color;Servers;queueing network;reinforcement learning;multi-agent system;communication},
  doi={10.1109/GLOBECOM48099.2022.10001584},
  ISSN={2576-6813},
  month={Dec},}@INPROCEEDINGS{10001147,
  author={Hou, Xuewei and Chen, Lixing and Tang, Junhua and Li, Jianhua},
  booktitle={GLOBECOM 2022 - 2022 IEEE Global Communications Conference}, 
  title={Multi-Agent Learning Automata for Online Adaptive Control of Large-Scale Traffic Signal Systems}, 
  year={2022},
  volume={},
  number={},
  pages={1497-1502},
  abstract={Adaptive traffic control systems are gaining attention in recent years as traditional hand-crafted traffic control experiences performance fall-offs with increasingly complicated metropolitan traffic patterns. This paper studies a learning automata (LA)-based traffic signal control scheme that adapts to real-time traffic patterns and optimizes traffic flows by dynamically changing the green split timings. A novel LA algorithm, called K-Neighbor Multi-Agent Learning Automata (KN-MALA), is proposed to learn the optimal decision online and adjust the traffic light accordingly in an attempt to minimize the overall waiting time at an intersection. In particular, KN-MALA employs an online distributed learning framework that integrates the traffic condition of neighboring intersections to efficiently learn and infer optimal decisions for large-scale traffic signal systems. Furthermore, a parameter insensitive update mechanism is designed for KN-MALA to overcome the instability caused by initialization variations. Experiments are conducted on real-world traffic patterns of Sioux Falls City and the performance of the proposed algorithm is compared with the pre-timed traffic light control scheme and an adaptive traffic light control scheme based on single-agent learning automata. The results show that the proposed algorithm outperforms the other schemes in terms of quick traffic clearance under various traffic patterns and initial conditions.},
  keywords={Learning automata;Heuristic algorithms;Roads;Urban areas;Traffic control;Real-time systems;Timing;traffic signal control;learning automata;multi-agent system},
  doi={10.1109/GLOBECOM48099.2022.10001147},
  ISSN={2576-6813},
  month={Dec},}@INPROCEEDINGS{10240132,
  author={Liu, Ziyi and Fang, Yongchun},
  booktitle={2023 42nd Chinese Control Conference (CCC)}, 
  title={Learning Diverse Control Strategies for Simulated Humanoid Combat via Motion Imitation and Multi-Agent Self-Play}, 
  year={2023},
  volume={},
  number={},
  pages={5595-5600},
  abstract={Human athletes can coordinate their bodies to achieve exquisite strategies with agile and diverse motions in multiplayer competitions, which is a long-standing challenge for robotics and physically embodied artificial agents. In this paper, we propose a hierarchical learning framework that generates diverse control strategies and agile motion for physically simulated humanoids, which can be divided into two stages: learning basic motion skills and learning high-level competitive strategies. The framework decouples low-level control and high-level strategy learning, where the low-level policy is trained via motion imitation and skill discovery objectives to generate agile motion skills, and the high-level policy is trained via prioritized fictitious self-play to generate diverse competitive strategies. Furthermore, we develop and release the world's first physically simulated humanoid combat environment. We evaluate our learning framework in this environment, and experimental results demonstrate that policies learned by our framework can generate both agile motion skills and diverse long-term competitive strategies.},
  keywords={Deep learning;Robot kinematics;Humanoid robots;Reinforcement learning;Aerospace electronics;Control systems;Behavioral sciences;Multi-Agent System;Motion Control;Hierarchical Reinforcement Learning},
  doi={10.23919/CCC58697.2023.10240132},
  ISSN={1934-1768},
  month={July},}@INPROCEEDINGS{10016412,
  author={Yilin, Liu and Jinglin, Li},
  booktitle={2022 IEEE 8th International Conference on Cloud Computing and Intelligent Systems (CCIS)}, 
  title={The Intersection of Companions: Differential Traffic Signal Control in Multi-Agent Systems}, 
  year={2022},
  volume={},
  number={},
  pages={181-185},
  abstract={Traffic signal control (TSC) is a classic application scenario in multi-agent systems. There are thousands of traffic lights in a city. In the training of a multi-agent system, it takes a lot of resources to make a model for each agent, and the training is very difficult. At present, most schemes use only one model for all agents and obtain actions by inputting the current state of the agent. Such a method in the actual intersection will be difficult to reflect on their differences. Therefore, we propose a method to classify intersections. GCN and Q-learning are combined to find its partner for the same type of intersection and share a model. Different models are used for different types of intersections. This not only maintains the difference between intersections, but also saves the cost of resources. The experiment proves that our scheme is superior.},
  keywords={Training;Cloud computing;Q-learning;Costs;Urban areas;Control systems;Intelligent systems;Agents and multi-agent systems;Deep learning;Smart city;Traffic light control},
  doi={10.1109/CCIS57298.2022.10016412},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9838808,
  author={Wang, Zunliang and Yao, Haipeng and Mai, Tianle and Xiong, Zehui and Yu, F. Richard},
  booktitle={ICC 2022 - IEEE International Conference on Communications}, 
  title={Cooperative Reinforcement Learning Aided Dynamic Routing in UAV Swarm Networks}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={The Unmanned Aerial Vehicle (UAV) swarm has attracted widespread attention from both academia and industry. It has been widely adopted in disaster recovery, military communication, agricultural production, and industrial automation. In critical situations or places where communication infrastructure is lacking, deploying a UAV swarm network is a cost-effective solution. However, considering the high speed of UAV devices, designing an effective routing mechanism has been a challenging problem. In this paper, enlightened by the recent success of multi-agent reinforcement learning, we propose a multi-agent policy gradients-based UAV routing algorithm. We adopt a centralized training and decentralized executing framework, where a centralized training platform is implemented to guide the policy updating of each UAV node. Moreover, we introduce a counterfactual baseline scheme in our algorithm to improve the convergence speed. Extensive simulation results validate the effectiveness of the proposed algorithms compared to the state-of-the-art schemes.},
  keywords={Training;Military communication;Heuristic algorithms;Simulation;Reinforcement learning;Production;Autonomous aerial vehicles;UAV;dynamic routing;multi-agent system;reinforcement learning},
  doi={10.1109/ICC45855.2022.9838808},
  ISSN={1938-1883},
  month={May},}@INPROCEEDINGS{10651091,
  author={Xie, Jing and Zhang, Yongjun and Ouyang, Qianying and Yang, Huanhuan and Dong, Fang and Shi, Dianxi and Jin, Songchang},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Improved Communication and Collision-Avoidance in Dynamic Multi-Agent Path Finding}, 
  year={2024},
  volume={},
  number={},
  pages={1-9},
  abstract={Multi-Agent Path Finding (MAPF) is a classic problem with a wide range of applications. To cope with more complex situations in reality, Dynamic MAPF (DMAPF) has received much attention. The existing DMAPF definition lacks completeness or considers too simple situations. In this paper, we comprehensively model DMAPF based on realistic scenarios. Consequently, dynamic scenarios bring many problems. The dynamics of agent tasks bring the problem of more difficult coordination and cooperation of the multi-agent system, and the dynamics of obstacles bring the problem of increased collisions. To address these problems, this paper proposes a fully decentralised multi-agent reinforcement learning method CO3, which uses COmmon knowledge in selective COmmunication and proposes obstacle COllision avoidance mechanism. Firstly, common knowledge for communication improves cooperation between agents, which improves system performance and reduces collisions between agents. Secondly, the obstacle collision avoidance mechanism consists of a collision avoidance helper module and a critical region. The collision avoidance helper module improves the agents’ alertness to nearby obstacles, and the critical region gives an early warning to the agents to beware of distant obstacles. The obstacle collision avoidance mechanism can effectively reduce collisions between agents and obstacles. Finally, experiments show that CO3 can solve the DMAPF problem quite well, and the number of collisions is significantly lower than other learning-based methods in a dynamic environment.},
  keywords={Learning systems;Adaptation models;System performance;Neural networks;Reinforcement learning;Collision avoidance;Multi-agent systems;multi-agent path finding;dynamic environment;deep reinforcement learning;communication;collision avoidance},
  doi={10.1109/IJCNN60899.2024.10651091},
  ISSN={2161-4407},
  month={June},}@ARTICLE{10757340,
  author={Ning, Chengyu and Lu, Guoming and He, Xuewan and Chen, Aiguo and Luo, Guangchun},
  journal={IEEE Transactions on Consumer Electronics}, 
  title={Learning Communication With Limited Range in Multi-Agent Cooperative Tasks}, 
  year={2024},
  volume={},
  number={},
  pages={1-1},
  abstract={In multi-agent systems, instability and partial observability will bring various problems. Communication is an effective way to solve these problems, but when the number of agents in the environment is large, it becomes a key problem to weigh the cost and effect of communication. Therefore, we put forward a model, and designed a communication mechanism in the model, so that agents can learn effective and efficient communication in the partially observable distributed environment of MARL with the lowest communication cost as much as possible. Finally, we show the advantages of our model in multi-agent writing navigation scenarios, in which agents can formulate more coordinated and complex policies than existing methods.},
  keywords={Multi-agent systems;Training;Vectors;Reinforcement learning;Costs;Aggregates;Scalability;Consumer electronics;Observability;Neural networks;multi-agent system;communication;cooperative navigation},
  doi={10.1109/TCE.2024.3502210},
  ISSN={1558-4127},
  month={},}@INPROCEEDINGS{10010864,
  author={Yang, Jianmei},
  booktitle={2022 International Conference on Augmented Intelligence and Sustainable Systems (ICAISS)}, 
  title={Intelligent Research on Online Video Training based on Multi-Agent Data Optimization}, 
  year={2022},
  volume={},
  number={},
  pages={556-559},
  abstract={Intelligent research on sports online classroom video training based on multi-agent agent data optimization is conducted in this paper. On the encoder side, a spatial downsampling is implemented by traditional methods by using Support Vector Machine (SVM) to decide whether to perform downsampling on each frame. At the decoder side, a convolutional network is used to upsample the decoded low-resolution video to the original resolution. With this model, the core multi-agent agent data optimization is also then integrated. Once the agent has a certain function, it will exist in the agent society and provide its own services to the application, so it is not conducive to the dynamic reconfiguration of the multi-agent system. With this integration, the simulation is conducted.},
  keywords={Training;Support vector machines;Data models;Decoding;Spatial resolution;Intelligent systems;Optimization;Intelligent system;online classroom;video processing;multi-agent;data optimization},
  doi={10.1109/ICAISS55157.2022.10010864},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10778300,
  author={Hairi and Fang, Minghong and Zhang, Zifan and Velasquez, Alvaro and Liu, Jia},
  booktitle={2024 22nd International Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks (WiOpt)}, 
  title={On the Hardness of Decentralized Multi-Agent Policy Evaluation Under Byzantine Attacks}, 
  year={2024},
  volume={},
  number={},
  pages={257-264},
  abstract={In this paper, we study a fully-decentralized multi-agent policy evaluation problem, which is an important sub-problem in cooperative multi-agent reinforcement learning, in the presence of up to $f$ faulty agents. In particular, we focus on the so-called Byzantine faulty model with model poisoning setting. In general, policy evaluation is to evaluate the value function of any given policy. In cooperative multi-agent system, the system-wide rewards are usually modeled as the uniform average of rewards from all agents. We investigate the multi-agent policy evaluation problem in the presence of Byzantine agents, particularly in the setting of heterogeneous local rewards. Ideally, the goal of the agents is to evaluate the accumulated system-wide rewards, which are uniform average of rewards of the normal agents for a given policy. It means that all agents agree upon common values (the consensus part) and furthermore, the consensus values are the value functions (the convergence part). However, we prove that this goal is not achievable. Instead, we consider a relaxed version of the problem, where the goal of the agents is to evaluate accumulated system-wide reward, which is an appropriately weighted average reward of the normal agents. We further prove that there is no correct algorithm that can guarantee that the total number of positive weights exceeds $\vert \mathcal{N}\vert -f$, where $\vert \mathcal{N}\vert$ is the number of normal agents. Towards the end, we propose a Byzantine-tolerant decentralized temporal difference algorithm that can guarantee asymptotic consensus under scalar function approximation. We then empirically test the effective of the proposed algorithm.},
  keywords={Wireless networks;Reinforcement learning;Approximation algorithms;Function approximation;Optimization;Multi-agent systems;Convergence;Multi-agent policy evaluation;Byzantine attack;Temporal difference learning},
  doi={},
  ISSN={2690-3342},
  month={Oct},}@INPROCEEDINGS{8790168,
  author={Hou, Y. and Jiang, N. and Ge, H. and Zhang, Q. and Qu, X. and Feng, L. and Gupta, A.},
  booktitle={2019 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={Memetic Multi-agent Optimization in High Dimensions using Random Embeddings}, 
  year={2019},
  volume={},
  number={},
  pages={135-141},
  abstract={In this paper, we propose a memetic multi-agent optimization (MeMAO) paradigm to enhance the search efficacy of classical EAs (i.e., Differential Evolution (DE)) in solving the complex optimization problems. The essential backbone of MeMAO is a recently proposed memetic multi-agent learning system wherein agents acquire increasing learning capabilities by interacting with the environment mainly in a reinforcement learning manner. Differing from MeMAS, the particular interest of MeMAO is placed on addressing the specific challenges when applying classical EAs to optimize the high dimensional optimization problems with a "low effective dimensionality". To achieve this, the target optimization problem is firstly re-formulated into multiple low dimensional tasks via random embedding methods. Further, MeMAO employs DE as the fundamental population based evolutionary solver for multiple agents to optimize multiple low dimensional tasks in a multi-agent scenario. Importantly, MeMAO constructs the social interaction mechanisms among multiple agents, hence improves their convergence speed for solving the target optimization problem by sharing the beneficial information across multiple agents. Lastly, to testify the efficacy of the proposed MeMAO, comprehensive empirical studies on 8 synthetic optimization problems with a dimensionality of 2,000 are provided.},
  keywords={Optimization;Memetics;Task analysis;Multi-agent systems;Biological cells;Search problems;Sociology;Memetic multi-agent system;high dimensional optimization;differential evolution;knowledge transfer.},
  doi={10.1109/CEC.2019.8790168},
  ISSN={},
  month={June},}@ARTICLE{10970024,
  author={Sun, Chuanneng and Huang, Songjun and Pompili, Dario},
  journal={IEEE Robotics and Automation Letters}, 
  title={LLM-Based Multi-Agent Decision-Making: Challenges and Future Directions}, 
  year={2025},
  volume={10},
  number={6},
  pages={5681-5688},
  abstract={In recent years, Large Language Models (LLMs) have shown great abilities in various tasks, including question answering, arithmetic problem solving, and poetry writing, among others. Although research on LLM-as-an-agent has shown that LLM can be applied to Decision-Making (DM) and achieve decent results, the extension of LLM-based agents to Multi-Agent DM (MADM) is not trivial, as many aspects, such as coordination and communication between agents, are not considered in the DM frameworks of a single agent. To inspire more research on LLM-based MADM, in this letter, we survey the existing LLM-based single-agent and multi-agent decision-making frameworks and provide potential research directions for future research. In particular, we focus on the cooperative tasks of multiple agents with a common goal and communication among them. We also consider human-in/on-the-loop scenarios enabled by the language component in the framework.},
  keywords={Training;Decision making;Reinforcement learning;Observability;Adaptation models;Surveys;Sun;System performance;Reflection;Recurrent neural networks;Multi-agent system;natural language models;robotics},
  doi={10.1109/LRA.2025.3562371},
  ISSN={2377-3766},
  month={June},}@INPROCEEDINGS{10278719,
  author={Nie, Zixiang and Chen, Kwang-Cheng and Alanezi, Yousef},
  booktitle={ICC 2023 - IEEE International Conference on Communications}, 
  title={Socially Networked Multi-Robot System of Time-Sensitive Multi-Link Access in a Smart Factory}, 
  year={2023},
  volume={},
  number={},
  pages={4918-4923},
  abstract={Smart factories driven by multi-robot systems (MRS) or multi-agent systems (MAS) integrate edge computing, mobile computing, and wireless communications for exceptionally dynamic, flexible, productive, and resource-efficient manufacturing. With real-time multi-robot task allocation assigning tasks to MRS, MRS executes tasks collaboratively to realize the objectives of smart factories. Ultra-low latency and reliable wireless communication enable such collaborative MRSs in an efficient and resilient manner. On top of time-sensitive multi-link wireless access, this paper takes advantage of social network properties to explore the topology control and channel/resource allocation for the transportation MRS of autonomous mobile robots (AMRs) in a smart factory. To satisfy the minimal latency through proactive communication, adaptive channel allocation occurs concurrently with the formation of social communities based on the robot's geometric trajectories. Computational experiments demonstrate that the proposed methodology delivers a more balanced collision rate and a substantial increase in the average channel access rate compared to what traditional static channel allocation offers.},
  keywords={Wireless communication;Social networking (online);Network topology;Transportation;Channel allocation;Topology;Resource management;smart factory;multi-robot system;multi-agent system;social network;community;multi-link operation;WLAN},
  doi={10.1109/ICC45041.2023.10278719},
  ISSN={1938-1883},
  month={May},}@ARTICLE{10175594,
  author={Palacios-Morocho, Elizabeth and Inca, Saúl and Monserrat, Jose F.},
  journal={IEEE Transactions on Intelligent Vehicles}, 
  title={Enhancing Cooperative Multi-Agent Systems With Self-Advice and Near-Neighbor Priority Collision Control}, 
  year={2024},
  volume={9},
  number={1},
  pages={2864-2877},
  abstract={The coordination of actions to be executed by multiple independent agents in a dynamic environment is one of the main challenges of multi-agent systems. To address this type of scenario, a key technology called Reinforcement Learning (RL) has emerged, which enables the training of optimal cooperative policies among agents. However, traditional value decomposition methods suffer from unstable convergence when the number of agents increases. To address this problem, this article proposes a novel algorithm based on centralized learning that employs a self-advice module to replace the joint action, thereby reducing the algorithmic complexity. The proposed algorithm uses the Joint Action Learning (JAL) concept to find an optimal approach and a collision controller module that was designed to further mitigate the risk of collisions. A comparison of the algorithm proposed is carried out with two benchmark algorithms. The first one focuses on decomposing the reward signal and the second one trains a different actor-critic network for each agent. Furthermore, multiple target points are defined to enhance cooperative scenarios during the learning process. According to the results, the proposed approach outperforms the two benchmarks by 8% and 49%, thus highlighting the effectiveness of the centralized learning approach in multi-agent systems.},
  keywords={Multi-agent systems;Reinforcement learning;Training;Task analysis;Recurrent neural networks;Intelligent vehicles;Nearest neighbor methods;Cooperative multi-agent system;reinforcement learning;independent learning;joint action learning;k-nearest neighbors;deep deterministic policy gradient},
  doi={10.1109/TIV.2023.3293198},
  ISSN={2379-8904},
  month={Jan},}@INPROCEEDINGS{10336011,
  author={Mariani, Stefano and Zambonelli, Franco},
  booktitle={2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS)}, 
  title={Learning Stigmergic Communication for Self-organising Coordination}, 
  year={2023},
  volume={},
  number={},
  pages={47-56},
  abstract={Self-organisation in multi-agent systems (MAS) requires agents to coordinate their actions according to the specific goal to be pursued by the system as a whole. When agents can fully observe their peers, coordination can be implicit, that is, relying solely on appropriate reactions to others’ actions and their effects. When observability is limited, instead, explicit communication is needed. In this case, it is often taken as a built-in capability of agents, and the communication policy is defined at design time. But learning to communicate may enable the MAS to better adapt to novel deployments, run-time changes, and/or changing goals. In this paper, we study learning stigmergic communication in a MAS whose self-organising goal changes between aggregating agents in clusters and its opposite— scattering them around. Through experiments in NetLogo we show four results: (i) learning to communicate is possible, and can also be more effective than built-in communication; (ii) agents learn to self-organise so that the "correct" (i.e. leading to goal achievement) global sequencing of actions arises (i.e. few agents deposit pheromones while most follow pheromone trails, in the case of aggregation goal); and (iii) learning communication policies instead of having them built-in enables self-(re)organisation towards a new goal without heavy intervention in the learning process itself (i.e. by simply changing the reward scheme).},
  keywords={Sequential analysis;Shape;Sensitivity analysis;Sociology;Scattering;Task analysis;Statistics;reinforcement learning;pheromone-based communication;multi-agent system;self-organisation},
  doi={10.1109/ACSOS58161.2023.00022},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9306222,
  author={Li, Hao and Li, Yuejiang and Zhao, H.Vicky},
  booktitle={2020 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)}, 
  title={Modeling Decision Process in Multi-Agent Systems: A Graphical Markov Game based Approach}, 
  year={2020},
  volume={},
  number={},
  pages={197-204},
  abstract={Multi-agent decision processes, where multiple agents interact with each other and make decisions independently, can be seen everywhere in life. Many of the multiagent systems in reality have underlying topological structures, which constraint the interactions and decision makings of agents such as social networks, computer networks, and cognitive radio networks. Some works considered the topological structure between agents, while the decision process of each agent is modeled as a simple imitation of neighbors. Thus, it is of critical importance to study and model how agents interact with each other with consideration about long-term rewards and how the system evolves when considering the topological structure between agents. In this paper, we consider the topological structure between agents and formulate the graphical Markov game. In graphical Markov game, each agent can only observe the actions of neighbors and make decisions based on the interactions with them. The goal of each agent is to maximize its long-term cumulative reward. To find the optimal policy of each agent, we implement a policy gradient based algorithm. We compare our framework with graphical evolutionary game theory where agents only consider the current rewards through experiments of different game settings.},
  keywords={Games;Markov processes;Game theory;Statistics;Sociology;Social networking (online);Reinforcement learning;multi-agent system;topological structure;multiagent reinforcement learning;Markov game},
  doi={},
  ISSN={2640-0103},
  month={Dec},}@INPROCEEDINGS{9275110,
  author={Ji, Honghai and Liu, Hao and Liu, Shida and Wang, Li and Fan, Lingling},
  booktitle={2020 IEEE 9th Data Driven Control and Learning Systems Conference (DDCLS)}, 
  title={Data-driven Adaptive Cooperative Control for Urban Traffic Signal Timing in Multi-intersections}, 
  year={2020},
  volume={},
  number={},
  pages={707-713},
  abstract={This paper presents a new distributed data-driven adaptive cooperative control method (DDACC) for urban traffic signal timing which can achieve the multi-directional queuing length balance with changeable cycle in multi-intersections. This method can guarantee the consensus convergence of the distributed coordinated errors of queuing length with the goal of reducing traffic congestion in multi-agent traffic systems. The proposed DDACC has three novel features, merely using the collected I/O traffic queueing length data and network topology of multi-directional signal controllers at multi-intersections, considering maximum and minimum green time constraints, well working on both undersaturation and supersaturation traffic flow conditions. The results are illustrated by numerical and experimental comparison simulations which are performed on a VISSIM-VB-MATLAB joint simulation platform.},
  keywords={Manganese;Adaptation models;Queueing analysis;Matlab;Data models;Vehicle dynamics;Training;Data-driven Control;Multi-agent System;Urban Traffic Signal Timing;Multi-directional;Multi-intersections},
  doi={10.1109/DDCLS49620.2020.9275110},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10326240,
  author={Yarahmadi, Hossein and Shiri, Mohammad Ebrahim and Navidi, Hamidreza and Sharifi, Arash and Piriaei, Hasan and Challenger, Moharram},
  booktitle={2023 13th International Conference on Computer and Knowledge Engineering (ICCKE)}, 
  title={Vaccine Distribution Modelling in Pandemics through Multi-Agent Systems: COVID-19 Case}, 
  year={2023},
  volume={},
  number={},
  pages={277-285},
  abstract={Multi-agent systems (MAS) have become increasingly used in addressing complex problems that require collaboration and coordination between multiple entities. In the context of infectious disease management, MAS can enable a more nuanced understanding of the complex social dynamics at play, including the behavior and interactions of individuals that determine the rate of infection and mortality.This paper proposes two vaccine distribution methods based on MAS and the concepts of bankruptcy and the Susceptible-Exposed-Infectious-Recovered (SEIR) model, which take into account the prioritization of individuals based on their behavior and their impact on the infection rate of the disease. To propose these methods first, we propose a base algorithm named Vaccine Allocation Problem (VAP) that prioritizes individuals based on their impact on community health.The proposed methods were evaluated in comparison with classic bankruptcy methods such as Proportional (P), Talmud, and Pinile methods. In addition, we use the Covid-19 data set provided by "Belgian Institute for Health" as a case study. Simulation results indicated the better performance of the proposed method in terms of infection, hospitalization, Intensive Care Unit (ICU), new-In, and mortality rates.},
  keywords={COVID-19;Knowledge engineering;Pandemics;Infectious diseases;Simulation;Vaccines;Behavioral sciences;Multi-agent System;Resource Management;Bankruptcy;Vaccine Distribution;COVID-19},
  doi={10.1109/ICCKE60553.2023.10326240},
  ISSN={2643-279X},
  month={Nov},}@INPROCEEDINGS{11113154,
  author={Wang, Qianyu and Tsai, Wei-Tek and Shi, Tianyu and Liu, Zhuang and Du, Bowen},
  booktitle={2025 IEEE 41st International Conference on Data Engineering (ICDE)}, 
  title={Catch Me If You Can: A Multi-Agent Synthetic Fraud Detection Framework for Complex Networks}, 
  year={2025},
  volume={},
  number={},
  pages={3629-3641},
  abstract={Detecting fraudulent behavior across diverse domains presents a significant challenge due to the adaptive and elusive activities of fraud agents. Furthermore, imbalanced data distributions and limited labeled examples increase the difficulty of detecting fraud agents. To address these challenges, we propose Catch Me If You Can—a Multi-Agent Framework to generate synthetic datasets and simulate various types of fraudulent behavior, including but not limited to anti-money laundering (AML), credit card fraud, bot attacks, and malicious traffic. Our framework comprises two core agent types: (1) Detectors, trained to identify suspicious patterns in scenarios, and (2) Transaction Agents, including both legitimate participants and adversarial fraud agents employing strategies to evade detection. In this framework, detectors iteratively refine their detection strategies while fraud agents evolve adaptive tactics to disguise illicit activities, creating an adversarial coevolutionary environment. This dynamic fosters the generation of high-dimensional and realistic datasets for training and testing. By integrating synthetic pre-training with transfer learning, the framework leverages a variety of real-world datasets—including IEEE-CIS Fraud Detection, Credit Card Fraud Detection, and Elliptic++—demonstrating its broad applicability across multiple fraud domains. Our approach significantly improves detection performance, bridging the gap between simulation and real-world applications. It enables robust training across heterogeneous fraud behaviors, contributing to the development of resilient, generalizable solutions for financial security and fraud prevention.},
  keywords={Training;Prevention and mitigation;Transfer learning;Detectors;Reinforcement learning;Credit cards;Fraud;Security;Testing;Synthetic data;Anti-money Laundering;Reinforcement Learning;Multi-Agent System;Transfer Learning},
  doi={10.1109/ICDE65448.2025.00271},
  ISSN={2375-026X},
  month={May},}@ARTICLE{10006753,
  author={Zhu, Liwen and Peng, Peixi and Lu, Zongqing and Tian, Yonghong},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={MetaVIM: Meta Variationally Intrinsic Motivated Reinforcement Learning for Decentralized Traffic Signal Control}, 
  year={2023},
  volume={35},
  number={11},
  pages={11570-11584},
  abstract={Traffic signal control aims to coordinate traffic signals across intersections to improve the traffic efficiency of a district or a city. Deep reinforcement learning (RL) has been applied to traffic signal control recently and demonstrated promising performance where each traffic signal is regarded as an agent. However, there are still several challenges that may limit its large-scale application in the real world. On the one hand, the policy of the current traffic signal is often heavily influenced by its neighbor agents, and the coordination between the agent and its neighbors needs to be considered. Hence, the control of a road network composed of multiple traffic signals is naturally modeled as a multi-agent system, and all agents’ policies need to be optimized simultaneously. On the other hand, once the policy function is conditioned on not only the current agent's observation but also the neighbors’, the policy function would be closely related to the training scenario and cause poor generalizability because the agents in various scenarios often have heterogeneous neighbors. To make the policy learned from a training scenario generalizable to new unseen scenarios, a novel Meta Variationally Intrinsic Motivated (MetaVIM) RL method is proposed to learn the decentralized policy for each intersection that considers neighbor information in a latent way. Specifically, we formulate the policy learning as a meta-learning problem over a set of related tasks, where each task corresponds to traffic signal control at an intersection whose neighbors are regarded as the unobserved part of the state. Then, a learned latent variable is introduced to represent the task's specific information and is further brought into the policy for learning. In addition, to make the policy learning stable, a novel intrinsic reward is designed to encourage each agent's received rewards and observation transition to be predictable only conditioned on its own history. Extensive experiments conducted on CityFlow demonstrate that the proposed method substantially outperforms existing approaches and shows superior generalizability.},
  keywords={Task analysis;Training;Reinforcement learning;Roads;Optimization;Deep learning;Decoding;Meta reinforcement learning;reinforcement learning;traffic signal control;variational autoencoder},
  doi={10.1109/TKDE.2022.3232711},
  ISSN={1558-2191},
  month={Nov},}@ARTICLE{8846672,
  author={Cheng, Yongyi and Xu, Gaochao},
  journal={IEEE Access}, 
  title={A Novel Task Provisioning Approach Fusing Reinforcement Learning for Big Data}, 
  year={2019},
  volume={7},
  number={},
  pages={143699-143709},
  abstract={The large-scale tasks processing for big data using cloud computing has become a hot research topic. Most of previous work on task processing is directly customized and achieved through existing methods. It may result in relatively more system response time, high algorithm complexity and resource waste, etc. Based on this argument, aiming at realizing overall load balancing, bandwidth cost minimization and energy conservation while satisfying resource requirements, a novel large-scale tasks processing approach called TOPE (Two-phase Optimization for Parallel Execution) is developed. The deep reinforcement learning model is designed for virtual link mapping decisions. We treat whole network as a multi-agent system and the whole process of selecting each node's next hop node is formalized via Markov decision process. We train the learning agent by deep neural network to store parameters of deep network model while approximating the value function, rather than tons of state-action values. The virtual node mapping is achieved by designed distributed multi-objective swarm intelligence to realize our two-phase optimization for task allocation in topology structure of Fat-tree. We provide experiments to show the ability of TOPE in analyzing task requests and infrastructure network. The superiority of TOPE for large-scale tasks processing is convincingly demonstrated by comparing with state-of-the-art approaches in cloud environment.},
  keywords={Task analysis;Big Data;Reinforcement learning;Resource management;Optimization;Cloud computing;Load management;Large-scale tasks;big data;two-phase optimization;reinforcement learning;fat-tree},
  doi={10.1109/ACCESS.2019.2943193},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9827289,
  author={Chen, Xu and Li, Zechu and Di, Xuan},
  booktitle={2022 IEEE Intelligent Vehicles Symposium (IV)}, 
  title={Social Learning In Markov Games: Empowering Autonomous Driving}, 
  year={2022},
  volume={},
  number={},
  pages={478-483},
  abstract={In a multi-agent system (MAS), a social learning scheme allows independent agents to learn through interactions with agents randomly selected from a pool. Such a scheme is important for autonomous vehicles (AV) to navigate complex traffic environments consisting of many road users. In this paper, we apply the social learning scheme to Markov games and leverage deep reinforcement learning (DRL) to investigate how individual AVs learn policies and form social norms in traffic scenarios. To capture agents’ different attitudes toward traffic environments, a heterogeneous agent pool with cooperative and defective AVs is introduced to the social learning scheme. To solve social norms formed by AVs, we propose a DRL algorithm, and apply them to traffic scenarios: unsignalized intersection and highway platoon. We find that compared to defective AVs, cooperative AVs can easily conform to expected social norms. In addition, cooperative AVs would lead to lower crash rates. We also find that prioritized roads/lanes can make AVs conform to expected social norms.},
  keywords={Navigation;Roads;Games;Reinforcement learning;Markov processes;Autonomous vehicles;Multi-agent systems;Autonomous vehicles;Markov Game;Social Norms},
  doi={10.1109/IV51971.2022.9827289},
  ISSN={},
  month={June},}@INPROCEEDINGS{10311311,
  author={Apaza, Rafael D. and Li, Hongxiang and Han, Ruixuan and Knoblock, Eric},
  booktitle={2023 IEEE/AIAA 42nd Digital Avionics Systems Conference (DASC)}, 
  title={Multi-agent Deep Reinforcement Learning for Spectrum and Air Traffic Management in UAM with Resource Constraints}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={The quest to optimize frequency utilization in aviation dates back to the very beginning of commercial flight. Over time, spectrum management techniques and new radio technologies have been developed and implemented to assist in meeting the growing demand placed on the system by increased airspace operations. A new challenge to existing aviation spectrum capacity and utilization is emerging with the development of new Urban Air Mobility operations. These new entrants to the airspace will place additional demands to an existing scarce resource, and a new approach to the way frequency is used and managed is needed. This paper investigates the use of deep reinforcement learning techniques to optimize airspace operations efficiency (i.e., minimize the average mission completion time) and safety while considering constrained resources. Through jointly managing spectrum allocation, vehicle scheduling and flight speeds, a multi-agent system using Value Decomposition Network (VDN) collaborates to optimize channel utilization, flight time and departure wait times. To minimize mission completion time, a multi-stage Markov Decision Process (MDP) is formulated where states such as frequency channel availability, uplink signal to interference plus noise power ratio, aircraft location and flight status are jointly considered. A model-free MDP utilizing temporal difference learning and gradient descent is used to train and optimize weight parameters of a three layer deep neural network. Finally, using modeling and simulation the proposed VDN solution is evaluated against two established baseline solutions, namely, the heuristic greedy algorithm (HGA) and orthogonal multiple access (OMA).},
  keywords={Deep learning;Time-frequency analysis;Atmospheric modeling;Simulation;Velocity control;Reinforcement learning;Interference;Aeronautics;Reinforcement Learning;Artificial Intelligence;Communications},
  doi={10.1109/DASC58513.2023.10311311},
  ISSN={2155-7209},
  month={Oct},}@INPROCEEDINGS{11078072,
  author={Findik, Yasin and Coco, Christopher and Azadeh, Reza},
  booktitle={2025 22nd International Conference on Ubiquitous Robots (UR)}, 
  title={Investigating Adaptive Tuning of Assistive Exoskeletons Using Offline Reinforcement Learning: Challenges and Insights}, 
  year={2025},
  volume={},
  number={},
  pages={562-567},
  abstract={Assistive exoskeletons have shown great potential in enhancing mobility for individuals with motor impairments, yet their effectiveness relies on precise parameter tuning for personalized assistance. In this study, we investigate the potential of offline reinforcement learning for optimizing effort thresholds in upper-limb assistive exoskeletons, aiming to reduce reliance on manual calibration. Specifically, we frame the problem as a multi-agent system where separate agents optimize biceps and triceps effort thresholds, enabling an adaptive and data-driven approach to exoskeleton control. Mixed Q-Functionals (MQF) is employed to efficiently handle continuous action spaces while leveraging pre-collected data, thereby mitigating the risks associated with real-time exploration. Experiments were conducted using the MyoPro 2 exoskeleton across two distinct tasks involving horizontal and vertical arm movements. Our results indicate that the proposed approach can dynamically adjust threshold values based on learned patterns, potentially improving user interaction and control, though performance evaluation remains challenging due to dataset limitations.},
  keywords={Performance evaluation;Exoskeletons;Reinforcement learning;Manuals;Motors;Real-time systems;Calibration;Tuning;Robots;Multi-agent systems},
  doi={10.1109/UR65550.2025.11078072},
  ISSN={},
  month={June},}@ARTICLE{10966452,
  author={Mendoza, Charmae Franchesca and Kaneko, Megumi and Rupp, Markus and Schwarz, Stefan},
  journal={IEEE Transactions on Cognitive Communications and Networking}, 
  title={Enhancing the Uplink of Cell-Free Massive MIMO Through Prioritized Sampling and Personalized Federated Deep Reinforcement Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Effective power control is key to solving the inter-user interference problem that degrades performance in cell-free massive multiple-input multiple-output (MIMO) systems. Motivated by its ability to operate online and model-free, without relying on training datasets, we leverage deep reinforcement learning (DRL) for uplink power control, aiming to maximize the guaranteed rate. We propose a fully centralized single-agent framework and two distributed schemes that employ several agents for improved scalability, leveraging prioritized experience replay to enable fast adaptation to the dynamic changes of the wireless environment. We investigate the performance of two multi-agent system architectures: (1) centralized training, decentralized execution (CTDE), where each agent forwards its RL experience to a central trainer, and (2) personalized federated learning (FedPer), where the training is performed locally at each agent, and only the base layer of the local deep neural network (DNN) model is forwarded periodically for aggregation at a server. We focus on the realistic scenario of dynamic device (de-)activation, combined with user mobility. Numerical evaluations demonstrate that the proposed FedPer with prioritized sampling achieves near-optimal rate and power performance while incurring the least amount of communication overhead.},
  keywords={Power control;Uplink;Training;Massive MIMO;Resource management;Optimization;Artificial neural networks;Convergence;Interference;Scalability;cell-free massive MIMO;power control;deep reinforcement learning;personalized federated learning;prioritized experience replay},
  doi={10.1109/TCCN.2025.3561289},
  ISSN={2332-7731},
  month={},}@ARTICLE{9766100,
  author={Shiri, Hamid and Seo, Hyowoon and Park, Jihong and Bennis, Mehdi},
  journal={IEEE Wireless Communications Letters}, 
  title={Attention-Based Communication and Control for Multi-UAV Path Planning}, 
  year={2022},
  volume={11},
  number={7},
  pages={1409-1413},
  abstract={Inspired by the multi-head attention (MHA) mechanism in natural language processing, this letter proposes an iterative single-head attention (ISHA) mechanism for multi-UAV path planning. The ISHA mechanism is run by a communication helper collecting the state embeddings of UAVs and distributing an attention score vector to each UAV. The attention scores computed by ISHA identify how many interactions with other UAVs should be considered in each UAV’s control decision-making. Simulation results corroborate that the ISHA-based communication and control framework achieves faster travel with lower inter-UAV collision risks than an MHA-aided baseline, particularly under limited communication resources.},
  keywords={Autonomous aerial vehicles;Training;Path planning;Artificial neural networks;Vehicle dynamics;Costs;Aerospace electronics;UAV path planning;communication-control co-design;attention;multi-agent reinforcement learning},
  doi={10.1109/LWC.2022.3171602},
  ISSN={2162-2345},
  month={July},}@ARTICLE{11069414,
  author={Gao, Xiang and Zhu, Ziqing and Bu, Siqi and Xia, Shiwei and Ye, Yujian},
  journal={CSEE Journal of Power and Energy Systems}, 
  title={Multi-timescale multi-type electricity and carbon emission market simulation by hierarchical quantum-classical decision making framework}, 
  year={2025},
  volume={},
  number={},
  pages={1-12},
  abstract={Electricity market simulations are designated to model the interactions and decision-making behaviors of market participants, such as GENCOs, under a set of predefined market rules. These simulations help policymakers and market designers assess how changes in market mechanisms will impact the behavior of market participants, the efficiency of energy distribution, and the fairness of market outcomes. However, current simulation tools exhibit notable limitations, such as challenges in fully modeling GENCOs' decision-making processes across multi-market environments, poor convergence in multi-timescale simulations, and fail to accurately capture the correlations and irrationality in GENCOs' behaviors. In this paper, we introduce an innovative quantum-classical decision simulation framework, utilizing quantum computing advantages to overcome these challenges. We first construct a hierarchical Markov decision process (H-MDP) model to simulate GENCOs' decision-making across different time scales, such as long-term and spot markets, and across diverse market types, including electricity and carbon emission auction markets. Building on this H-MDP model, we propose a quantum-enhanced multi-agent soft actor-critic (Q-MASAC) algorithm, employing a variational quantum circuit (VQC) in place of conventional deep neural networks to optimize GENCOs' decision-making. We exploit quantum computing's parallel processing capabilities to accelerate simulations and significantly improve convergence performance. Furthermore, by leveraging the superposition and entanglement properties of quantum states, our framework more effectively captures the correlations and irrationality in GENCOs' decisions. The case study results demonstrate that our algorithm achieves superior convergence, with simulation outcomes more closely mirroring real-world market dynamics.},
  keywords={Decision making;Electricity supply industry;Electricity;Carbon dioxide;Convergence;Quantum computing;Integrated circuit modeling;Carbon;Renewable energy sources;Markov decision processes;Electricity Market Simulation;Hierarchical Markov Decision Process;Quantum Multi-Agent Reinforcement Learning;Variational Quantum Circuit},
  doi={10.17775/CSEEJPES.2024.07290},
  ISSN={2096-0042},
  month={},}@INPROCEEDINGS{9830485,
  author={Ibrahim, Mostafa and Hashmi, Umair Sajid and Nabeel, Muhammad and Imran, Ali and Ekin, Sabit},
  booktitle={2022 1st International Conference on 6G Networking (6GNet)}, 
  title={Complex Agent-based Modeling for HetNets Design and Optimization}, 
  year={2022},
  volume={},
  number={},
  pages={1-7},
  abstract={In wireless heterogeneous networks (HetNets), complexity is an intrinsic property. This paper presents agent-based modeling (ABM) as a tool to optimize complex HetNets. We introduce and analyze a HetNet ABM model that employs parallel algorithms for interference management, resource allocation, and load balancing at both micro and macro levels. Two reinforcement learning (RL) algorithms jointly work together in the model to resolve co-tier and cross-tier interferences. The first RL algorithm controls the transmission power of the small cells, whereas the second assigns the users to the sub-bands with less interference levels. Concurrently, the user association is decided by the users based on their preferences and the resources available at the cells. The model is analyzed in three different operation modes, by switching processes on and off. Results show that individual processes contribute to overall system performance, while jointly maximizing the network’s aggregate signal-to-interference-and-noise ratio (SINR) and minimizing load-induced latency by efficient load balancing.},
  keywords={Analytical models;Aggregates;Interference;Reinforcement learning;Load management;Agent-based modeling;Behavioral sciences},
  doi={10.1109/6GNet54646.2022.9830485},
  ISSN={},
  month={July},}@INPROCEEDINGS{9691696,
  author={Singh, Arshdeep and Jha, Shashi Shekhar},
  booktitle={2021 IEEE 18th India Council International Conference (INDICON)}, 
  title={Learning Safe Cooperative Policies in Autonomous Multi-UAV Navigation}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={The deployment of multiple Unmanned Aerial Vehicles (UAV) in constrained environments has various challenges concerning trajectory optimization with the target(s) reachability and collisions. In this paper, we formulate multi-UAV navigation in constrained environments as a multi-agent learning problem. Further, we propose a reinforcement learning based Safe-MADDPG method to learn safe and cooperative multi-UAV navigation policies in a constrained environment. The safety constraints to handle inter-UAV collisions during navigation are modeled through action corrections of the learned autonomous navigation policies using an additional safety layer. We have implemented our proposed approach on the Webots Simulator and provided a detailed analysis of the proposed solution. The results demonstrate that the proposed Safe-MADDPG approach is effective in learning safe actions for multi-UAV navigation in constrained environments.},
  keywords={Navigation;Conferences;Reinforcement learning;Autonomous aerial vehicles;Safety;Task analysis;Trajectory optimization;UAV;Reinforcement Learning;Policy Gradient;Safe Navigation;Multi-agent System;Webots},
  doi={10.1109/INDICON52576.2021.9691696},
  ISSN={2325-9418},
  month={Dec},}@INPROCEEDINGS{11152877,
  author={Erbayat, Egemen and Mei, Yongsheng and Adam, Gina and Subramaniam, Suresh and Coffey, Sean and Bastian, Nathaniel D. and Lan, Tian},
  booktitle={IEEE INFOCOM 2025 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)}, 
  title={LAMPS: Learning-based Mobility Planning via Posterior State Inference using Gaussian Cox Process Models}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Learning-based mobility planning has proven effective in optimizing performance metrics like latency, throughput, and cost in applications such as path planning and network security. However, real-world networks often face partial or dynamic observability, limiting the applicability of existing robust optimization approaches, which can be conservative, inefficient, or require extensive retraining under changing conditions. This paper introduces LAMPS, a new learning-based mobility planning framework that leverages Gaussian Cox processes to estimate spatiotemporal network states and their uncertainty, enabling robust decision-making under varying observability without retraining. These posterior estimates are integrated into a utility-based planning algorithm that adapts policies trained under full observability to diverse conditions, optimizing average performance or ensuring robustness in near-worst-case scenarios. We analyze the LAMPS framework in a real-world situation involving UAV mobility and wireless resource management, demonstrating the framework’s scalability, adaptability, and efficiency in dynamic network environments. Our evaluation results demonstrate the remarkable adaptability and robustness of the LAMPS. It consistently outperforms other methods under different observability conditions and setups, proving its effectiveness in dynamic environments without requiring retraining.},
  keywords={Wireless communication;Uncertainty;Decision making;Autonomous aerial vehicles;Robustness;Planning;Resource management;Communication system security;Observability;Optimization;Mobility planning;kernel density estimation;reinforcement learning;multi-agent system},
  doi={10.1109/INFOCOMWKSHPS65812.2025.11152877},
  ISSN={2833-0587},
  month={May},}@ARTICLE{10415859,
  author={Xie, Yijing and Zhang, Yichen and Lee, Wei-Jen and Lin, Zongli and Shamash, Yacov A.},
  journal={IEEE/CAA Journal of Automatica Sinica}, 
  title={Virtual Power Plants for Grid Resilience: A Concise Overview of Research and Applications}, 
  year={2024},
  volume={11},
  number={2},
  pages={329-343},
  abstract={The power grid is undergoing a transformation from synchronous generators (SGs) toward inverter-based resources (IBRs). The stochasticity, asynchronicity, and limited-inertia characteristics of IBRs bring about challenges to grid resilience. Virtual power plants (VPPs) are emerging technologies to improve the grid resilience and advance the transformation. By judiciously aggregating geographically distributed energy resources (DERs) as individual electrical entities, VPPs can provide capacity and ancillary services to grid operations and participate in electricity wholesale markets. This paper aims to provide a concise overview of the concept and development of VPPs and the latest progresses in VPP operation, with the focus on VPP scheduling and control. Based on this overview, we identify a few potential challenges in VPP operation and discuss the opportunities of integrating the multi-agent system (MAS)-based strategy into the VPP operation to enhance its scalability, performance and resilience.},
  keywords={Climate change;Renewable energy sources;Smart grids;Virtual power plants;Resilience;Synchronous generators;Research initiatives;Inverters;Distributed power generation;Multi-agent systems;Climate change;renewable energy resources;resilience;smart grids;virtual power plants (VPPs)},
  doi={10.1109/JAS.2024.124218},
  ISSN={2329-9274},
  month={February},}@INPROCEEDINGS{10336023,
  author={Aguzzi, Gianluca and Viroli, Mirko and Esterle, Lukas},
  booktitle={2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS)}, 
  title={Field-informed Reinforcement Learning of Collective Tasks with Graph Neural Networks}, 
  year={2023},
  volume={},
  number={},
  pages={37-46},
  abstract={Coordinating a multi-agent system of intelligent situated agents is a traditional research problem, impacted by the challenges posed by the very notion of distributed intelligence. These problems arise from agents acquiring information locally, sharing their knowledge, and acting accordingly in their environment to achieve a common, global goal. These issues are even more evident in large-scale collective adaptive systems, where agent interactions are necessarily proximity-based, thus making the emergence of controlled global collective behaviour harder.In this context, two main approaches have been proposed for creating distributed controllers out of macro-level task/goal descriptions: manual design, in which programmers build the controllers directly, and automatic design, which involves synthesizing programs using machine learning methods. In this paper, we consider a new hybrid approach called Field-Informed reinforcement learning (FIRL). We utilise manually designed computational fields (globally distributed data structures) to manage global agent coordination. Then, using Deep Q-learning in combination with Graph Neural Networks we enable the agents to learn the necessary local behaviour automatically to solve collective tasks, relying on those fields through local perception. We demonstrate the effectiveness of this new approach in simulated use cases where tracking and covering tasks for swarm robotics are successfully solved.},
  keywords={Q-learning;Aggregates;Swarm robotics;Data structures;Graph neural networks;Task analysis;Distributed computing;Aggregate Computing;Graph Neural Networks;Cyber-Physical Swarms;Many Agent Reinforcement Learning},
  doi={10.1109/ACSOS58161.2023.00021},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9500922,
  author={Ko, Eisaku and Chen, Kwang-Cheng and Lien, Shao-Yu},
  booktitle={ICC 2021 - IEEE International Conference on Communications}, 
  title={Collaborative Partially-Observable Reinforcement Learning Using Wireless Communications}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={Each robot utilizes the reinforcement learning (RL) to control its maneuver and these robots can collaborate to accomplish a common goal to form a collaborative multi-agent system (MAS). Due to the constraints of distributive locations and different poses of robots, in practice, each agent (robot) in such a collaborative MAS can only partially observe the environment and other agents (such as competitive agents), and consequently operate based on its belief of the state(s). The alignment of the beliefs of collaborative agents can be therefore enhanced by adopting wireless communications, but is rarely studied in literature. To explore wireless communications applied to collaborative partially-observable reinforcement learning (PORL), we propose that each collaborative agent predicts the environment dynamics, including the behavior of those agents outside the collaborative MAS, and then constructs the learning-based belief of the world (i.e. global state). To assist such prediction and learning, we modify the RL assisted by the wireless communication functionality into two stages: prediction of the state and local actor-and-critic on global value(s). In other words, while one agent predicts and learns its own policy, another agent can updates critics on the sequence of history to update global value(s) that can further assist to validate the prediction. From numerical experiments, we find that the timing of communication or information exchange among collaborative agents has critical impact on the duration of learning and prediction, and thus the performance of MAS, which suggests the desirable communication for distributed PORL among collaborative agents toward an efficient MAS.},
  keywords={Wireless communication;Distance learning;Collaboration;Reinforcement learning;Markov processes;Delays;History;RL;artificial intelligence;MAS;machine learning;Hidden Markov Model;wireless communications;collaborative robots},
  doi={10.1109/ICC42927.2021.9500922},
  ISSN={1938-1883},
  month={June},}@INPROCEEDINGS{9515871,
  author={Zhang, Zhaoyu and Wei, Daozhi and Li, Jiong and Zhao, Yan},
  booktitle={2021 International Conference on Control Science and Electric Power Systems (CSEPS)}, 
  title={Multi-agent cross prompt mechanism based on Actor-Critic gradient strategy}, 
  year={2021},
  volume={},
  number={},
  pages={112-117},
  abstract={With the continuous integration of artificial intelligence and multi-agent system theory, using the Actor-Critic framework decision-making method suitable for multi-agent systems can better realize dynamic decision-making in multi-agent tasks. In this paper, the evaluation of a single agent in the traditional Critic framework network leads to the inability to achieve communication and cooperation between multiple agents, which leads to problems such as long system learning time or sparse rewards. This paper proposes a multi-agent cross-prompt method that combines reinforcement learning with a multi-agent system. In order to enhance the intelligent level of the system, the cross prompt function of the agent is realized, and focus on analyzing and improving the Actor-Critic action decision rules in the multi-agent system. Made on the basis of improved Actor-critic framework, to build multi-agent detection is performed tasks action decision rules to experiment, In the experiment, the central system evaluation network in the improved algorithm is compared with the traditional algorithm under the closed single-agent evaluation in the same multi-agent system and the cross-prompt framework. The experimental results prove that the improved algorithm is effective and has more application value.},
  keywords={Energy consumption;Decision making;Reinforcement learning;Power systems;Task analysis;Artificial intelligence;Multi-agent systems;Multi-agent reinforcement learning;Cross reminder system;Actor-Critic framework},
  doi={10.1109/CSEPS53726.2021.00030},
  ISSN={},
  month={May},}@ARTICLE{9712301,
  author={Hou, Yaqing and Sun, Mingyang and Zhu, Wenxuan and Zeng, Yifeng and Piao, Haiyin and Chen, Xuefeng and Zhang, Qiang},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, 
  title={Behavior Reasoning for Opponent Agents in Multi-Agent Learning Systems}, 
  year={2022},
  volume={6},
  number={5},
  pages={1125-1136},
  abstract={One important component of developing autonomous agents lies in the accurate prediction of their opponents’ behaviors when the agents interact with others in an uncertain environment. Most recent study focuses on first constructing predictive types (or models) of the opponents, considering their various properties of interest, and subsequently using these models to predict their behaviors accordingly. However, as the possible type space can be rather large, it is time-consuming, and sometimes even infeasible, to predict the actual behaviors of opponents with all candidate types. Thus, in this paper a tractable opponent behavior reasoning approach is proposed that facilitates ($a$) extraction of a small yet representative summary of all candidates using sub-modular-type maximization, and accordingly, ($b$) identification of the most appropriate type for real-time behavior prediction based on multi-armed bandits. In addition, we propose a knowledge-transfer scheme through demonstration learning to synchronize subject agents’ knowledge about their opponents’ behaviors. This further reduces the burden of reasoning with all models of their opponents from the perspective of individual subject agents. We integrate the new behavior prediction and reasoning method into a state-of-the-art evolutionary multi-agent framework, namely a memetic multi-agent system (MeMAS), and demonstrate empirical performance in two problem domains.},
  keywords={Computational modeling;Cognition;Task analysis;Predictive models;Multi-agent systems;Memetics;Real-time systems;Opponent modeling;multi-agent systems;behavior prediction and reasoning;memetic computing},
  doi={10.1109/TETCI.2022.3147011},
  ISSN={2471-285X},
  month={Oct},}@INPROCEEDINGS{9551130,
  author={Nagarajan, Karthikeyan and Yi, Zhong},
  booktitle={2021 IEEE International Conference on Autonomous Systems (ICAS)}, 
  title={Lane Changing Using Multi-Agent DQN}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={This study explores the feasibility of autonomous lane changing using a novel approach of multi-agent Deep Q-Network. Most existing algorithms that use Deep Reinforcement Learning adopt a single-agent approach, with the assumption of only ego-agent changing lanes. We argue that such an approach is merely a simplification of the real-world without considering multi-agent negotiations. In this work, we model the lane changing problem as a multi-agent system and develop a decision-making policy using Deep Q-Network. We address the non-stationarity problem caused by our multi-agent setup which includes an Experience Replay. While prior research recommends avoiding the Experience Replay under such conditions, we report for the first time that an Experience Replay can help yield a robust negotiation policy in our lane changing experiment, without impairing the training of the Deep Q-Network. We show that our approach enables the model to learn negotiating-behaviors like overtaking, yielding, lane interchanging, and lane merging.},
  keywords={Training;Autonomous systems;Conferences;Decision making;Merging;Reinforcement learning;Pragmatics;Autonomous Driving;Deep Reinforcement Learning;Multi-agent Reinforcement Learning},
  doi={10.1109/ICAS49788.2021.9551130},
  ISSN={},
  month={Aug},}@ARTICLE{10052728,
  author={Zhu, Congcong and Cheng, Zishuo and Ye, Dayong and Hussain, Farookh Khadeer and Zhu, Tianqing and Zhou, Wanlei},
  journal={IEEE Transactions on Vehicular Technology}, 
  title={Time-Driven and Privacy-Preserving Navigation Model for Vehicle-to-Vehicle Communication Systems}, 
  year={2023},
  volume={72},
  number={7},
  pages={8459-8470},
  abstract={Effective time-driven navigation is an operative way to alleviate traffic congestion, which is also a challenging problem in the Internet of Vehicles context. Most existing centralized navigation systems often cannot react promptly to real-time local traffic situations, while most existing distributed navigation systems are vulnerable to privacy attacks. To overcome these drawbacks, we propose a learning model that provides a provable guarantee of vehicles' privacy while still enabling efficient navigation under real-time traffic conditions. The proposed model adopts a novel multi-agent system with customized differentially private mechanisms. To verify the effectiveness and stability of our approach, we implement the proposed method on CARLA, which is an autonomous driving simulator. In four experimental tasks with varying parameters, we demonstrate fully that our proposed method outperforms other benchmarks.},
  keywords={Navigation;Privacy;Reinforcement learning;Deep learning;Vehicle dynamics;Real-time systems;Hidden Markov models;Decentralized navigation;deep reinforcement learning;Internet of vehicles;multi-vehicle system},
  doi={10.1109/TVT.2023.3248613},
  ISSN={1939-9359},
  month={July},}
