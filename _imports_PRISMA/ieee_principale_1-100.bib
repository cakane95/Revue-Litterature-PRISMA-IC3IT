@INPROCEEDINGS{10487484,
  author={Abdulghani, Abdulghani M. and Abdulghani, Mokhles M. and Walters, Wilbur L. and Abed, Khalid H.},
  booktitle={2023 Congress in Computer Science, Computer Engineering, & Applied Computing (CSCE)}, 
  title={Multi-Agent Reinforcement Learning System Using Multi-Agent Proximal Policy Optimizer Algorithm in SMAC Environment}, 
  year={2023},
  volume={},
  number={},
  pages={357-360},
  abstract={Multi-Agent Reinforcement Learning (MARL) is a sub-field of reinforcement learning that focuses on studying the behavior of multiple learning agents that coexist in a shared environment. In this paper, we will examine a war scenario within StartCraft II Multi-Agent Challenges (SMAC) environment to implement a multi-agent system. For training, one of the well-known MARL algorithms was used, namely Multi-Agent Proximal Policy Optimization (MAPPO). This algorithm works on navigating the agents to cooperate with each other to achieve the desired goals. We will then use two offered metrics: battle won mean and dead allies mean in the SMAC environment to evaluate the performance of the MAPPO algorithm. The result showed that the MAPPO algorithm reached the greatest value of battle won mean with one million iterations and reached the lowest value of dead allies mean metrics with less than one million iterations. The hardware that we use in this work is CPU Cor i7 11800H, with 32 GB Ram and RTX 3080 laptop GPU, with CUDA Toolkit 11.7.1, and Pytorch 1.7.1.},
  keywords={Measurement;Training;Portable computers;Navigation;Graphics processing units;Random access memory;Reinforcement learning;Artificial Intelligence;AI;Multi-Agent Reinforcement Learning (MARL);Reinforcement Learning;Multi-Agent Proximal Policy Optimization (MAPPO);Autonomous agents;StarCraft II Multi-Agent Challenges (SMAC) environment},
  doi={10.1109/CSCE60160.2023.00062},
  ISSN={},
  month={July},}@INPROCEEDINGS{10487385,
  author={Abdulghani, Abdulghani M. and Abdulghani, Mokhles M. and Walters, Wilbur L. and Abed, Khalid H.},
  booktitle={2023 Congress in Computer Science, Computer Engineering, & Applied Computing (CSCE)}, 
  title={Multi-Agent Reinforcement Learning System Using Value-Decomposition Network Algorithm in StarCraft Environment}, 
  year={2023},
  volume={},
  number={},
  pages={309-312},
  abstract={Multi-Agent Reinforcement Learning (MARL) has been shown to be extremely successful in cooperative assignments. MARL allows for the control of multiple agents to complete multiple tasks in a certain environment and provide helpful services. In this paper, we will examine a war scenario with the StarCraft II Multi-Agent Challenges (SMAC) environment to implement a multi-agent system. For training, we employed one of the most popular MARL algorithms, which is the Value-Decomposition Network (VDN). This algorithm works on controlling the agents to cooperate with each other to achieve the desired goals. We will then use the battle won mean and the dead allies mean metrics to measure the performance of the VDN algorithm. The result showed that the VDN algorithm reaching the highest value of battle won mean with one million iterations and the lowest value of dead allies mean metrics with less than one million iterations. The hardware that we use in this work is CPU Cor i7 11800H, with 32 GB Ram and RTX 3080 laptop GPU, with CUDA Toolkit 11.7.1, and Pytorch 1.7.1.},
  keywords={Measurement;Training;Portable computers;Graphics processing units;Random access memory;Reinforcement learning;Learning (artificial intelligence);Artificial Intelligence;AI;Multi-Agent Reinforcement Learning (MARL);Reinforcement Learning (RL);Multi Autonomous agents;StarCraft II Multi-Agent Challenges (SMAC) environment},
  doi={10.1109/CSCE60160.2023.00053},
  ISSN={},
  month={July},}@INPROCEEDINGS{11177307,
  author={Maryasin, Oleg Yu. and Plohotnyuk, Artem},
  booktitle={2025 International Russian Automation Conference (RusAutoCon)}, 
  title={Building Energy Consumption Control Based on Multi-Agent Reinforcement Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1024-1029},
  abstract={The article describes an example multi-agent system for building microclimate and energy consumption control, where agents act based on multi-agent reinforcement learning. This paper presents the comparison of results obtained using various multi-agent reinforcement learning algorithms for the value of total energy consumption for heating and cooling the building during the day. It is established that the Markov game variant with a common set of states and individual rewards can produce the best results compared to other variants with multi-agent reinforcement learning and single-agent learning. Variants with an independent set of states showed slightly worse results than the respective variants with a common set of states. The usage of a common reward also results in a slight increase in energy consumption compared to the respective variants with individual rewards. This paper also compares the consecutive and parallel implementation of actions by agents.},
  keywords={Heating systems;Training;Energy consumption;Automation;Cooling;Buildings;Reinforcement learning;Games;Optimization;Multi-agent systems;multi-agent system;reinforcement learning;multi-agent reinforcement learning;energy consumption optimization},
  doi={10.1109/RusAutoCon65989.2025.11177307},
  ISSN={2836-614X},
  month={Sep.},}@INPROCEEDINGS{8955073,
  author={Suryanarayanan, N.A.V and Hitoshi, Iba},
  booktitle={2019 IEEE 11th International Workshop on Computational Intelligence and Applications (IWCIA)}, 
  title={Diversifying experiences in multi agent reinforcement learning}, 
  year={2019},
  volume={},
  number={},
  pages={47-52},
  abstract={Deep Reinforcement learning algorithms have traditionally been applied to tasks that train challenging control behavior. Actor Critic based versions of these algorithms have been used to train agents in state of the art settings. While proving to be sample efficient in multi agent learning, these algorithms tend to perform poorly in the exploration phases. In this paper, the experience gained by the replay buffer during the exploration phase is improved by diversifying the input results using a genetic algorithm. We have tested this method on predator prey environment and other team based tasks. The evaluation shows that our method tends to produce a more robust solutions outperforming the traditional methods.},
  keywords={Genetic algorithms;Reinforcement learning;Neural networks;Sociology;Statistics;Measurement;Indexes;Artificial intelligence;Neural networks;Multi Agent System;Deep Reinforcement Learning;Genetic algorithms;Evolutionary computation;Actor Critic;Experience Replay},
  doi={10.1109/IWCIA47330.2019.8955073},
  ISSN={1883-3977},
  month={Nov},}@INPROCEEDINGS{10588337,
  author={Cui, Zhongren and Yang, Chaoqun and Cao, Xianghui},
  booktitle={2024 36th Chinese Control and Decision Conference (CCDC)}, 
  title={A Distributed Simulation-to-Reality Transfer Framework for Multi-agent Reinforcement Learning}, 
  year={2024},
  volume={},
  number={},
  pages={3807-3812},
  abstract={Recent works have demonstrated the applications of multi-agent reinforcement learning (MARL) algorithms in many fields, including autonomous driving, finance, and games. However, how to transfer the MARL models trained in the simulation environment to the real world is a challenging issue due to the gap between simulation and reality. To address this issue, we propose a distributed simulation to reality (Sim2Real) transfer framework that encompasses three components, including a platform, MARL algorithms, and a distributed multi-agent Sim2Real transfer algorithm. In addition, to demonstrate the effectiveness of the proposed framework, we design an experimental collaborative hunting task. The experiment results indicate that the proposed framework can enhance the real-world performance of MARL models.},
  keywords={Collaboration;Finance;Reinforcement learning;Games;Task analysis;Autonomous vehicles;Multi-agent systems;Multi-agent reinforcement learning;Simulation to reality;Domain randomization;Multi-agent system},
  doi={10.1109/CCDC62350.2024.10588337},
  ISSN={1948-9447},
  month={May},}@INPROCEEDINGS{10219199,
  author={Ma, Ricky and Hernandez, Gabe and Hernandez, Carrie},
  booktitle={2023 IEEE Cognitive Communications for Aerospace Applications Workshop (CCAAW)}, 
  title={Scaling Collaborative Space Networks with Deep Multi-Agent Reinforcement Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Future space communication architectures deployed across heterogeneous space systems will require novel methods of coordinating inter-system communication and command distribution. As network complexity increases in time and distance, the ability to facilitate command and control across a large number of systems is a significant constraint on mission performance. This study presents the application of multi-agent reinforcement learning (MARL) to demonstrate a collaborative mesh network of inter-satellite links that self-configure and self-optimize in response to varying mission data needs.This paper explores methods of scaling distributed reinforcement learning-based approaches where satellites modeled as RL agents can observe their local wireless environment, share knowledge with other satellites, and cooperatively achieve network-wide mission objectives. It also implements a transfer learning approach for increasing the network size of a distributed, multi-agent system without modifying action and observation spaces.},
  keywords={Command and control systems;Wireless communication;Mesh networks;Satellites;Scalability;Transfer learning;Collaboration;cognitive network;satellite communications;natural language processing;deep reinforcement learning;multi-agent reinforcement learning},
  doi={10.1109/CCAAW57883.2023.10219199},
  ISSN={},
  month={June},}@INPROCEEDINGS{9327850,
  author={Wang, Zhi-Pu and Wu, Huai-Ning},
  booktitle={2020 Chinese Automation Congress (CAC)}, 
  title={Gas Source Localization using Improved Multi-Agent Reinforcement Learning}, 
  year={2020},
  volume={},
  number={},
  pages={6696-6701},
  abstract={In this paper, an improved multi-agent reinforcement learning (MARL) algorithm is proposed to solve the localization problem of gas source with disturbance sources. Firstly, based on Gaussian dispersion model, multi-point sources are estimated at the initial position of sensor network. Secondly, the multi-agent system is pre-trained in the synthetic environment derived from dispersion model and the estimated source term. Then, the improved MARL algorithm is used to guide the mobile sensors to localize the actual target source. Finally, numerical simulations are given to verify the efficiency of this method.},
  keywords={Sensors;Dispersion;Estimation;Cost function;Location awareness;Genetic algorithms;Reinforcement learning;Gas source localization (GSL);multi-agent reinforcement learning (MARL);source term estimation (STE);multi-point sources},
  doi={10.1109/CAC51589.2020.9327850},
  ISSN={2688-0938},
  month={Nov},}@INPROCEEDINGS{10454813,
  author={Siddiqua, Ayesha and Liu, Siming and Iqbal, Razib and Irfan, Fahim Ahmed and Ross, Logan and Zweerink, Brian},
  booktitle={2024 IEEE 21st Consumer Communications & Networking Conference (CCNC)}, 
  title={TIM-MARL: Information Sharing for Multi-Agent Reinforcement Learning in Smart Environments}, 
  year={2024},
  volume={},
  number={},
  pages={1044-1045},
  abstract={Information sharing among agents to jointly solve problems is challenging for multi-agent reinforcement learning algorithms (MARL) in smart environments. In this paper, we present a novel information sharing approach for MARL, which introduces a Team Information Matrix (TIM) that integrates scenario-independent spatial and environmental information combined with the agent's local observations, augmenting both individual agent's performance and global awareness during the MARL learning. To evaluate this approach, we conducted experiments on three multi-agent scenarios of varying difficulty levels implemented in Unity ML-Agents Toolkit. Experimental results show that the agents utilizing our TIM-Shared variation outperformed those using decentralized MARL and achieved comparable performance to agents employing centralized MARL.},
  keywords={Navigation;Information sharing;Collaboration;Reinforcement learning;Task analysis;Robots;Deep reinforcement learning;multi-agent system;hierarchical information sharing;Unity ML-Agent Toolkit},
  doi={10.1109/CCNC51664.2024.10454813},
  ISSN={2331-9860},
  month={Jan},}@INPROCEEDINGS{10552987,
  author={Lan, Xi and Qiao, Yuansong and Lee, Brian},
  booktitle={2024 10th International Conference on Automation, Robotics and Applications (ICARA)}, 
  title={Application of Multi Agent Reinforcement Learning to Robotic Pick and Place System}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Recent advances in deep reinforcement learning are enabling the creation and use of powerful multi-agent systems in complex areas such as multi-robot coordination. These show great promise to help solve many of the difficult challenges of rapidly growing domains such as smart manufacturing. In this paper we present a novel simulator for a multi-robot pick and place system, leveraging the OpenGym framework. We further evaluate the performance of three distinct reinforcement learning algorithms, name as Qmix, VDN, and IQL, employing the Epymarl framework with our simulator. Our primary objective is to show the effectiveness of these algorithms within a manufacturing context, with a specific focus on their impact on the gripping rate-a vital metric for assessing performance and efficiency.},
  keywords={Measurement;Job shop scheduling;Robot kinematics;Manipulators;Deep reinforcement learning;Multi-robot systems;Task analysis;multi-agent system;pick and place;deep rein-forcement learning;multi-robot system;Dec-POMDP},
  doi={10.1109/ICARA60736.2024.10552987},
  ISSN={2767-7745},
  month={Feb},}@ARTICLE{11083582,
  author={Park, Kiwoong and Shim, Sangheun},
  journal={IEEE Access}, 
  title={Intelligent Counterforce Allocation Method Using Multi-Agent Reinforcement Learning for Ground Operations}, 
  year={2025},
  volume={13},
  number={},
  pages={127009-127022},
  abstract={Modern military operations require commanders to make complex tactical decisions involving the effective allocation of multiple friendly forces to counter enemy threats while managing diverse streams of battlefield information. This study proposes the Intelligent Counterforce Allocation for Ground Operations (ICAGO) method, which leverages multi-agent reinforcement learning (MARL) to support command-level decision-making in ground warfare. Designed for brigade-level and higher force optimization, ICAGO enables effective counterforce deployment in defensive, offensive, and simultaneous operations. The method models key tactical actions, such as maneuver force allocation by avenue of approach and artillery targeting, as MARL agent actions, with doctrinal tactical factors embedded into the reward function. The MARL agent is implemented using the Soft Actor-Critic algorithm and scaled through a Graph Attention Network to manage over 40 heterogeneous agents (infantry, tanks, and artillery). Compared to rule-based approaches, ICAGO demonstrates lower performance variance and significantly improves doctrinal metrics, including up to a 30% increase in win rate and enhanced protection of critical friendly zones, even under uncertainty and diverse operational scenarios. These results highlight the potential of MARL to enhance intelligent, large-scale force allocation under constrained human resources.},
  keywords={Resource management;Reinforcement learning;Force;Artificial intelligence;Decision making;Weapons;Planning;Training;Dynamics;Adaptation models;Command decision support;ground operations;intelligent counterforce allocation;multi-agent reinforcement learning},
  doi={10.1109/ACCESS.2025.3590019},
  ISSN={2169-3536},
  month={},}@ARTICLE{9529182,
  author={Cui, Haoyan and Zhang, Zhen},
  journal={IEEE Access}, 
  title={A Cooperative Multi-Agent Reinforcement Learning Method Based on Coordination Degree}, 
  year={2021},
  volume={9},
  number={},
  pages={123805-123814},
  abstract={Multi-agent reinforcement learning (MARL) has become a prevalent method for solving cooperative problems owing to its tractable implementation and task distribution. The goal of the MARL algorithms for fully cooperative scenarios is to obtain the optimal joint strategy that maximizes the expected common cumulative reward for all agents. However, to date, the analysis of MARL dynamics has focused on repeated games with few agents and actions. To this end, we propose a cooperative MARL algorithm based on the coordination degree (CMARL-CD) and analyze its dynamics in more general cases in which repeated games with more agents and actions are considered. Theoretical analysis shows that if the component action of every optimal joint action is unique, all optimal joint actions are asymptotically stable critical points. The CMARL-CD algorithm realizes coordination among agents without the need to estimate the global Q-value function. Each agent estimates the coordination degree of its own action, which represents the potential of being the optimal action. The efficacy of the CMARL-CD algorithm is studied through repeated games and stochastic games.},
  keywords={Games;Task analysis;Reinforcement learning;Heuristic algorithms;Approximation algorithms;Training;Multi-agent reinforcement learning;multi-agent system;reinforcement learning;independent learner},
  doi={10.1109/ACCESS.2021.3110255},
  ISSN={2169-3536},
  month={},}@ARTICLE{9272805,
  author={Liu, Hui and Zhang, Zhen and Wang, Dongqing},
  journal={IEEE Access}, 
  title={WRFMR: A Multi-Agent Reinforcement Learning Method for Cooperative Tasks}, 
  year={2020},
  volume={8},
  number={},
  pages={216320-216331},
  abstract={Multi-agent reinforcement learning (MARL) for cooperative tasks has been extensively studied in recent years. The balance of exploration and exploitation is crucial to MARL algorithms' performance in terms of the learning speed and the quality of the obtained strategy. To this end, we propose an algorithm known as the weighted relative frequency of obtaining the maximal reward (WRFMR), which uses a weight parameter and the action probability to balance exploration and exploitation and accelerate convergence to the optimal joint action. For the WRFMR algorithm, each agent needs to share the state and the immediate reward and does not need to observe the actions of the other agents. Theoretical analysis on the model of WRFMR in cooperative repeated games shows that each optimal joint action is an asymptotically stable critical point if the component action of every optimal joint action is unique. The box-pushing task, the distributed sensor network (DSN) task, and a strategy game known as blood battlefield are used for empirical studies. Both the DSN task and the box-pushing task involve full cooperation, while blood battle comprises both cooperation and competition. The simulation results show that the WRFMR algorithm outperforms the other algorithms regarding the success rate and the learning speed.},
  keywords={Games;Task analysis;Reinforcement learning;Convergence;Approximation algorithms;Blood;Prediction algorithms;Multi-agent reinforcement learning;reinforcement learning;multi-agent system;repeated game},
  doi={10.1109/ACCESS.2020.3040985},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10012835,
  author={Qiu, Yunbo and Zhan, Yuzhu and Jin, Yue and Wang, Jian and Zhang, Xudong},
  booktitle={2022 IEEE 96th Vehicular Technology Conference (VTC2022-Fall)}, 
  title={Sample-Efficient Multi-Agent Reinforcement Learning with Demonstrations for Flocking Control}, 
  year={2022},
  volume={},
  number={},
  pages={1-7},
  abstract={Flocking control is a significant problem in multi-agent systems such as multi-agent unmanned aerial vehicles and multi-agent autonomous underwater vehicles, which enhances the cooperativity and safety of agents. In contrast to traditional methods, multi-agent reinforcement learning (MARL) solves the problem of flocking control more flexibly. However, methods based on MARL suffer from sample inefficiency, since they require a huge number of experiences to be collected from interactions between agents and the environment. We propose a novel method Pretraining with Demonstrations for MARL (PwD-MARL), which can utilize non-expert demonstrations collected in advance with traditional methods to pretrain agents. During the process of pretraining, agents learn policies from demonstrations by MARL and behavior cloning simultaneously, and are prevented from overfitting demonstrations. By pretraining with non-expert demonstrations, PwD-MARL improves sample efficiency in the process of online MARL with a warm start. Experiments show that PwD-MARL improves sample efficiency and policy performance in the problem of flocking control, even with bad or few demonstrations.},
  keywords={Vehicular and wireless technologies;Autonomous underwater vehicles;Cloning;Reinforcement learning;Control systems;Autonomous aerial vehicles;Safety;flocking control;multi-agent system;multi-agent reinforcement learning;learn from demonstrations},
  doi={10.1109/VTC2022-Fall57202.2022.10012835},
  ISSN={2577-2465},
  month={Sep.},}@INPROCEEDINGS{10859967,
  author={Yuan, Huihong and Xu, Chaoyang and Tu, Feng and Ma, Aijun and Shi, Hong and Wei, Shutian and Zhang, Yunfeng},
  booktitle={2024 International Conference on Integrated Intelligence and Communication Systems (ICIICS)}, 
  title={Multi Robot Cooperative Control Based on Edge Computing and Multi Agent Reinforcement Learning with Cooperative Planning}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={In this era, multi-robot systems play vital role in the sector of environmental monitoring, disaster response, and smart manufacturing. Previous researchers have suggested various traditional methods but still there are challenges such as high latency, bandwidth limitations, and single points of failure, making them unsuitable for time-sensitive and large-scale operations. This article is proposed to develop a system for Multi robot Cooperative Control based on edge computing by employing robust methods such as; Hierarchical Inference, dynamic workload distribution, real-time collaboration on multi-agent. Further, the hierarchical Inference is classified into three layers; cloud layer which is designed using Open Artificial Intelligence (AI) DALL-E, fog layer employed for data preprocessing using various techniques such as median filtering for noise filtering and Linear Discriminant Analysis (LDA) for dimensionality reduction and edge layer is designed by incorporating MobileViT technology. Then, the extracted features are allocated with specific task using federated learning technique namely federated averaged (FedAvg). Finally the Multi Agent Reinforcement Learning (MARL) with Cooperative Planning (CP) is imposed for real-time collaboration in multi agent and has resulted with 0.99 Accumulative Coverage Score (ACS) when compared to the existing Large Language Model (LLM).},
  keywords={Filtering;Noise;Collaboration;Reinforcement learning;Feature extraction;Real-time systems;Planning;Linear discriminant analysis;Robots;Edge computing;cooperative planning;federated learning;linear discriminant analysis;mobilevit technology;multi agent reinforcement learning},
  doi={10.1109/ICIICS63763.2024.10859967},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10401605,
  author={Yu, Wenhan and Zhao, Jun},
  booktitle={2023 International Conference on Computer and Applications (ICCA)}, 
  title={Quantum Multi-Agent Reinforcement Learning as an Emerging AI Technology: A Survey and Future Directions}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={This paper surveys Quantum Multi-Agent Reinforcement Learning (QMARL), an emerging fusion of quantum computing and multi-agent systems. It begins by introducing the transformative potential of quantum computing in computational capabilities. Examining the principles of multi-agent reinforcement learning (MARL), the paper explores how quantum computing enhances learning efficiency and decision-making. Focusing on the current state of QMARL, it reviews literature, methodologies, and case studies showcasing the integration of quantum algorithms with MARL frameworks. The survey addresses challenges and opportunities arising from quantum technologies in multi-agent systems, such as entanglement and superposition, exploring their implications for agent coordination and learning dynamics. Practical applications in domains like cybersecurity and finance underscore QMARL’s transformative potential. Concluding, the paper identifies research gaps and proposes future directions, emphasizing the need for scalable quantum algorithms, exploring quantum-resistant strategies in adversarial settings, and integrating quantum principles in agent communication and collaboration. This concise survey serves as a foundational guide for researchers and practitioners, offering insights into the current state and future possibilities of QMARL.},
  keywords={Surveys;Technological innovation;Quantum algorithm;Quantum entanglement;Focusing;Reinforcement learning;Multi-agent systems;Quantum computing;quantum multi-agent reinforcement learning;quantum AI;quantum machine learning;quantum deep learning;multi-agent system},
  doi={10.1109/ICCA59364.2023.10401605},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9692913,
  author={Rosenberger, Julia and Urlaub, Michael and Schramm, Dieter},
  booktitle={2021 IEEE Global Conference on Artificial Intelligence and Internet of Things (GCAIoT)}, 
  title={Multi-agent reinforcement learning for intelligent resource allocation in IIoT networks}, 
  year={2021},
  volume={},
  number={},
  pages={118-119},
  abstract={In the industrial Internet of Things (IIoT), a high number of devices with limited resources, like computational power, memory, bandwidth and, in case of wireless sensor networks, also energy, communicate. At the same time, the amount of data as well as the demand for data processing in the edge is rapidly increasing. To enable Industry 4.0 (I4.0) and the IIoT, an intelligent resource allocation is required to make optimal use of the available resources. For this purpose, a multi-agent system (MAS) based on deep reinforcement learning (DRL) is proposed. Multi-agent reinforcement learning (MARL) is already taken into account in different communication networks, e.g. for intelligent routing. Despite its great potential, little attention is paid to these methods in industry so far. In this work, DRL is applied for resource allocation and load balancing for industrial edge computing. An optimal usage of the available resources of the IIoT devices should be achieved. Due to the structure of IIoT systems as well as for security reasons, a MAS is preferred for decentralized decision making. In subsequent steps, it is planned to add and remove devices during runtime, to change the number of tasks to be executed as well as evaluations on single- and multi-policy-approaches. The following aspects will be considered for evaluation: (1) improvement of the resource usage of the devices and (2) overhead due to the MAS.},
  keywords={Wireless sensor networks;Runtime;Reinforcement learning;Load management;Routing;Resource management;Security;multi-agent-system;deep reinforcement learning;resource allocation;load balancing;industrial internet of things;streaming data},
  doi={10.1109/GCAIoT53516.2021.9692913},
  ISSN={},
  month={Dec},}@ARTICLE{10900364,
  author={Younas, Rabbiya and Raza Ur Rehman, Hafiz Muhammad and Lee, Ingyu and On, Byung-Won and Yi, Sungwon and Choi, Gyu Sang},
  journal={IEEE Access}, 
  title={SA-MARL: Novel Self-Attention-Based Multi-Agent Reinforcement Learning With Stochastic Gradient Descent}, 
  year={2025},
  volume={13},
  number={},
  pages={35674-35687},
  abstract={In the rapidly advancing Reinforcement Learning (RL) field, Multi-Agent Reinforcement Learning (MARL) has emerged as a key player in solving complex real-world challenges. A pivotal development in this realm is the introduction of the mixing network, representing a significant leap forward in the capabilities of multi-agent systems. Drawing inspiration from COMA and VDN methodologies, the mixing network overcomes limitations in extracting combined Q-values from joint state-action interactions. Previous approaches like COMA and VDN faced constraints in fully utilizing the state-provided information during training, limiting their effectiveness. QMIX and QVMinMax addressed this issue by employing neural networks to convert centralized states into weights for a second neural network, akin to hyper- networks. However, these solutions presented challenges such as computational intensity and susceptibility to local minima. To overcome these hurdles, our proposed methodology introduces three key contributions. First, we introduce the state- fusion network, an innovative alternative to traditional mixing, with a self-attention mechanism. Second, to address the local optima problem in MARL algorithms, we leverage the Grey Wolf Optimizer for weight and bias selection, adding a stochastic element for improved optimization. Finally, we comprehensively compare with QMIX, evaluating performance under two optimization methods: Gradient Descent and Stochastic Optimizer. Using the StarCraft II Learning Environment (SC2LE) as our experimental platform, our results demonstrate the superiority of our methodology over QMIX, QVMinMax, and QSOD in absolute performance, particularly when operating under resource constraints. Our proposed methodology contributes to the ongoing evolution of MARL techniques, showcasing advancements in attention mechanisms and optimization strategies for enhanced multi-agent system capabilities.},
  keywords={Vectors;Training;Reinforcement learning;Neural networks;Optimization;Stochastic processes;Multi-agent systems;Knowledge engineering;Decision making;Robot kinematics;MARL;optimization;self-attention;StarCraft II},
  doi={10.1109/ACCESS.2025.3544961},
  ISSN={2169-3536},
  month={},}@ARTICLE{10555250,
  author={Jiang, Kun and Liu, Wenzhang and Wang, Yuanda and Dong, Lu and Sun, Changyin},
  journal={IEEE/CAA Journal of Automatica Sinica}, 
  title={Discovering Latent Variables for the Tasks With Confounders in Multi-Agent Reinforcement Learning}, 
  year={2024},
  volume={11},
  number={7},
  pages={1591-1604},
  abstract={Efficient exploration in complex coordination tasks has been considered a challenging problem in multi-agent reinforcement learning (MARL). It is significantly more difficult for those tasks with latent variables that agents cannot directly observe. However, most of the existing latent variable discovery methods lack a clear representation of latent variables and an effective evaluation of the influence of latent variables on the agent. In this paper, we propose a new MARL algorithm based on the soft actor-critic method for complex continuous control tasks with confounders. It is called the multi-agent soft actor-critic with latent variable (MASAC-LV) algorithm, which uses variational inference theory to infer the compact latent variables representation space from a large amount of offline experience. Besides, we derive the counterfactual policy whose input has no latent variables and quantify the difference between the actual policy and the counterfactual policy via a distance function. This quantified difference is considered an intrinsic motivation that gives additional rewards based on how much the latent variable affects each agent. The proposed algorithm is evaluated on two collaboration tasks with confounders, and the experimental results demonstrate the effectiveness of MASAC-LV compared to other baseline algorithms.},
  keywords={Training;Current measurement;Reinforcement learning;Approximation algorithms;Probabilistic logic;Prediction algorithms;Inference algorithms;Latent variable model;maximum entropy;multi-agent reinforcement learning (MARL);multi-agent system},
  doi={10.1109/JAS.2024.124281},
  ISSN={2329-9274},
  month={July},}@INPROCEEDINGS{10941365,
  author={Shaheen, Dalaali and Paulraj, Getzi Jeba Leelipushpam and Jebadurai, Immanuel Johnraja and Kirubakaran, Stewart},
  booktitle={2025 International Conference on Electronics and Renewable Systems (ICEARS)}, 
  title={Intelligent Traffic Management System using Multi-Agent Reinforcement Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1975-1981},
  abstract={This paper presents a decentralized multi-agent system for intelligent traffic management in urban environments, where each agent represents a traffic light controller at an intersection. The proposed system leverages a combination of Distributed W-Learning and Deep Q-Networks (DQN) to optimize traffic flow. Distributed W-Learning enables agents to prioritize decisions based on multiple performance policies, while DQN enhances their ability to handle complex state-action mappings through neural network approximations. By utilizing locally available traffic data, each agent adapts dynamically to diverse traffic conditions. The integration of these machine learning techniques ensures a fully decentralized and self-organizing approach, minimizing congestion, improving traffic efficiency, and addressing multiple objectives simultaneously. Simulation experiments are conducted using SUMO (Simulation of Urban MObility) to evaluate the system's performance in realistic traffic scenarios. This study highlights the potential of combining Distributed W-Learning and DQN in multi-agent reinforcement learning to revolutionize traffic management systems and achieve scalable, adaptive, and efficient urban traffic control.},
  keywords={Adaptation models;Renewable energy sources;System performance;Urban areas;Reinforcement learning;Traffic control;Real-time systems;Vehicle dynamics;Traffic congestion;Optimization;Traffic congestion;intelligent transportation systems;multi-agent reinforcement learning;traffic management},
  doi={10.1109/ICEARS64219.2025.10941365},
  ISSN={},
  month={Feb},}@ARTICLE{9172059,
  author={Pan, Zhaotian and Qu, Zhaowei and Chen, Yongheng and Li, Haitao and Wang, Xin},
  journal={IEEE Access}, 
  title={A Distributed Assignment Method for Dynamic Traffic Assignment Using Heterogeneous-Adviser Based Multi-Agent Reinforcement Learning}, 
  year={2020},
  volume={8},
  number={},
  pages={154237-154255},
  abstract={The Dynamic Traffic Assignment (DTA) is one of the important measures to alleviate urban network traffic congestion. The congestions are usually caused by stochastic traffic demands, which are generally unassignable from time dimension in the real-world but are assumed to be assignable in existing DTA methods (i.e. real-time travel demands). In this paper, a distributed DTA method for preventing urban network traffic congestion caused by stochastic real-time travel demands by improving Multi-Agent Reinforcement Learning (MARL). A team structure, which consists of decision-makers and advisers, is designed to learn parallelly in realistic DTA tasks. To reduce the size of the solution space adaptively, the dynamic critical values advised by adviser agents are adopted as constraints for the strategy space of decision-makers (i.e. main agents). A collaborative heterogeneous-adviser mechanism is designed to avoid deviation of guidance. To enhance the adaptability of DTA to the changeable external environment, the mixed strategy concept is introduced to improve the decision-making process of main agents. The respective mapping mechanisms are designed to define adaptive learning rates to improve the sensitivity of MARL. The Sioux Falls (SF) network is established as a test platform via a Dynamic Network Loading (DNL). The effectiveness of the suggested DTA method is assessed through numerical simulations SF network. Under the influence of the scenario with stochastic real-time travel demands, the results show that the proposed method outperforms in terms of the throughput of the network and the individual average travel time among the overall network. Additionally, the ability of the proposed method in response to the external environment rapidly has also been demonstrated. Adopting the suggested method can improve the state of the art to assign stochastic real-time travel demands dynamically and to avoid potential traffic congestion fundamentally.},
  keywords={Real-time systems;Reinforcement learning;Stochastic processes;Convergence;Load modeling;Computer architecture;Decision making;Dynamic traffic assignment;intelligent transportation system;multi-agent system;reinforcement learning;multi-agent reinforcement learning;numerical simulation},
  doi={10.1109/ACCESS.2020.3018267},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11193239,
  author={Zhang, Qi and Cui, He and Li, Xiran},
  booktitle={2025 6th International Conference on Clean Energy and Electric Power Engineering (ICCEPE)}, 
  title={Research on Deep Interactive Collaborative Control Strategy of Source Grid Load Storage for New Power Systems}, 
  year={2025},
  volume={},
  number={},
  pages={458-461},
  abstract={Driven by dual-carbon goals, the power system is shifting to a renewable-led, digital, and distributed architecture. Deep source-grid-load-storage coordination is key to handling renewable volatility and ensuring security. This paper proposes a multi-agent reinforcement learning-based control method and validates it via simulation. It analyzes the interaction characteristics and theoretical basis, constructs a multi-agent model, and designs a collaborative control framework using centralized training and decentralized execution (CTDE). Simulation on the IEEE 33-node system shows the proposed strategy improves renewable absorption, reduces costs, minimizes load peaks and valleys, and enhances voltage stability compared to traditional methods.},
  keywords={Costs;Collaboration;Reinforcement learning;Power system stability;Stability analysis;Robustness;Security;Voltage control;Qualifications;Multi-agent systems;New power system;Collaborative control of source network load storage;Multi agent reinforcement learning;New energy consumption},
  doi={10.1109/ICCEPE66357.2025.11193239},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10285746,
  author={Darwin, Stevanus and Tamba, Tua A.},
  booktitle={2023 International Conference on Computer, Control, Informatics and its Applications (IC3INA)}, 
  title={Multi-Agent Reinforcement Learning with Information Sharing for Optimal Drone Mapping}, 
  year={2023},
  volume={},
  number={},
  pages={66-71},
  abstract={Multi-drone systems have been widely used for various applications which will otherwise be difficult to be done using only single drone operation. One important challenge in such multi-drone system operation is how to optimize their performance when required to operate in unknown environment. In this paper, a multi-agent reinforcement learning (MARL) scheme is proposed to initiate a cooperative operation of a multi-drone system that is tasked to perform a 3D space mapping of a region. The proposed MARL method is designed to optimize the multi-drone system’s energy consumption by introducing a sparse cooperative interaction scheme. In this regard, each drone either communicates with the other when needed or perform its individual learning otherwise. Simulation results are shown to illustrate the convergence/optimality of the used MARL scheme.},
  keywords={Training;Three-dimensional displays;Simulation;Systems operation;Information sharing;Space mapping;Reinforcement learning;Multi-agent system;reinforcement learning;3D space mapping},
  doi={10.1109/IC3INA60834.2023.10285746},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9562951,
  author={Ahmadi, Kamilia and Allan, Vicki H},
  booktitle={2021 IEEE International Smart Cities Conference (ISC2)}, 
  title={Smart City: Application of Multi-agent Reinforcement Learning Systems in Adaptive Traffic Management}, 
  year={2021},
  volume={},
  number={},
  pages={1-7},
  abstract={Traffic congestion on urban road networks has increased substantially during the last decade, characterized by slower speeds, longer travel times, increased vehicular queuing, and increased pollution. The main pain point in traffic management is the static nature of our city structures that cannot adapt to the traffic dynamics changing throughout the day. This work focuses on a futuristic smart city design where making structural changes to the city graph is possible. These changes include modifying number of lanes, opening or closing the ramps, and changing signal timings on road segments. We also assume the local observability of the system where sensors can provide all the data needed for decision making. Under these assumptions, we propose a multi agent reinforcement learning framework for improving traffic flow in city networks. Our learning agents observe their assigned environment and find the best structural changes based on a set of features that represent the recent traffic condition, dependent road segment characteristics, recent structural modifications, etc. Our results show that the proposed framework improves the total travel time (TTT) by 31.5% during rush hours in Salt Lake City area.},
  keywords={Pollution;Smart cities;Pain;Roads;Reinforcement learning;Sensor systems;Timing;Multi-agent system;Reinforcement learning;Markov Decision Process;Explore/Exploit},
  doi={10.1109/ISC253183.2021.9562951},
  ISSN={2687-8860},
  month={Sep.},}@ARTICLE{11021570,
  author={Al-Habashna, Ala’a and Menard, Jon and Wainer, Gabriel and Boudreau, Gary},
  journal={IEEE Access}, 
  title={Decentralized and Joint Resource Allocation, Beamforming, and Beamcombining for 5G Networks With Heterogeneous MARL}, 
  year={2025},
  volume={13},
  number={},
  pages={101491-101506},
  abstract={In this paper, we propose a novel Multi-Agent Reinforcement Learning (MARL) -based paradigm for distributed and joint resource allocation, beamforming (BF), and beam combining of uplink transmissions in 5G networks. The proposed paradigm employs two types of heterogenous agents that learn to perform and optimize different tasks in order to achieve the main objective of the system, as well as the objective of the individual agents. In the proposed paradigm, UEs can be multi-agents that optimize their own resource allocation and BF. In addition to these multi agents (i.e., UEs), the BS is a different type of agent that optimizes the combining of UEs’ transmissions. We developed three different implementations of our proposal using three different MARL algorithms: Independent Q Learners (IQL), Multi-Agent Deep Deterministic Policy Gradient (MADDPG), and QTRAN. Various experiments were conducted to validate the usability of our proposal. Our results show that the proposed paradigm can successfully optimize the task of joint resource allocation, beamforming, and combining. Furthermore, we provide a comparative analysis of the three different implementations, highlighting noteworthy insights into the strengths and limitations of fully distributed algorithms, such as IQL, in comparison to algorithms employing the Centralized Training with Decentralized Execution (CTDE) framework, exemplified by QTRAN and MADDPG.},
  keywords={Resource management;5G mobile communication;Throughput;Interference;Array signal processing;Uplink;Training;Device-to-device communication;Computational complexity;Proposals;Deep reinforcement learning;multi-agent reinforcement learning;distributed resource allocation;beamforming;5G},
  doi={10.1109/ACCESS.2025.3576190},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9027776,
  author={Baer, Schirin and Bakakeu, Jupiter and Meyes, Richard and Meisen, Tobias},
  booktitle={2019 Second International Conference on Artificial Intelligence for Industries (AI4I)}, 
  title={Multi-Agent Reinforcement Learning for Job Shop Scheduling in Flexible Manufacturing Systems}, 
  year={2019},
  volume={},
  number={},
  pages={22-25},
  abstract={In this paper, we first outline the motivation and the need for a new approach for online scheduling in flexible manufacturing systems (FMS) based on reinforcement learning (RL). We then present an initial concept for such an approach. In our method, we use Deep RL agents, who have learned to efficiently guide products through the plant and achieve near-optimal timing regarding resource allocation. Each product is controlled by its own agent, which can handle unforeseen machine failures, re-configurations of the plant topology and the consideration of local and global optimization goals. We created a virtual representation of the FMS using a Petri net, modelling the plant topology and the product flow. The agents' task is to navigate the product to the corresponding machine by choosing the according transition of the Petri net. The training consists of four stages from learning rough rules in order to fulfill a job in a Single-Agent RL setup to learning thoughtful collaboration between agents in a Multi-Agent RL (MARL) setup. We proved the feasibility of the first and second training stage by applying it to the virtual depiction of a specific research plant and obtained promising results. With this concept of a self-learning control policy, online-scheduling can be applied with less effort required to customize the setup for various FMSs.},
  keywords={Optimization;Training;Task analysis;Topology;Frequency modulation;Job shop scheduling;Petri nets;Job Shop Scheduling, Flexible Manufacturing System, Reinforcement Learning, Multi-Agent System},
  doi={10.1109/AI4I46381.2019.00014},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{11137847,
  author={Fang, Bruce and Gao, Danyi},
  booktitle={2025 7th International Conference on Artificial Intelligence Technologies and Applications (ICAITA)}, 
  title={Collaborative Multi-Agent Reinforcement Learning Approach for Elastic Cloud Resource Scaling}, 
  year={2025},
  volume={},
  number={},
  pages={415-419},
  abstract={This paper addresses the challenges of rapid resource variation and highly uncertain task loads in cloud computing environments. It proposes an optimization method for elastic cloud resource scaling based on a multi-agent system. The method deploys multiple autonomous agents to perceive resource states in parallel and make local decisions. While maintaining the distributed nature of the system, it introduces a collaborative value function to achieve global coordination. This improves the responsiveness of resource scheduling and enhances overall system performance. To strengthen system foresight, a lightweight state prediction model is designed. It assists agents in identifying future workload trends and optimizes the selection of scaling actions. For policy training, the method adopts a centralized training and decentralized execution reinforcement learning framework. This enables agents to learn effectively and coordinate strategies under conditions of incomplete information. The paper also constructs typical cloud scenarios, including multi-tenancy and burst traffic, to evaluate the proposed method. The evaluation focuses on resource isolation, service quality assurance, and robustness. Experimental results show that the proposed multi-agent scaling strategy outperforms existing methods in resource utilization, SLA violation control, and scheduling latency. The results demonstrate strong adaptability and intelligent regulation. This provides an efficient and reliable new approach to solving the problem of elastic resource scaling in complex cloud platforms.},
  keywords={Training;Cloud computing;Job shop scheduling;Scalability;Collaboration;Reinforcement learning;Dynamic scheduling;Resource management;Thermal stability;Multi-agent systems;Multi-agent system;elastic expansion;resource scheduling;reinforcement learning},
  doi={10.1109/ICAITA67588.2025.11137847},
  ISSN={},
  month={June},}@INPROCEEDINGS{9003345,
  author={Malysheva, Aleksandra and Kudenko, Daniel and Shpilman, Aleksei},
  booktitle={2019 XVI International Symposium "Problems of Redundancy in Information and Control Systems" (REDUNDANCY)}, 
  title={MAGNet: Multi-agent Graph Network for Deep Multi-agent Reinforcement Learning}, 
  year={2019},
  volume={},
  number={},
  pages={171-176},
  abstract={Over recent years, deep reinforcement learning has shown strong successes in complex single-agent tasks, and more recently this approach has also been applied to multi-agent domains. In this paper, we propose a novel approach, called MAGNet, to multi-agent reinforcement learning that utilizes a relevance graph representation of the environment obtained by a self-attention mechanism, and a message-generation technique. We applied our MAGnet approach to the synthetic predator-prey multi-agent environment and the Pommerman game and the results show that it significantly outperforms state-of-the-art MARL solutions, including Multi-agent Deep Q-Networks (MADQN), Multi-agent Deep Deterministic Policy Gradient (MADDPG), and QMIX.},
  keywords={Neural networks;Magnetic domains;Learning (artificial intelligence);Redundancy;Games;Weapons;Decision making;multi-agent system;relevance graphs;deep-learning},
  doi={10.1109/REDUNDANCY48165.2019.9003345},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10605415,
  author={Qiu, Yunbo and Jin, Yue and Yu, Lebin and Wang, Jian and Zhang, Xudong},
  booktitle={2024 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={Safe Multi-Agent Reinforcement Learning via Dynamic Shielding}, 
  year={2024},
  volume={},
  number={},
  pages={1254-1257},
  abstract={Improving the safety of policies trained by multi-agent reinforcement learning (MARL) is an essential problem for practical utilization. Traditional methods for safe MARL either fail to improve safety during training process, or require strong prior knowledge about the specific task, such as human intervention, expert policy, and state transition model. However, in practical applications, the safety during training process is also important, and strong prior knowledge of the task is generally inaccessible. In this paper, we propose a novel algorithm Dynamic Shielding for MARL (DS-MARL), which utilizes simple prior knowledge including agents’ own motion model to provide a dynamic shield for MARL. DS-MARL aims to improve not only the safety of final policy, but also the safety of training process, without strong prior knowledge. Experimental results show that DS-MARL promotes the safety of both training process and final policy, and also increases success rate of final policy.},
  keywords={Training;Heuristic algorithms;Dynamics;Reinforcement learning;Safety;Task analysis;Artificial intelligence;safe reinforcement learning;multi-agent system;shielding},
  doi={10.1109/CAI59869.2024.00222},
  ISSN={},
  month={June},}@INPROCEEDINGS{9728588,
  author={Zhu, Hua-yu and Mao, Weijie},
  booktitle={2021 China Automation Congress (CAC)}, 
  title={Multi-Agent Reinforcement Learning Control for Consensus Problems of Uncertain Nonlinear Multi-Agent Systems}, 
  year={2021},
  volume={},
  number={},
  pages={6561-6566},
  abstract={In this paper, we consider the consensus problem of distributed multi-agent systems with nonlinear dynamics and disturbances. The underlying system is described based upon the stochastic communication network topology. To solve this problem, we propose a multi-agent reinforcement learning-based method that converts the consensus problem with multiple nonlinear agents into one learning target and automatically learns effective strategies. The actor-critic method is adopted for updating learning policies. Moreover, we design a distributed communication method to ensure that each agent in the multi-agent system can obtain consensus information. Two examples are presented to show the effectiveness and potential of the proposed design technique.},
  keywords={Learning systems;Uncertainty;Network topology;Heuristic algorithms;Reinforcement learning;Information processing;Nonlinear dynamical systems;multi-agent systems;consensus;nonlinear;uncertainties;multi-agent reinforcement learning},
  doi={10.1109/CAC53003.2021.9728588},
  ISSN={2688-0938},
  month={Oct},}@INPROCEEDINGS{10448500,
  author={Meng, Linghui and Zhang, Xi and Xing, Dengpeng and Xu, Bo},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={A New Pre-Training Paradigm for Offline Multi-Agent Reinforcement Learning with Suboptimal Data}, 
  year={2024},
  volume={},
  number={},
  pages={7520-7524},
  abstract={Offline multi-agent reinforcement learning (MARL) with pre-training paradigm, which uses a large quantity of trajectories for offline pre-training and online deployment, has become fashionable lately. While performing well on various tasks, conventional pre-trained decision-making models based on imitation learning typically require many expert trajectories or demonstrations, which limits the development of pre-trained policies in multi-agent case. To address this problem, we propose a new setting, where a multi-agent policy is pre-trained offline using suboptimal (non-expert) data and then tested online with the expectation of high rewards. In this practical setting inspired by contrastive learning , we propose YANHUI, a simple yet effective framework utilizing a well-designed reward contrast function for multi-agent policy representation learning from a dataset including various reward-level data instead of just expert trajectories. Furthermore, we enrich the multi-agent policy pre-training with mixture-of-experts to dynamically represent it. With the same quantity of offline StarCraft Multi-Agent Challenge datasets, YANHUI achieves significant improvements over offline MARL baselines. In particular, our method surprisingly competes in performance with earlier state-of-the-art approaches, even with 10% of the expert data used by other baselines and the rest replaced by poor data.},
  keywords={Representation learning;Decision making;Reinforcement learning;Self-supervised learning;Signal processing;Robustness;Acoustics;Multi-agent system;reinforcement learning;pre-training},
  doi={10.1109/ICASSP48485.2024.10448500},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10328322,
  author={Jimenez-Aparicio, Miguel and Darbali-Zamora, Rachid},
  booktitle={2023 IEEE PES Innovative Smart Grid Technologies Latin America (ISGT-LA)}, 
  title={Cooperative Multi-Agent Reinforcement Learning Method for Fast Voltage Regulation using Distributed Wind Turbine Generators}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={This paper proposes a Multi-Agent Reinforcement Learning (MARL) voltage regulation method for distribution systems with presence of Wind Turbine Generators (WTGs). The control employs reactive power injection or absorption in three WTGs to regulate feeder voltages. A weighted global voltage metric is calculated to summarize nodes' voltages according to their relevance. Three independent RL Agents are trained using policy parameter sharing. Models are trained and tested on real-time Hardware-In-the-Loop setup. The proposed control effectively regulates voltage through the system, bringing median and mean voltages closer to 1 p.u. and reducing standard deviation.},
  keywords={Measurement;Training;Reactive power;Reinforcement learning;Generators;Real-time systems;Wind turbines;Voltage Regulation;DERMS;Wind Turbine Generator;Multi-Agent System;Reinforcement Learning},
  doi={10.1109/ISGT-LA56058.2023.10328322},
  ISSN={2643-8798},
  month={Nov},}@INPROCEEDINGS{9414993,
  author={Liu, Yuntao and Dou, Yong and Shen, Siqi and Qiao, Peng},
  booktitle={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Global-Localized Agent Graph Convolution for Multi-Agent Reinforcement Learning}, 
  year={2021},
  volume={},
  number={},
  pages={3480-3484},
  abstract={A lot of efforts have been devoted to solving the problem about complex relationship and localized cooperation among a large number of agents in large-scale multi-agent systems. However, global cooperation among all agents is also important while interactions between agents often happen locally. It is a challenging problem to enable agent to learn global and localized cooperate information simultaneously in multi-agent systems. In this paper, we model the global and localized cooperation among agents by global and localized agent graphs and propose a novel graph convolutional reinforcement learning mechanism based on these two graphs which allows each agent to communicate with neighbors and all a-gents to cooperate at the high level. Experiments on the large-scale multi-agent scenarios in StarCraft II show that our pro-posed method gets better performance compared with state-of-the-art algorithms and allows agents learning to cooperate efficiently.},
  keywords={Convolution;Conferences;Signal processing algorithms;Reinforcement learning;Acoustics;Speech processing;Multi-agent systems;Machine Learning;Reinforcement Learning;Multi-agent System;Graph Convolutional Network;StarCraft II},
  doi={10.1109/ICASSP39728.2021.9414993},
  ISSN={2379-190X},
  month={June},}@INPROCEEDINGS{9641663,
  author={Selvi, S Sethu and M V, Dhananjaya Kumar and N V, Lohith and Bhardwaj, M P Venkatesh and Shetty, Kartik S},
  booktitle={2021 IEEE Mysore Sub Section International Conference (MysuruCon)}, 
  title={Deep Reinforcement Learning Algorithms for Multi-Agent Systems - A Solution for Modeling Epidemics}, 
  year={2021},
  volume={},
  number={},
  pages={322-327},
  abstract={Multi-agent reinforcement learning (MARL) consists of large number of artificial intelligence-based agents interacting with each other in the same environment, often collaborating towards a common end goal. In single-agent reinforcement learning system the change in the environment is only due to the actions of a particular agent. In contrast, a multi-agent environment is subject to the actions of all the agents involved. Multiagent systems can be deployed in various applications like stock trading to maximize profits in stock market, control and coordination of a swarm of robots, modeling of epidemics, autonomous vehicle and traffic control, smart grids and self-healing networks. It is not possible to solve these complex tasks with a pre-programmed single agent. Instead, the many agents should be trained to automatically search for a solution through reinforcement learning (RL) based algorithms. In general, arriving at a decision in a multi-agent system is almost close to impossible due to exponential increase of problem size with an increase in the number of agents. In this paper, multi-agent systems using Deep Reinforcement Learning (DRL) is explored with a possible application in modeling of epidemics. Different stochastic environments are considered, and various multi-agent policies are implemented using DRL. The performance of various MARL algorithms was evaluated against single agent RL algorithms under different environments. MARL agents were able to learn much faster compared to single RL agents with a more stable training phase. Mean Field Q-Learning was able to scale and perform much better even in the situation of hundreds of agents in the environment and is a sure candidate to model and predict the epidemics, in the existing frightening dangerous situation of corona pandemic.},
  keywords={Training;Epidemics;Q-learning;Robot kinematics;Traffic control;Prediction algorithms;Predator prey systems;Deep Reinforcement Learning (DRL);Multi-agent Reinforcement Learning (MARL);Multi-agent Deep Deterministic Policy Gradient (MADDPG)},
  doi={10.1109/MysuruCon52639.2021.9641663},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9567835,
  author={Uwano, Fumito},
  booktitle={2021 4th International Symposium on Agents, Multi-Agent Systems and Robotics (ISAMSR)}, 
  title={A Cooperative Learning Method for Multi-Agent System with Different Input Resolutions}, 
  year={2021},
  volume={},
  number={},
  pages={84-90},
  abstract={Multi-Agent Reinforcement Learning controls some agents to learn group action with cooperation each other. For example, AGVs in warehouse as the agents cooperate with others and put on and off the supplies to organize them. Though Multi-Agent Reinforcement Learning seems to make advantage to apply multi-robot and more domains, this method has some problems, in particular, it cannot consider the sensor resolution in real world problem. This paper addresses this problem as hetero informational problem, and discuss how to solve the problem by the topology and learning of the neural network of the deep reinforcement learning. Concretely, This paper employed Asynchronous Advantageous Actor-Critic (A3C) with some kinds of neural networks to discuss through two experimental cases, single and multi agent domains. This paper compared performance of agents with different number of hidden layers of neural networks in the single agent domain, and investigate the performance on the environment whose agents have different resolution each other in the multi-agent domain.},
  keywords={Learning systems;Network topology;Neural networks;Reinforcement learning;Robot sensing systems;Topology;Multi-agent systems;Multi-Agent System;Reinforcement Learning;Neural Network;Hetero Resolution;Abstraction},
  doi={10.1109/ISAMSR53229.2021.9567835},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10892447,
  author={Li, Shunfeng and Wang, Peng and Zhu, Xiuli and Li, Ning and Yan, Weiwu},
  booktitle={2024 IEEE Sustainable Power and Energy Conference (iSPEC)}, 
  title={Peer-to-Peer Electricity-Carbon Coupling Trading Market Based on Multi-Agent Reinforcement Learning}, 
  year={2024},
  volume={},
  number={},
  pages={410-415},
  abstract={With the the vigorous development of new power system, energy and carbon coupling trading has emerged as an effective way to promote the renewable generation and reduce carbon footprint. It is a challenge to design an effective market framework for joint electricity and carbon trading to achieve better strategies. This paper introduces a peer-to-peer electricity-carbon coupling trading (ECCT) market and develops a carbon-electricity virtual battery (CEVB) model to represent the relationship between dynamic carbon credits and energy consumption. The trading optimization problem is formulated as a partially observable Markov decision process, then multi-agent double soft actor-critic (MADSAC) algorithm is introduced to solve the problem. The algorithm makes the method more exploratory, protects personal privacy and solves the overestimation problems effectively. Result studies demonstrate the effectiveness of the proposed market and algorithm in optimizing total cost and reducing carbon emissions.},
  keywords={Couplings;Privacy;Costs;Heuristic algorithms;Power system dynamics;Reinforcement learning;Carbon dioxide;Peer-to-peer computing;Carbon;Carbon footprint;Electricity-carbon coupling trading market;Peer-to-peer;Multi-agent reinforcement learning},
  doi={10.1109/iSPEC59716.2024.10892447},
  ISSN={2837-522X},
  month={Nov},}@INPROCEEDINGS{9414712,
  author={Lin, Qifeng and Ling, Qing},
  booktitle={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Self-Inference Of Others’ Policies For Homogeneous Agents In Cooperative Multi-Agent Reinforcement Learning}, 
  year={2021},
  volume={},
  number={},
  pages={3490-3494},
  abstract={Multi-agent reinforcement learning (MARL) has been widely applied in various cooperative tasks, where multiple agents are trained to collaboratively achieve global goals. During the training stage of MARL, inferring policies of other agents is able to improve the coordination efficiency. However, most of the existing policy inference methods require each agent to model all other agents separately, which results in quadratic growth of resource consumption as the number of agents increases. In addition, inferring the policy of an agent solely from its observations and actions may lead to failure of agent modeling. To address this issue, we propose to let each agent infer the others’ policies with its own model, given that the agents are homogeneous. This self-inference approach significantly reduces the computation and storage consumption, and guarantees the quality of agent modeling. Experimental results demonstrate effectiveness of the proposed approach.},
  keywords={Training;Computational modeling;Conferences;Reinforcement learning;Signal processing;Acoustics;Task analysis;Reinforcement learning;policy inference;multi-agent system},
  doi={10.1109/ICASSP39728.2021.9414712},
  ISSN={2379-190X},
  month={June},}@INPROCEEDINGS{10889252,
  author={Zhaikhan, Ainur and Sayed, Ali H.},
  booktitle={ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Multi-Agent Reinforcement Learning in Partially Observable Environments Using Social Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1-4},
  abstract={This work employs a social learning strategy to estimate the global state in a partially observable multi-agent reinforcement learning (MARL) setting. We prove that the proposed methodology can achieve results within an ε-neighborhood of the solution for a fully observable setting, provided that a sufficient number of social learning updates are performed. We illustrate the results through computer simulations.},
  keywords={Technological innovation;Estimation error;Computer simulation;Signal processing algorithms;Reinforcement learning;Signal processing;Acoustics;Speech processing;social learning;partial observability;multi-agent system;reinforcement learning},
  doi={10.1109/ICASSP49660.2025.10889252},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10590191,
  author={Indu, Shukla and Wilson, William R. and Henslee, Althea C. and Dozier, Haley R.},
  booktitle={2023 International Conference on Computational Science and Computational Intelligence (CSCI)}, 
  title={Offline Multi-Agent Reinforcement Learning in Custom Game Scenario}, 
  year={2023},
  volume={},
  number={},
  pages={329-331},
  abstract={Offline reinforcement learning (RL) has garnered considerable attention in recent years due to its attractive capability of learning policies from offline datasets without environmental interactions. The goal of this research is to establish a proof of concept for offline Multi-Agent Reinforcement Learning (OMARL) in combat simulations. While MARL has made impressive progress, OMARL remains a relatively underexplored area. In this paper, we investigate the application of OMARL in a custom game environment and explore the transfer of this learning to a pre-collected dataset from combat simulations for a simplified scenario with no further environmental interactions. To gain a complete understanding of how OMARL works, we initially focus on a custom multi-agent grid environment for two agents. During the learning process, each agent needs to identify the environment dynamics and cooperate with the other agent. Agents are expected to learn a new policy from a single static dataset of previously collected data generated by random actions. Real-world mission planning is complex and involves a multi-agent system; therefore, our pre-collected dataset missions are restricted to motion and action planning in a discrete grid environment.},
  keywords={Scientific computing;Dynamics;Reinforcement learning;Games;Planning;Computational intelligence;Multi-agent systems;Offline Reinforcement Learning;Multi-agent;OpenAI Gym;data collection;“Poster Research Paper”},
  doi={10.1109/CSCI62032.2023.00058},
  ISSN={2769-5654},
  month={Dec},}@ARTICLE{10502122,
  author={Wu, Peiliang and Tian, Liqiang and Zhang, Qian and Mao, Bingyi and Chen, Wenbai},
  journal={IEEE Robotics and Automation Letters}, 
  title={MARRGM: Learning Framework for Multi-Agent Reinforcement Learning via Reinforcement Recommendation and Group Modification}, 
  year={2024},
  volume={9},
  number={6},
  pages={5385-5392},
  abstract={Sample usage efficiency is an important factor affecting the convergence speed of multi-agent deep reinforcement learning (MADRL) algorithms. Most existing experience replay (ER) methods manually select experience samples to update the agent's policy. It is difficult to give suitable and efficient experience samples for different stages of agent policy learning as well as to effectively mine the potential value of experience samples in the replay buffer. Inspired by the idea of recommendation systems, this paper proposes a MADRL framework based on reinforcement recommendation and group modification to improve sample use efficiency and the ability to find the optimal solution of the multi-agent system in different task scenario categories. First, we use the sampling probability of each experience sample output from the recommendation network to recommend sampling instead of manual sampling; simultaneously, we collect the performance of the multi-agent system after updating the policy with the experience sample of recommendation sampling and construct the reinforcement learning process of the recommendation network. Next, we modify the individual policy of the agent according to the group rewards to improve the agent's ability to learn the optimal solution. We then combine and embed the reinforcement recommendation and group modification modules into the MADRL algorithm MAAC. Finally, we experiment with task scenarios, including cooperative collection, command movement, and target navigation, and extend this framework to the MADDPG algorithm to verify its scalability. The experimental results show that the off-policy MADRL algorithms combined with the proposed framework outperform the baseline algorithm in terms of sample usage efficiency and have better universality for the number of agents and scene categories.},
  keywords={Task analysis;Training;Recommender systems;Multi-agent systems;Manuals;Scalability;Navigation;Multi-agent systems;Deep reinforcement learning;Multi-agent reinforcement learning;Reinforcement recommendation system;Group modification},
  doi={10.1109/LRA.2024.3389813},
  ISSN={2377-3766},
  month={June},}@ARTICLE{10623464,
  author={Kuroe, Yasuaki and Iima, Hitoshi},
  journal={IEEE Access}, 
  title={Classes of Dilemma Problems and Their Multi-Agent Reinforcement Learning Method}, 
  year={2024},
  volume={12},
  number={},
  pages={107353-107367},
  abstract={Multi-agent systems appear in a wide variety of fields and there have been several studies on multi-agent reinforcement learning. Dilemma problems are typical classes of multi-agent problems. In these problems, the best policy for each agent differs from the best policy for the group of agents, which makes them difficult to solve. The purpose of this paper is to discuss multi-agent reinforcement learning methods for the dilemma problems. Firstly, we propose definitions of classes of dilemma problems in a general framework of reinforcement learning. We also discuss the relationship among our definitions and existing definitions and show the generality of our proposed definitions. Secondly, we propose a reinforcement learning method that can acquire the cooperative policies for the dilemma problems. In the method, each agent assumes the policies which the other agents would take, and learns through maximizing its return, expecting them to take the assumed policy. We apply the proposed method to the n-person iterative Prisoner’s dilemma (NIPD) and the Tragedy of the Commons which are typical examples of dilemma problems and investigate its performance. It is shown through the experiments that the proposed method makes it possible to learn the cooperative policies more reliably than the existing methods and possesses superior performance to them.},
  keywords={Reinforcement learning;Learning systems;Games;Reliability;Multi-agent systems;Q-learning;Stochastic processes;Dilemma problem;multi-agent system;prisoner’s dilemma;reinforcement learning;social dilemma;tragedy of the Common},
  doi={10.1109/ACCESS.2024.3438937},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9687069,
  author={Zhan, Mengying and Chen, Jinchao and Du, Chenglie and Duan, Yuxin},
  booktitle={2021 IEEE International Conference on Progress in Informatics and Computing (PIC)}, 
  title={Twin Delayed Multi-Agent Deep Deterministic Policy Gradient}, 
  year={2021},
  volume={},
  number={},
  pages={48-52},
  abstract={Recently, reinforcement learning has made remarkable achievements in the fields of natural science, engineering, medicine and operational research. Reinforcement learning addresses sequence problems and considers long-term returns. This long-term view of reinforcement learning is critical to find the optimal solution of many problems. The existing multi- agent reinforcement learning algorithms have the problem of overestimation in estimating the Q value. Unfortunately, there have not been many studies on overestimation of agent reinforcement learning, which will affect the learning efficiency of reinforcement learning. Based on the traditional multi-agent reinforcement learning algorithm, this paper improves the actor network and critic network, optimizes the overestimation of Q value and adopts the update delayed method to make the actor training more stable. In order to test the effectiveness of the algorithm structure, the modified method is compared with the traditional MADDPG, DDPG and DQN methods in the simulation environment.},
  keywords={Training;Conferences;Reinforcement learning;Delays;Informatics;Convergence;Reinforcement learning;Deep learning;neural networks;multi-agent system;overestimation},
  doi={10.1109/PIC53636.2021.9687069},
  ISSN={2329-6259},
  month={Dec},}@INPROCEEDINGS{10426686,
  author={Liu, Xiantao and Zhang, Jie and Feng, Hao and Huang, Wei},
  booktitle={2023 4th International Conference on Computer Engineering and Intelligent Control (ICCEIC)}, 
  title={Millimeter-Wave Network Handover Control based on Multi-Agent System}, 
  year={2023},
  volume={},
  number={},
  pages={413-418},
  abstract={Dense network and mmWave technology are two key factors to meet the requirements of the fifth generation mobile network (5G), which have a significant effect on improving network capacity and data rate. But at the same time, the network environment will become more complex. In this case, it becomes more difficult to obtain global network information and perform real-time handover control on the network. In this paper, we design a multi-agent reinforcement learning-based handover algorithm to address this problem. We have established a fully cooperative handover (HO) control scheme, considering user rate requirements, increasing the overall effective transmission rate of the network while reducing the HO rate. Due to the interplay among user (UE) decisions in the network, we view HO control as a multi-agent cooperative task with the same goal, where all users focus on optimizing the overall network gain. The UE can get global network knowledge during the centralized training strategy, then the UE obtains a decentralized strategy, so after the training is completed, the UE no longer needs global information, and only needs to perform handover control based on the local observation of each UE. The simulation results show that the scheme improves the effective total transmission rate of the network while reducing the handover rate of the network.},
  keywords={Training;Simulation;Reinforcement learning;Handover;Real-time systems;Task analysis;Multi-agent systems;millimeter-wave;handover;cooperative;multi-agent reinforcement learning},
  doi={10.1109/ICCEIC60201.2023.10426686},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10625099,
  author={Taupin, Jérôme and Leturc, Xavier and Bergeron, Cyril and Le Martret, Christophe J.},
  booktitle={2024 IEEE International Conference on Machine Learning for Communication and Networking (ICMLCN)}, 
  title={Improved Trial and Error Learning and Application to Distributed Dynamic Channel Assignment}, 
  year={2024},
  volume={},
  number={},
  pages={88-93},
  abstract={This paper addresses the distributed dynamic channel assignment (DDCA) problem in wireless clustered networks using a fully distributed multi agent reinforcement learning (MARL) approach named trial and error learning (TEL). By fully distributed, we mean that there is no communication between the clusters and as a consequence they have only access to local information to chose their channel. The TEL algorithm has the interesting property to converge to a solution with global optimality guarantee in such context, and has already been used to solve the DDCA problem. In this paper, we propose three heuristics to improve the convergence speed and the stability of the TEL algorithm, resulting into the improved TEL (ITEL) algorithm. We are able to prove that the good convergence property of the TEL algorithm still holds for the ITEL one. We provide simulation results that show the performance gains of the ITEL algorithm as compared with the conventional TEL. We also perform an ablation study to identify which heuristics are the most important among the proposed ones.},
  keywords={Wireless communication;Fading channels;Heuristic algorithms;Simulation;Clustering algorithms;Reinforcement learning;Channel allocation;Multi agent reinforcement learning;trial and error learning;distributed dynamic channel allocation},
  doi={10.1109/ICMLCN59089.2024.10625099},
  ISSN={},
  month={May},}@INPROCEEDINGS{10225785,
  author={Cianfrani, Antonio and Aureli, Davide and Listanti, Marco and Polverini, Marco},
  booktitle={IEEE INFOCOM 2023 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)}, 
  title={Multi Agent Reinforcement Learning Based Local Routing Strategy to Reduce End-to-End Delays in Segment Routing Networks}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={In this paper we propose a framework based on Deep Reinforcement Learning to proactively and autonomously take under control links loads in Segment Routing (SR) networks. The main idea is to monitor local link loads and, in case of anomalous situation, to execute local routing changes at milliseconds timescale. The solution proposed is based on a Multi Agent Reinforcement Learning (MARL) approach: a subset of nodes is equipped with a local agent, powered by a Deep Q-Network (DQN) algorithm, referred to as SRv6 rerouting for Local In-network Link Load Control (SR-LILLC). The main feature of SR-LILLC is to train the agents in a collaborative way, by defining a “shared” reward function, while working in an independent way during the operating phase. Moreover, the re-routing operation is performed in a transparent way for other network devices, without involving the centralized control plane, by exploiting the source routing feature of the SR. The performance evaluation conducted over real data sets shows that SR-LILLC is able to reduce the load on agents links without increasing the maximum link utilization of the network; moreover, the overall network performance are improved in terms of end-to-end delays.},
  keywords={Performance evaluation;Training;Deep learning;Conferences;Collaboration;Reinforcement learning;Routing;Self Driving Networks;Segment Routing;Deep Reinforcement Learning;Traffic Engineering},
  doi={10.1109/INFOCOMWKSHPS57453.2023.10225785},
  ISSN={2833-0587},
  month={May},}@ARTICLE{9512483,
  author={Shao, Yan and Li, Rongpeng and Hu, Bing and Wu, Yingxiao and Zhao, Zhifeng and Zhang, Honggang},
  journal={IEEE Transactions on Vehicular Technology}, 
  title={Graph Attention Network-Based Multi-Agent Reinforcement Learning for Slicing Resource Management in Dense Cellular Network}, 
  year={2021},
  volume={70},
  number={10},
  pages={10792-10803},
  abstract={Network slicing (NS) management devotes to providing various services to meet distinct requirements over the same physical communication infrastructure and allocating resources on demands. Considering a dense cellular network scenario that contains several NS over multiple base stations (BSs), it remains challenging to design a proper real-time inter-slice resource management strategy, so as to cope with frequent BS handover and satisfy the fluctuations of distinct service requirements. In this paper, we propose to formulate this challenge as a multi-agent reinforcement learning (MARL) problem in which each BS represents an agent. Then, we leverage graph attention network (GAT) to strengthen the temporal and spatial cooperation between agents. Furthermore, we incorporate GAT into deep reinforcement learning (DRL) and correspondingly design an intelligent real-time inter-slice resource management strategy. More specially, we testify the universal effectiveness of GAT for advancing DRL in the multi-agent system, by applying GAT on the top of both the value-based method deep Q-network (DQN) and a combination of policy-based and value-based method advantage actor-critic (A2C). Finally, we verify the superiority of the GAT-based MARL algorithms through extensive simulations.},
  keywords={Resource management;Real-time systems;5G mobile communication;Reinforcement learning;Heuristic algorithms;Ultra reliable low latency communication;Quality of service;5G;network slicing;multi-agent reinforcement learning;graph attention network;resource management},
  doi={10.1109/TVT.2021.3103416},
  ISSN={1939-9359},
  month={Oct},}@INPROCEEDINGS{9552052,
  author={Wang, Xiaohan and Zhang, Lin and Laili, Yuanjun and Xie, Kunyu and Lu, Han and Zhao, Chun},
  booktitle={2021 Annual Modeling and Simulation Conference (ANNSIM)}, 
  title={MADES: A Unified Framework for Integrating Agent-Based Simulation with Multi-Agent Reinforcement Learning}, 
  year={2021},
  volume={},
  number={},
  pages={1-12},
  abstract={Agent-Based Simulation (ABS) provides distributed entities for simulating agent emergence or interactive behaviors, but the agent behaviors usually rely on the hard rules, thus lacking the intelligent decision-making capability. With the development of artificial intelligence, Multi-Agent Reinforcement Learning (MARL) has shown positive potential in robot control, autonomous driving, and human-machine battles as its powerful learning capability for making intelligent decisions. There are many challenges in applying MARL directly to ABS, and there is no unified framework that integrates them. The paper proposed the Multi-Agent Discrete Event Simulation (MADES) framework based on several DEVS atomic models to construct the multi-agent system, which has advantages for representing various MARL architectures. A predator-prey system simulation with a mainstream MARL algorithm is built under our framework, the training curves and event transition time figure have verified the learning and the simulation performance of the framework.},
  keywords={Training;Analytical models;Decision making;Robot control;Reinforcement learning;Learning (artificial intelligence);Predator prey systems;agent-based simulation;reinforcement learning;multi-agent system;discrete event simulation},
  doi={10.23919/ANNSIM52504.2021.9552052},
  ISSN={},
  month={July},}@INPROCEEDINGS{9643366,
  author={Leung, Chin-Wing and Hu, Shuyue and Leung, Ho-Fung},
  booktitle={2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Formal Modeling of Reinforcement Learning with Many Agents through Repeated Local Interactions}, 
  year={2021},
  volume={},
  number={},
  pages={714-718},
  abstract={Modelling the dynamics of multi-agent reinforcement learning has long been an important research topic. Most of the previous works focus on agents learning under global interactions. In this paper, we investigate learning in a population of agents with local interactions, such that agents learn their policies concurrently by playing with some other agents locally, without the knowledge of the whole population. We derive the stochastic differential equations (SDEs) to describe the Q-values dynamics of each individual agent under the stochastic environment. Applying the Fokker-Planck equation, the time evolution of the probability distribution (PDF) of the population Q-values is worked out. We validate our model through comparisons with agent-based simulations on typical symmetric games with various settings, and the results verify that the model can precisely capture the behaviour of the multi-agent system.},
  keywords={Heuristic algorithms;Sociology;Stochastic processes;Reinforcement learning;Games;Probability density function;Mathematical models;multi-agent system;reinforcement learning;population dynamics},
  doi={10.1109/ICTAI52525.2021.00113},
  ISSN={2375-0197},
  month={Nov},}@INPROCEEDINGS{10155066,
  author={Pecioski, Damjan and Gavriloski, Viktor and Domazetovska, Simona and Ignjatovska, Anastasija},
  booktitle={2023 12th Mediterranean Conference on Embedded Computing (MECO)}, 
  title={An overview of reinforcement learning techniques}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={Writing control code for a system where the optimal solution is not known in advance can be a very time-consuming process. The Artificial Intelligence (AI) methods typically involve designing a set of rules which can be effective in situations where the problem is precisely defined and well understood. As in real world problems the optimal solution is rarely known, the reinforcement learning framework which incorporates trial and error attempts can be used. Reinforcement learning (RL) is a machine learning technique that involves training an agent to make decisions which are based on the feedback it receives from the environment. One important decision to make when designing an RL system is whether to use a single or multiple agents. This decision depends on the type of problem that needs to be solved as well the environment complexity. Having a goal that can be achieved by a single agent (one player) it is recommended to use single-agent RL while if there is a need for coordination between multiple agents (players) then a multi-agent approach is recommended. In this article, the differences between single agent RL and multi agent RL techniques, as well as their advantages and disadvantages have been presented, and insights are provided into when one approach may be more appropriate than the other.},
  keywords={Training;Embedded computing;Scalability;Process control;Reinforcement learning;Learning (artificial intelligence);Writing;Artificial intelligence (AI);Reinforcement learning (RL);Single agent RL (SARL);Multi agent RL (MARL)},
  doi={10.1109/MECO58584.2023.10155066},
  ISSN={2637-9511},
  month={June},}@ARTICLE{10665939,
  author={Fu, Qingxu and Qiu, Tenghai and Yi, Jianqiang and Pu, Zhiqiang and Ai, Xiaolin},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, 
  title={Self-Clustering Hierarchical Multi-Agent Reinforcement Learning With Extensible Cooperation Graph}, 
  year={2025},
  volume={9},
  number={2},
  pages={1688-1698},
  abstract={Multi-Agent Reinforcement Learning (MARL) has been successful in solving many cooperative challenges. However, classic non-hierarchical MARL algorithms still cannot address various complex multi-agent problems that require hierarchical cooperative behaviors. The cooperative knowledge and policies learned in non-hierarchical algorithms are implicit and not interpretable, thereby restricting the integration of existing knowledge. This paper proposes a novel hierarchical MARL model called Hierarchical Cooperation Graph Learning (HCGL) for solving general multi-agent problems. HCGL has three components: a dynamic Extensible Cooperation Graph (ECG) for achieving self-clustering cooperation; a group of graph operators for adjusting the topology of ECG; and an MARL optimizer for training these graph operators. HCGL's key distinction from other MARL models is that the behaviors of agents are guided by the topology of ECG instead of policy neural networks. ECG is a three-layer graph consisting of an agent node layer, a cluster node layer, and a target node layer. To manipulate the ECG topology in response to changing environmental conditions, four graph operators are trained to adjust the edge connections of ECG dynamically. The hierarchical feature of ECG provides a unique approach to merge primitive actions (actions executed by the agents) and cooperative actions (actions executed by the clusters) into a unified action space, allowing us to integrate fundamental cooperative knowledge into an extensible interface. In our experiments, the HCGL model has shown outstanding performance in multi-agent benchmarks with sparse rewards. We also verify that HCGL can easily be transferred to large-scale scenarios with high zero-shot transfer success rates.},
  keywords={Electrocardiography;Topology;Reinforcement learning;Heuristic algorithms;Training;Network topology;Computational intelligence;Hierarchical MARL;multi-agent system;reinforcement learning},
  doi={10.1109/TETCI.2024.3449873},
  ISSN={2471-285X},
  month={April},}@INPROCEEDINGS{10962505,
  author={Ferdous, Raihana and Kifetew, Fitsum and Prandi, Davide and Susi, Angelo},
  booktitle={2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
  title={Curiosity Driven Multi-agent Reinforcement Learning for 3D Game Testing}, 
  year={2025},
  volume={},
  number={},
  pages={121-129},
  abstract={Recently testing of games via autonomous agents has shown great promise in tackling challenges faced by the game industry, which mainly relied on either manual testing or record/replay. In particular Reinforcement Learning (RL) solutions have shown potential by learning directly from playing the game without the need for human intervention.In this paper, we present cMarlTest, an approach for testing 3D games through curiosity driven Multi-Agent Reinforcement Learning (MARL). cMarlTest deploys multiple agents that work collaboratively to achieve the testing objective. The use of multiple agents helps resolve issues faced by a single agent approach.We carried out experiments on different levels of a 3D game comparing the performance of cMarlTest with a single agent RL variant. Results are promising where, considering three different types of coverage criteria, cMarlTest achieved higher coverage. cMarlTest was also more efficient in terms of the time taken, with respect to the single agent based variant.},
  keywords={Software testing;Industries;Three-dimensional displays;Q-learning;Conferences;Computer bugs;Games;Manuals;Software;Testing;Curiosity driven Reinforcement learning;game testing;coverage based testing},
  doi={10.1109/ICSTW64639.2025.10962505},
  ISSN={2159-4848},
  month={March},}@ARTICLE{10600472,
  author={Siddiqua, Ayesha and Liu, Siming and Siddika Nipu, Ayesha and Harris, Anthony and Liu, Yan},
  journal={IEEE Access}, 
  title={Co-Evolving Multi-Agent Transfer Reinforcement Learning via Scenario Independent Representation}, 
  year={2024},
  volume={12},
  number={},
  pages={99439-99451},
  abstract={Multi-Agent Reinforcement Learning (MARL) is extensively utilized for addressing intricate tasks that involve cooperation and competition among agents in Multi-Agent Systems (MAS). However, learning such tasks from scratch is challenging and often unfeasible, especially for MASs with a large number of agents. Hence, leveraging knowledge from prior experiences can effectively expedite the MARL learning process. Prior work has shown that we successfully facilitated transfer learning for MARL by consolidating various state spaces into fixed-size inputs, enabling a single unified deep-learning policy applicable to several scenarios within the StarCraft Multi-Agent Challenge (SMAC) environment. In this study, we expand SMAC to Multi-Player enabled SMAC (MP-SMAC) by enabling the dynamic selection of training opponents and introducing a co-evolving MARL framework, which creates a co-evolutionary arena where multiple policies learn simultaneously. Our arena comprised the simultaneous training of multiple policies in diverse scenarios, pitting them against both static AI opponents and their peers within MP-SMAC. Furthermore, we integrate co-evolution with curriculum transfer learning into Co-MACTRL framework, enabling our MARL policies to systematically acquire knowledge and skills across predetermined scenarios organized by varying difficulty levels, including evolving opponents. The results revealed significant enhancements in MARL learning performance, demonstrating the advantage of leveraging the co-evolving opponents and maneuvering skills obtained from different scenarios. Additionally, the Co-MACTRL learners consistently attained high performance across a range of SMAC scenarios, showcasing the robustness and generalizability of Co-MACTRL.},
  keywords={Artificial intelligence;Training;Games;Reinforcement learning;Task analysis;Transfer learning;Multi-agent systems;Deep reinforcement learning;Curriculum development;Deep reinforcement learning;multi-agent system;transfer learning;curriculum learning;co-evolutionary multi-agent reinforcement learning;StarCraft II;SMAC},
  doi={10.1109/ACCESS.2024.3430037},
  ISSN={2169-3536},
  month={},}@ARTICLE{10123387,
  author={Zhou, Huan and Jiang, Kai and He, Shibo and Min, Geyong and Wu, Jie},
  journal={IEEE Transactions on Wireless Communications}, 
  title={Distributed Deep Multi-Agent Reinforcement Learning for Cooperative Edge Caching in Internet-of-Vehicles}, 
  year={2023},
  volume={22},
  number={12},
  pages={9595-9609},
  abstract={Edge caching is a promising approach to reduce duplicate content transmission in Internet-of-Vehicles (IoVs). Several Reinforcement Learning (RL) based edge caching methods have been proposed to improve the resource utilization and reduce the backhaul traffic load. However, they only obtain the local sub-optimal solution, as they neglect the influence from environments by other agents. This paper investigates the edge caching strategies with consideration of the content delivery and cache replacement by exploiting the distributed Multi-Agent Reinforcement Learning (MARL). A hierarchical edge caching architecture for IoVs is proposed and the corresponding problem is formulated with the goal to minimize the long-term content access cost in the system. Then, we extend the Markov Decision Process (MDP) in the single agent RL to the context of a multi-agent system, and tackle the corresponding combinatorial multi-armed bandit problem based on the framework of a stochastic game. Specifically, we firstly propose a Distributed MARL-based Edge caching method (DMRE), where each agent can adaptively learn its best behaviour in conjunction with other agents for intelligent caching. Meanwhile, we attempt to reduce the computation complexity of DMRE by parameter approximation, which legitimately simplifies the training targets. However, DMRE is enabled to represent and update the parameter by creating a lookup table, essentially a tabular-based method, which generally performs inefficiently in large-scale scenarios. To circumvent the issue and make more expressive parametric models, we incorporate the technical advantage of the Deep- $Q$  Network into DMRE, and further develop a computationally efficient method (DeepDMRE) with neural network-based Nash equilibria approximation. Extensive simulations are conducted to verify the effectiveness of the proposed methods. Especially, DeepDMRE outperforms DMRE,  $Q$ -learning, LFU, and LRU, and the edge hit rate is improved by roughly 5%, 19%, 40%, and 35%, respectively, when the cache capacity reaches 1, 000 MB.},
  keywords={Computer architecture;Delays;Costs;Backhaul networks;Reinforcement learning;Quality of service;Optimization;Edge caching;Internet-of-Vehicles;content delivery;cache replacement;multi-agent reinforcement learning},
  doi={10.1109/TWC.2023.3272348},
  ISSN={1558-2248},
  month={Dec},}@INPROCEEDINGS{8616166,
  author={Suzuki, Masaaki and Ito, Mari and Takashima, Ryuta},
  booktitle={2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={Evaluating Renewable Energy Policies Using a Multi-agent Reinforcement Learning Model}, 
  year={2018},
  volume={},
  number={},
  pages={959-963},
  abstract={Even as governments combat greenhouse emissions through a range of initiatives, it has yet to be clarified how renewable energy policy, energy market structure, and number of energy producers impact social welfare. We model a deregulated market for electricity as a blind single-price call auction and construct a multi-agent system with reinforcement learning that facilitates more realistic market evaluations and observation of equilibrium processes. We validate our simulation by comparing its results with the results from theoretical analysis in a simplified market.},
  keywords={Renewable energy sources;Portfolios;Standards;Analytical models;Production;Reinforcement learning;Electricity supply industry;renewable energy;multi-agent simulation;reinforcement learning},
  doi={10.1109/SMC.2018.00170},
  ISSN={2577-1655},
  month={Oct},}@INPROCEEDINGS{8766739,
  author={Althamary, Ibrahim and Huang, Chih-Wei and Lin, Phone},
  booktitle={2019 15th International Wireless Communications & Mobile Computing Conference (IWCMC)}, 
  title={A Survey on Multi-Agent Reinforcement Learning Methods for Vehicular Networks}, 
  year={2019},
  volume={},
  number={},
  pages={1154-1159},
  abstract={Under the rapid development of the Internet of Things (IoT), vehicles can be recognized as mobile smart agents that communicating, cooperating, and competing for resources and information. The task between vehicles is to learn and make decisions depending on the policy to improve the effectiveness of the multi-agent system (MAS) that deals with the continually changing environment. The multi-agent reinforcement learning (MARL) is considered as one of the learning frameworks for finding reliable solutions in a highly dynamic vehicular MAS. In this paper, we provide a survey on research issues related to vehicular networks such as resource allocation, data offloading, cache placement, ultra-reliable low latency communication (URLLC), and high mobility. Furthermore, we show the potential applications of MARL that enables decentralized and scalable decision making in vehicle-to-everything (V2X) scenarios.},
  keywords={Resource management;Reinforcement learning;Multi-agent systems;Vehicle-to-everything;Delays;Task analysis;Interference;Multi-agent;Reinforcement Learning;Vehicular Network;URLLC;Caching;Data Offloading;5G},
  doi={10.1109/IWCMC.2019.8766739},
  ISSN={2376-6506},
  month={June},}@INPROCEEDINGS{9665119,
  author={Kattepur, Ajay and David, Sushanth},
  booktitle={2021 IEEE Conference on Network Function Virtualization and Software Defined Networks (NFV-SDN)}, 
  title={Malta: Multi-Agent Reinforcement Learning for Differentiated Services in Fat Tree Networks}, 
  year={2021},
  volume={},
  number={},
  pages={129-134},
  abstract={Fat tree topologies have been gaining traction in datacenter networking due to the benefits of scalability, efficiency and fault resilience. Fat tree networks typically employ Equal Cost Multi-Path (ECMP) routing techniques for traffic load balancing. However, ECMP techniques are sub-optimal at distinguishing and providing differentiated services to various flows, which is a necessary requirement for 5G networks. In this paper, we propose Malta, a Multi-Agent Reinforcement Learning technique to provide differentiated service guarantees in fat tree networks. Multi-agent reinforcement learning techniques offer scale, flexibility in reward structure and can be used to learn optimal behaviour with respect to differing traffic patterns. We demonstrate the utility of such agents over a real use case involving multiple flows with heterogeneous actions at the leaf, spine and super-spine level. The efficacy of the approach is shown in resolving congestions at the spine and super-spine level, that are unable to be resolved by ECMP. In addition, Malta is shown to provide superior differentiated service guarantees with 46% latency improvement and 34% throughput improvement over vanilla ECMP.},
  keywords={Costs;Reinforcement learning;Telecommunication traffic;Throughput;Routing;Virtual machining;Fats;Reinforcement Learning;Fat Trees;Datacenter Network;Differentiated Service.},
  doi={10.1109/NFV-SDN53031.2021.9665119},
  ISSN={},
  month={Nov},}@ARTICLE{9133381,
  author={Park, Young Joon and Lee, Young Jae and Kim, Seoung Bum},
  journal={IEEE Access}, 
  title={Cooperative Multi-Agent Reinforcement Learning With Approximate Model Learning}, 
  year={2020},
  volume={8},
  number={},
  pages={125389-125400},
  abstract={In multi-agent reinforcement learning, it is essential for agents to learn communication protocol to optimize collaboration policies and to solve unstable learning problems. Existing methods based on actor-critic networks solve the communication problem among agents. However, these methods have difficulty in improving sample efficiency and learning robust policies because it is not easy to understand the dynamics and nonstationary of the environment as the policies of other agents change. We propose a method for learning cooperative policies in multi-agent environments by considering the communications among agents. The proposed method consists of recurrent neural network-based actor-critic networks and deterministic policy gradients to centrally train decentralized policies. The actor networks cause the agents to communicate using forward and backward paths and to determine subsequent actions. The critic network helps to train the actor networks by sending gradient signals to the actors according to their contribution to the global reward. To address issues with partial observability and unstable learning, we propose using auxiliary prediction networks to approximate state transitions and the reward function. We used multi-agent environments to demonstrate the usefulness and superiority of the proposed method by comparing it with existing multi-agent reinforcement learning methods, in terms of both learning efficiency and goal achievements in the test phase. The results demonstrate that the proposed method outperformed other alternatives.},
  keywords={Reinforcement learning;Training;Protocols;Task analysis;Neural networks;Observability;Mathematical model;Reinforcement learning;model-free method;multi-agent system;multi-agent cooperation;actor-critic method;deterministic policy gradient},
  doi={10.1109/ACCESS.2020.3007219},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10067429,
  author={Noppakun, Patsornchai and Akkarajitsakul, Khajonpong},
  booktitle={2022 6th International Conference on Information Technology (InCIT)}, 
  title={Permutation Invariant Agent-Specific Centralized Critic in Multi-Agent Reinforcement Learning}, 
  year={2022},
  volume={},
  number={},
  pages={15-18},
  abstract={We proposed a permutation invariant agent-specific centralized critic using graph convolutional networks in multiagent reinforcement learning. We consider an environment with partial observability where a joint observation of homogeneous agents is used as a state information in centralized training. A joint observation of homogeneous agents is permutation invariant, meaning that different permutations must be treated as the same. However, a traditional deep network like multilayer perceptron (MLP) outputs different values to different permutations, despite being the same data. A centralized critic using MLPs to represent joint observation of homogeneous agents suffers from data inefficiency because it only learns a single permutation instead of all permutations. Previous work has addressed this problem using graph convolutional networks (GCN) for “agent-agnostic’' centralized critics. Our work extends the use of GCNs to an “agent-specific’' centralized critic such as the critic used in Counterfactual Multi-Agent Policy Gradients (COMA) algorithm. We introduce three GCN variants of agentspecific critic architectures. Our experimental results on the multi-agent particle environment with COMA algorithm show that all GCN critics outperform the MLP baseline critics. Finally, we concluded that as the number of agents increases, the critic that takes advantage of agent homogeneity by separating global and local feature representation is the most scalable in terms of time complexity.},
  keywords={Training;Measurement;Reinforcement learning;Information representation;Multilayer perceptrons;Convolutional neural networks;Time complexity;graph convolutional network;homogeneous agents;multi-agent system;permutation invariance;reinforcement learning},
  doi={10.1109/InCIT56086.2022.10067429},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9299839,
  author={Chen, Kun and Li, Rongpeng and Zhao, Zhifeng and Zhang, Honggang},
  booktitle={2020 International Conference on Wireless Communications and Signal Processing (WCSP)}, 
  title={The Implementation of Asynchronous Advantage Actor-Critic with Stigmergy in Network-assisted Multi-agent System}, 
  year={2020},
  volume={},
  number={},
  pages={1082-1087},
  abstract={Multi-agent system (MAS) needs to mobilize multiple simple agents to complete complex tasks. However, it is difficult to coherently coordinate distributed agents by means of limited local information. In this paper, we propose a decentralized collaboration method named as "stigmergy" in network-assisted MAS, by exploiting digital pheromones (DP) as an indirect medium of communication and utilizing deep reinforcement learning (DRL) on top. Correspondingly, we implement an experimental platform, where KHEPERA IV robots form targeted specific shapes in a decentralized manner. Experimental results demonstrate the effectiveness and efficiency of the proposed method. Our platform could be conveniently extended to investigate the impact of network factors (e.g., latency, data rate, etc) on the level of collective intelligence.},
  keywords={Task analysis;Decentralized control;Solid modeling;Robot kinematics;Reinforcement learning;Neural networks;Collaboration;multi-agent system;stigmergy mechanism;digital pheromones;deep reinforcement learning;KHEPERA IV robots},
  doi={10.1109/WCSP49889.2020.9299839},
  ISSN={2472-7628},
  month={Oct},}@INPROCEEDINGS{10830044,
  author={Yan, Jiapeng and Li, Qiyue and Li, Yuanqing and Zheng, Zhenxing and Hu, Huimin and Li, Jiaojiao},
  booktitle={2024 8th CAA International Conference on Vehicular Control and Intelligence (CVCI)}, 
  title={Exploring Heterogeneous Multi-agent Reinforcement Learning with Knowledge Transfer for CACC of ICVs Platoon}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Intelligent connected vehicles (ICVs) platoon can be abstracted as a multi-agent system (MAS), and the cooperative adaptive cruise control (CACC) for it has long been a research hot spot. In this paper, a multi-agent reinforcement learning (MARL) with knowledge transfer approach has been developed for solving CACC. Based on the advanced successor feature, the knowledge transfer from source MASs is supposed to enable the control strategy to fully understand the cooperative relationships within various heterogeneous MASs, which is also proved to help the algorithm achieve jump-start improvement. Experimental results demonstrate the effectiveness of the MARL-based CACC solution.},
  keywords={Training;Costs;Connected vehicles;Reinforcement learning;Knowledge transfer;Multi-agent systems;Cruise control;intelligent connected vehicles;heterogeneous MAS;multi-agent reinforcement learning;cooperative adaptive cruise control;successor feature},
  doi={10.1109/CVCI63518.2024.10830044},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9356080,
  author={Jiang, Kai and Zhou, Huan and Zeng, Deze and Wu, Jie},
  booktitle={2020 IEEE 17th International Conference on Mobile Ad Hoc and Sensor Systems (MASS)}, 
  title={Multi-Agent Reinforcement Learning for Cooperative Edge Caching in Internet of Vehicles}, 
  year={2020},
  volume={},
  number={},
  pages={455-463},
  abstract={Edge caching has been emerged as a promising solution to alleviate the redundant traffic and the content access latency in the future Internet of Vehicles (IoVs). Several Reinforcement Learning (RL) based edge caching methods have been proposed to improve the cache utilization and reduce the backhaul traffic load. However, they can only obtain the local sub-optimal solution, as they neglect the influence of environment by other agents. In this paper, we investigate the edge caching strategy with consideration of the content delivery and cache replacement by exploiting the distributed Multi-Agent Reinforcement Learning (MARL). We first propose a hierarchical edge caching architecture for IoVs and formulate the corresponding problem with the objective to minimize the long-term cost of content delivery in the system. Then, we extend the Markov Decision Process (MDP) in the single agent RL to the multi-agent system, and propose a distributed MARL based edge caching algorithm to tackle the optimization problem. Finally, extensive simulations are conducted to evaluate the performance of the proposed distributed MARL based edge caching method. The simulation results show that the proposed MARL based edge caching method significantly outperforms other benchmark methods in terms of the total content access cost, edge hit rate and average delay. Especially, our proposed method greatly reduces an average of 32% total content access cost compared with the conventional RL based edge caching methods.},
  keywords={Simulation;Reinforcement learning;Telecommunication traffic;Sensor systems;Internet;Optimization;Multi-agent systems;edge caching;multi-agent reinforcement learning;content delivery;cache replacement;markov decision process},
  doi={10.1109/MASS50613.2020.00062},
  ISSN={2155-6814},
  month={Dec},}@INPROCEEDINGS{10651299,
  author={Hu, Tianyi and Ai, Xiaolin and Pu, Zhiqiang and Qiu, Tenghai and Yi, Jianqiang},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Heterogeneous Observation Aggregation Network for Multi-agent Reinforcement Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-9},
  abstract={Learning effective policies is challenging for a multi-agent system in partially observable environments, where agents need to extract relevant features from local observations. Most approaches in multi-agent reinforcement learning (MARL) are limited to feature extraction for homogenous agents. They struggle to deal with local observations in heterogeneous multi-agent scenarios, where agents have different observation spaces and are necessitated to process semantically varied information. To address this issue, we analyze the observational heterogeneity of multi-agent systems, and propose a heterogeneous-graph-based approach for feature extraction in MARL. We model agent observations as heterogeneous graphs, and design a heterogeneous observation aggregation network (HOA-Net) for processing these graph-based observations. HOA-Net is specifically designed to address various forms of observational heterogeneity. It employs class-specific weighting networks and computes across-class attentions for observed entities, effectively reducing the number of learnable parameters. The proposed method is evaluated on SMAC and an Unreal-Engine-based heterogeneous multi-agent testbed. Experimental results demonstrate that our method significantly outperforms other baselines in effectively aggregating an agent’s observation, and finally enhancing the performance of heterogeneous multi-agent systems.},
  keywords={Filters;Computational modeling;Neural networks;Reinforcement learning;Feature extraction;Task analysis;Multi-agent systems;Heterogeneity;multi-agent system;reinforcement learning;graph attention network},
  doi={10.1109/IJCNN60899.2024.10651299},
  ISSN={2161-4407},
  month={June},}@ARTICLE{10348557,
  author={Liao, Dengyu and Zhang, Zhen and Song, Tingting and Liu, Mingyang},
  journal={IEEE Access}, 
  title={An Efficient Centralized Multi-Agent Reinforcement Learner for Cooperative Tasks}, 
  year={2023},
  volume={11},
  number={},
  pages={139284-139294},
  abstract={Multi-agent reinforcement learning (MARL) for cooperative tasks has been extensively researched over the past decade. The prevalent framework for MARL algorithms is centralized training and decentralized execution. Q-learning is often employed as a centralized learner. However, it requires finding the maximum value by comparing the Q-value of each joint action a’ in the next state s’ to update the Q-value of the last visited state-action pair (s,a). When the joint action space is extensive, the maximization operation involving comparisons becomes time-consuming and becomes the dominant computational burden of the algorithm. To tackle this issue, we propose an algorithm to reduce the number of comparisons by saving the joint actions with the top 2 Q-values (T2Q). Updating the top 2 Q-values involves seven cases, and the T2Q algorithm can avoid traversing the Q-table to update the Q-value in five of these seven cases, thus alleviating the computational burden. Theoretical analysis demonstrates that the upper bound of the expected ratio of comparisons between T2Q and Q-learning decreases as the number of agents increases. Simulation results from two-stage stochastic games are consistent with the theoretical analysis. Furthermore, the effectiveness of the T2Q algorithm is validated through the distributed sensor network task and the target transportation task. The T2Q algorithm successfully completes both tasks with a 100% success rate and minimal computational overhead.},
  keywords={Q-learning;Games;Task analysis;Training;Computational modeling;Upper bound;Simulation;Multi-agent systems;Reinforcement learning;Multi-agent reinforcement learning;reinforcement learning;Q-learning;multi-agent system},
  doi={10.1109/ACCESS.2023.3340867},
  ISSN={2169-3536},
  month={},}@ARTICLE{10769529,
  author={Wang, Yannan and Liu, Zhen and Geng, Chong and Li, Yidong and Liu, Xinyu and Gao, Qiang},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Distributed Multi-Agent Reinforcement Learning on a Hierarchical Game Model for Railway Engineering Data Collaborative Edge Caching}, 
  year={2025},
  volume={26},
  number={2},
  pages={2643-2655},
  abstract={The rapid expansion and intelligent development of railway infrastructure are driving significant growth in railway engineering data. For the dispersed users across railway networks’ complex topology, traditional centralized storage systems are insufficient for their low-latency, cost-efficient data retrieval. Existing edge caching solutions based on multi-agent reinforcement learning fail to address the asymmetric relationships among railway nodes, such as data centers, stations, and sections, etc. Besides, the complexity of computing Nash equilibrium points also gets higher as the number of agents (edge caching servers) increases. This study introduces a Hierarchical Game model-based MADRL-driven Collaborative Edge Caching method(HG-MCEC) tailored for railway engineering data. By considering the distribution characteristics and caching strategy games among railway nodes, a hierarchical game model for collaborative edge caching is constructed. This model treats the railway edge caching as a multi-agent system, in which each railway node server is regarded as an agent. HG-MCEC utilizes deep learning to mitigate computational complexity and recognize agents’ asymmetry. Upper-level agents adjust cache replacement strategies according to environmental changes and decisionmaking experience. Lower-level agents, under the guidance of upper-level decisions, optimize collaborative caching strategies toward achieving hierarchical game equilibrium. Using a highspeed railway building information modeling data for validation, the method significantly outperforms existing approaches by enhancing content hit rates and reducing latency at edge caching servers while decreasing system content transmission costs.},
  keywords={Servers;Collaboration;Railway engineering;Games;Data models;Reinforcement learning;Vehicle dynamics;Distributed databases;Computational modeling;Scalability;Collaborative edge caching;railway engineering data;multi-agent reinforcement learning;hierarchical game model},
  doi={10.1109/TITS.2024.3500093},
  ISSN={1558-0016},
  month={Feb},}@INPROCEEDINGS{10610314,
  author={Chung, Hojun and Oh, Jeongwoo and Heo, Jaeseok and Lee, Gunmin and Oh, Songhwai},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={MAC-ID: Multi-Agent Reinforcement Learning with Local Coordination for Individual Diversity}, 
  year={2024},
  volume={},
  number={},
  pages={15233-15239},
  abstract={With the increase of robots navigating through crowded environments in our daily lives, the demand for designing a socially-aware navigation method considering humanrobot interaction has risen. When developing and assessing socially-aware navigation methods, pedestrian motion modeling plays a significant role. However, existing pedestrian models often struggle in complex environments and do not have the capacity to generate diverse pedestrian styles.In this paper, we propose multi-agent reinforcement learning with local coordination for individual diversity (MAC-ID), which can synthesize diverse pedestrian motions via local coordination factor (LCF). Our experiments have demonstrated that the manipulation of the LCF induces interpretable changes in pedestrian behaviors, along with a superior performance compared to existing pedestrian motion models. For evaluating socially-aware navigation methods using MAC-ID, we present a novel benchmark called BSON. It offers realistic and diverse social environments with pedestrians modeled via MAC-ID. We have trained and compared various navigation methods in BSON using a newly proposed metric called socially-aware navigation score (SNS). Through BSON, users can evaluate their socially-aware navigation methods and compare them to baselines.},
  keywords={Measurement;Pedestrians;Navigation;Robot kinematics;Reinforcement learning;Benchmark testing;Behavioral sciences},
  doi={10.1109/ICRA57147.2024.10610314},
  ISSN={},
  month={May},}@INPROCEEDINGS{10651259,
  author={Zhang, Qingyang and Xu, Bo},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Towards Zero-Shot Generalization: Mutual Information-Guided Hierarchical Multi-Agent Coordination}, 
  year={2024},
  volume={},
  number={},
  pages={1-9},
  abstract={Multi-agent systems often face the challenge of adapting to dynamic team composition and variable partial observability, which can hinder the generalization ability of agent policies. This research introduces a novel method, Mutual Information-guided Multi-Agent coordination (MIMA), to address these issues. MIMA utilizes a hierarchical structure that includes a meta-controller, an information extractor, and agents acting as controllers. The meta-controller partitions the team into distinct groups, while the information extractor uses this partition to extract relevant information. The controllers then make decisions based on this information. We propose two objectives based on mutual information to learn individual and group-specific information. The information extractor uses individual information to form inner-group information, addressing the variable partial observability challenge. It also extracts group-specific information to improve the agents’ adaptability to scenarios with dynamic team composition. Both types of information guide agents’ distributed execution and influence policy updates during centralized training. Our experiments in multi-agent particle environments and StarCraft II micromanagement tasks show that MIMA improves the zero-shot generalization ability by a large margin, demonstrating its effectiveness in handling dynamic team composition and variable partial observability.},
  keywords={Training;Neural networks;Reinforcement learning;Data mining;Observability;Mutual information;Faces;Multi-Agent Reinforcement Learning;Multi-Agent System;Partial Observability;Zero-Shot Generalization;Hierarchical Multi-Agent Reinforcement Learning},
  doi={10.1109/IJCNN60899.2024.10651259},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10590879,
  author={Siddiqua, Ayesha and Liu, Siming and Iqbal, Razib and Ross, Logan and Zweerink, Brian and Eskridge, Ryan},
  booktitle={2024 IEEE International Symposium on Robotic and Sensors Environments (ROSE)}, 
  title={Information Sharing for Cooperative Robots via Multi-Agent Reinforcement Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Facilitating collaboration within a team of robots poses a challenging question for the field of multi-agent reinforcement learning (MARL) in smart environments. Many existing cooperative MARL methods utilize centralized or decentralized frameworks leveraging global or local information for decision-making without sufficiently considering information exchange among agents. This research presents an innovative information-sharing approach for MARL, aiming to enhance collaboration among robots and improve overall team performance in multi-agent systems. In particular, our approach introduces an Information Sharing Matrix (ISM) that combines scenario-independent spatial and environmental information with each robot's local observations, thereby enhancing the performance of individual robots and improving their global awareness during the MARL learning process. To assess the efficacy of our approach, we conducted experiments on three cooperative multi-agent scenarios with varying difficulty levels implemented in Unity ML-Agents Toolkit. The experimental results indicate that robots employing our approach have effectively learned collaborative abilities, enabling them to maximize space coverage while avoiding conflicts among themselves. The robots utilizing our ISM-Shared variation outperformed those using decentralized MARL. They achieved performance comparable to robots employing centralized MARL, where complete global information is used for decision-making during the execution. Additionally, our ISM-MARL is adaptable across team sizes and consistently maintains high performance when transferring knowledge to teams of varying sizes, without being explicitly learned during the training phase. This suggests a resilient MARL learning technique that can adapt to changing environments.},
  keywords={Training;Decision making;Collaboration;Information sharing;Reinforcement learning;Robot sensing systems;Sensor systems;Deep reinforcement learning;multi-agent system;information-sharing;cooperative robots;Unity ML-Agent Toolkit},
  doi={10.1109/ROSE62198.2024.10590879},
  ISSN={},
  month={June},}@INPROCEEDINGS{10056648,
  author={El Houda, Zakaria Abou and Nabousli, Diala and Kaddoum, Georges},
  booktitle={2022 IEEE Future Networks World Forum (FNWF)}, 
  title={Cost-efficient Federated Reinforcement Learning- Based Network Routing for Wireless Networks}, 
  year={2022},
  volume={},
  number={},
  pages={243-248},
  abstract={Advances in Artificial Intelligence (AI) provide new capabilities to handle network routing problems. However, the lack of up-to-date training data, slow convergence, and low robustness due to the dynamic change of the network topology, makes these AI-based routing systems inefficient. To address this problem, Reinforcement Learning (RL) has been introduced to design more flexible and robust network routing protocols. However, the amount of data ($i$. e., state-action space) shared be- tween agents, in a Multi-Agent Reinforcement Learning (MARL) setup, can consume network bandwidth and may slow down the process of training. Moreover, the curse of dimensionality of RL encompasses the exponential growth of the discrete state-action space, thus limiting its potential benefit. In this paper, we present a novel approach combining Federated Learning (FL) with Deep Reinforcement Learning (D RL) in order to ensure an effective network routing in wireless environment. First, we formalize the problem of network routing as a problem of RL, where multiple agents that are geographically distributed train the policy model in a fully distributed manner. Thus, each agent can quickly obtain the optimal policy that maximizes the cumulative expected reward, while preserving the privacy of each agent's data. Experiments results show that our proposed Federated Reinforcement Learning (FRL) approach is robust and effective.},
  keywords={Training;Data privacy;Federated learning;Wireless networks;Training data;Reinforcement learning;Routing;Network Routing;Federated Learning;Rein-forcement Learning},
  doi={10.1109/FNWF55208.2022.00050},
  ISSN={2770-7679},
  month={Oct},}@INPROCEEDINGS{9960774,
  author={Chupakhin, Andrei and Kazantaev, Alexey},
  booktitle={2022 International Conference on Modern Network Technologies (MoNeTec)}, 
  title={MARL-LDB: Multi-Agent Reinforcement Learning Load Balancing Algorithm in a Distributed Computing System}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={The load balancing problem in a heterogeneous distributed computing system is considered. The problem is mathematically formulated as a stochastic game. As a multi-agent system, the heterogeneous distributed computing system can be viewed as a system with trainable agents at each computational node. There are some computational nodes that receive tasks according to a pre-established distribution (Uniform, Poisson). Tasks must be distributed among computational nodes in such a way that the average load on each computational node's resources (CPU, RAM, Storage) is the same. Based on the Proximal Policy Optimization (PPO) algorithm, a multi-agent reinforcement learning algorithm has been developed to solve the stochastic game. As a result of the experimental results, the proposed algorithm is capable of solving the formulated problem and is stable under various scenarios of computing system changes.},
  keywords={Random access memory;Reinforcement learning;Games;Load management;Distributed computing;Task analysis;Optimization;Load Balancing;Multi-agent systems;Rein-forcement learning;Game theory},
  doi={10.1109/MoNeTec55448.2022.9960774},
  ISSN={},
  month={Oct},}@ARTICLE{10443575,
  author={Zhang, Tong and Gou, Yu and Liu, Jun and Song, Shanshan and Yang, Tingting and Cui, Jun-Hong},
  journal={IEEE Transactions on Mobile Computing}, 
  title={Joint Link Scheduling and Power Allocation in Imperfect and Energy-Constrained Underwater Wireless Sensor Networks}, 
  year={2024},
  volume={23},
  number={10},
  pages={9863-9880},
  abstract={Underwater wireless sensor networks (UWSNs) stand as promising technologies facilitating diverse underwater applications. However, the major design issues of the considered system are the severely limited energy supply and unexpected node malfunctions. This paper aims to provide fair, efficient, and reliable (FER) communication to the imperfect and energy-constrained UWSNs (IC-UWSNs). Therefore, we formulate a FER-communication optimization problem (FERCOP) and propose ICRL-JSA to solve the formulated problem. ICRL-JSA is a deep multi-agent reinforcement learning (MARL)-based optimizer for IC-UWSNs through joint link scheduling and power allocation, which automatically learns scheduling algorithms without human intervention. However, conventional RL methods are unable to address the challenges posed by underwater environments and IC-UWSNs. To construct ICRL-JSA, we integrate deep Q-network into IC-UWSNs and propose an advanced training mechanism to deal with complex acoustic channels, limited energy supplies, and unexpected node malfunctions. Simulation results demonstrate the superiority of the proposed ICRL-JSA scheme with an advanced training mechanism compared to various benchmark algorithms.},
  keywords={Resource management;Training;Optimization;Energy consumption;Wireless sensor networks;Transmitters;Reliability;Link scheduling;power allocation;Underwater Wireless Sensor Networks (UWSNs);multi-agent system (MAS);deep multi-agent reinforcement learning (Deep MARL)},
  doi={10.1109/TMC.2024.3368425},
  ISSN={1558-0660},
  month={Oct},}@INPROCEEDINGS{9891948,
  author={Yang, Guangkai and Chen, Hao and Zhang, Junge and Yin, Qiyue and Huang, Kaiqi},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Multi-Agent Uncertainty Sharing for Cooperative Multi-Agent Reinforcement Learning}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Cooperative multi-agent reinforcement learning has been considered promising to complete many complex cooperative tasks in the real world such as coordination of robot swarms and self-driving. To promote multi-agent cooperation, Centralized Training with Decentralized Execution emerges as a popular learning paradigm due to partial observability and communication constraints during execution and computational complexity in training. Value decomposition has been known to produce competitive performance to other methods in complex environment within this paradigm such as VDN and QMIX, which approximates the global joint Q-value function with multiple local individual Q-value functions. However, existing works often neglect the uncertainty of multiple agents resulting from the partial observability and very large action space in the multi-agent setting and can only obtain the sub-optimal policy. To alleviate the limitations above, building upon the value decomposition, we propose a novel method called multi-agent uncertainty sharing (MAUS). This method utilizes the Bayesian neural network to explicitly capture the uncertainty of all agents and combines with Thompson sampling to select actions for policy learning. Besides, we impose the uncertainty-sharing mechanism among agents to stabilize training as well as coordinate the behaviors of all the agents for multi-agent cooperation. Extensive experiments on the StarCraft Multi-Agent Challenge (SMAC) environment demonstrate that our approach achieves significant performance to exceed the prior baselines and verify the effectiveness of our method.},
  keywords={Training;Uncertainty;Robot kinematics;Neural networks;Buildings;Reinforcement learning;Behavioral sciences;Multi-Agent System;Bayesian Neural Network;Uncertainty;Exploration},
  doi={10.1109/IJCNN55064.2022.9891948},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{9893608,
  author={Wang, Jitao and Xue, Dongyun and Zhao, Jian and Zhou, Wengang and Li, Houqiang},
  booktitle={2022 IEEE Conference on Games (CoG)}, 
  title={Mastering the Game of 3v3 Snakes with Rule-Enhanced Multi-Agent Reinforcement Learning}, 
  year={2022},
  volume={},
  number={},
  pages={229-236},
  abstract={As a popular game around the world, Snakes has multiple modes with different settings. In this work, we are dedicated to the 3v3 Snakes, which is characterized by a complex mixture of competition and cooperation. To address this mode of Snakes, most existing AI agents adopt rule based methods, which achieve limited performance due to human’s oversight of some special circumstances. Inspired by the superiority of multi-agent reinforcement learning (MARL), we propose a rule-enhanced multi-agent reinforcement learning algorithm and build a 3v3 Snakes AI. Specifically, we introduce the territory matrix which is commonly utilized in rule based methods to the state features and mask the illegal actions through designed rules. The relationships of individual-team and friends-foes are also merged into reward design. Trained with Distributed PPO and self-play on a single GeForce RTX 2080 GPU for twenty-four hours, our AI achieves state-of-the-art performance and beats human players. On JIDI platform, our agent outperforms the other 132 participating agents and ranks the first for more than 20 consecutive days.},
  keywords={Graphics processing units;Games;Reinforcement learning;Benchmark testing;Artificial intelligence;Snakes;Multi-Agent System;Game;Reinforcement Learning},
  doi={10.1109/CoG51982.2022.9893608},
  ISSN={2325-4289},
  month={Aug},}@INPROCEEDINGS{9838588,
  author={Chang, Qi and Jiang, Yanxiang and Zheng, Fu-Chun and Bennis, Mehdi and You, Xiaohu},
  booktitle={ICC 2022 - IEEE International Conference on Communications}, 
  title={Cooperative Edge Caching via Multi Agent Reinforcement Learning in Fog Radio Access Networks}, 
  year={2022},
  volume={},
  number={},
  pages={3641-3646},
  abstract={In this paper, the cooperative edge caching problem in fog radio access networks (F-RANs) is investigated. To minimize the content transmission delay, we formulate the cooperative caching optimization problem to find the globally optimal caching strategy. By considering the non-deterministic polynomial hard (NP-hard) property of this problem, a Multi Agent Reinforcement Learning (MARL)-based cooperative caching scheme is proposed. Our proposed scheme applies a double deep Q-network (DDQN) in every fog access point (F-AP), and introduces the communication process in a multi-agent system. Every F-AP records the historical caching strategies of its associated F-APs as the observations of communication procedure. By exchanging the observations, F-APs can leverage the cooperation and make the globally optimal caching strategy. Simulation results show that the proposed MARL-based cooperative caching scheme has remarkable performance compared with the benchmark schemes in minimizing the content transmission delay.},
  keywords={Cooperative caching;Simulation;Conferences;Reinforcement learning;Benchmark testing;Delays;Recording;Fog radio access networks;cooperative edge caching;multi agent reinforcement learning;double deep Q-network},
  doi={10.1109/ICC45855.2022.9838588},
  ISSN={1938-1883},
  month={May},}@INPROCEEDINGS{9549970,
  author={Xu, Jianyou and Zhang, Zhichao and Zhang, Shuo and Miao, Jiayao},
  booktitle={2021 40th Chinese Control Conference (CCC)}, 
  title={An Improved Traffic Signal Control Method Based on Multi-agent Reinforcement Learning}, 
  year={2021},
  volume={},
  number={},
  pages={6612-6616},
  abstract={Area traffic signal control is important to alleviate urban traffic congestion. In this paper, we propose an improved multi-agent proximal policy optimization (MAPPO) algorithm via combine intrinsic curiosity module and proximal policy optimization to control area traffic signal. In the proposed algorithm, a multi-intersection traffic network is modeled as a multi-agent system and each agent is trained to search the optimal strategy. We validate our algorithm performance on the simulation of mobility (SUMO) platform. Experimental results show that the proposed algorithm can effectively reduce queue lengths and waiting time. Also, the performance of our algorithm is superior to MAPPO and fixed-time control.},
  keywords={Reinforcement learning;Optimization;Traffic congestion;Multi-agent systems;Traffic Signal Control;Deep Reinforcement Learning;Proximal Policy Optimization;Intrinsic Curiosity Module},
  doi={10.23919/CCC52363.2021.9549970},
  ISSN={1934-1768},
  month={July},}@ARTICLE{10818445,
  author={Chang, Qi and Jiang, Yanxiang and Huang, Yige and Zheng, Fu-Chun and Niyato, Dusit and You, Xiaohu},
  journal={IEEE Transactions on Communications}, 
  title={Multi-Agent Reinforcement Learning Based Cooperative Caching With Low Entropy Communications in Fog-RANs}, 
  year={2025},
  volume={73},
  number={8},
  pages={5935-5949},
  abstract={In this paper, we investigate a cooperative edge caching problem in the fog radio access networks (F-RANs). In order to obtain the globally optimal caching strategy that minimizes the content transmission delay and maximizes communication efficiency, we propose a multi-agent reinforcement learning based cooperative caching policy with low entropy communications. First, we propose a double deep Q network (DDQN) based caching policy by taking into account the non-deterministic polynomial hard (NP-hard) aspect of this cooperative caching optimization problem. Then, we extend the state transition model of Markov Decision Process (MDP) under the single agent system into the Stochastic Game (SG) one under the multi-agent system. By employing the DDQN in each agent, the agents can learn and make the global decision for caching. For utilizing the cooperation resources of fog access points (F-APs), the interaction of information is introduced to exchange the historical cache records of cooperative F-APs. However, the information in the interaction may require lower entropy in the fiber link. Therefore, the information entropy is largely reduced to improve the communication efficiency by quantifying the information. Finally, due to the non-computable gradient of information entropy, we apply a pseudo gradient descent method to approximate the gradient descent in the local model. Simulation results show that our policy achieves better performance in terms of reducing the transmission delay and improving the cooperation among F-APs compared to the benchmark policies. Additionally, it is demonstrated that the proposed policy improves communication efficiency without compromising the performance of cooperative caching.},
  keywords={Cooperative caching;Entropy;Wireless networks;Servers;Optimization;Information entropy;Delays;Training;Laboratories;Simulation;F-RANs;cooperative caching;multi-agent reinforcement learning;low entropy communications},
  doi={10.1109/TCOMM.2024.3524024},
  ISSN={1558-0857},
  month={Aug},}@ARTICLE{8846699,
  author={Qie, Han and Shi, Dianxi and Shen, Tianlong and Xu, Xinhai and Li, Yuan and Wang, Liujing},
  journal={IEEE Access}, 
  title={Joint Optimization of Multi-UAV Target Assignment and Path Planning Based on Multi-Agent Reinforcement Learning}, 
  year={2019},
  volume={7},
  number={},
  pages={146264-146272},
  abstract={One of the major research topics in unmanned aerial vehicle (UAV) collaborative control systems is the problem of multi-UAV target assignment and path planning (MUTAPP). It is a complicated optimization problem in which target assignment and path planning are solved separately. However, recalculation of the optimal results is too slow for real-time operations in dynamic environments because of the large number of calculations required. In this paper, we propose an artificial intelligence method named simultaneous target assignment and path planning (STAPP) based on a multi-agent deep deterministic policy gradient (MADDPG) algorithm, which is a type of multi-agent reinforcement learning algorithm. In STAPP, the MUTAPP problem is first constructed as a multi-agent system. Then, the MADDPG framework is used to train the system to solve target assignment and path planning simultaneously according to a corresponding reward structure. The proposed system can deal with dynamic environments effectively as its execution only requires the locations of the UAVs, targets, and threat areas. Real-time performance can be guaranteed as the neural network used in the system is simple. In addition, we develop a technique to improve the training effect and use experiments to demonstrate the effectiveness of our method.},
  keywords={Path planning;Heuristic algorithms;Optimization;Training;Reinforcement learning;Task analysis;Unmanned aerial vehicles;Multi-UAV;target assignment and path planning;multi-agent reinforcement learning;MADDPG;dynamic environments},
  doi={10.1109/ACCESS.2019.2943253},
  ISSN={2169-3536},
  month={},}@ARTICLE{10963687,
  author={Ma, Xiao and Li, Wu-Jun},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Grey-Box Adversarial Attack on Communication in Communicative Multi-Agent Reinforcement Learning}, 
  year={2025},
  volume={20},
  number={},
  pages={4679-4693},
  abstract={Effective communication is a necessary condition for intelligent agents to collaborate in multi-agent environments. Although increasing attention has been paid to communicative multi-agent reinforcement learning (CMARL), the vulnerability of the communication mechanism in CMARL has not been well investigated, especially when there exist malicious agents that send adversarial communication messages to other regular agents. Existing works about adversarial communication in CMARL focus on black-box attacks where the attacker cannot access any model within the multi-agent system (MAS). However, grey-box attacks are a type of more practical attack, where the attacker has access to the models of its controlled agents. To the best of our knowledge, no research has been conducted to investigate grey-box attacks on communication in CMARL. In this paper, we propose the first grey-box attack method on communication in CMARL, which is called victim-simulation based adversarial attack (VSAA). At each timestep, the attacker simulates a victim attacked by other regular agents’ communication messages and generates adversarial perturbations on its received communication messages. The attacker then sends the aggregation of these perturbations to the regular agents through communication messages, which will induce non-optimal actions of the regular agents and subsequently degrade the performance of the MAS. Experimental results on multiple tasks show that VSAA can effectively degrade the performance of the MAS. The findings in this paper will make researchers aware of the grey-box attack in CMARL.},
  keywords={Robots;Robot kinematics;Collision avoidance;Closed box;Training;Reinforcement learning;Glass box;Intelligent agents;Collaboration;Broadcasting;Multi-agent;reinforcement learning;communication;grey-box attack},
  doi={10.1109/TIFS.2025.3560203},
  ISSN={1556-6021},
  month={},}@INPROCEEDINGS{9107997,
  author={Huang, Yixin and Wu, Shufan and Mu, Zhongcheng and Long, Xiangyu and Chu, Sunhao and Zhao, Guohong},
  booktitle={2020 6th International Conference on Control, Automation and Robotics (ICCAR)}, 
  title={A Multi-agent Reinforcement Learning Method for Swarm Robots in Space Collaborative Exploration}, 
  year={2020},
  volume={},
  number={},
  pages={139-144},
  abstract={Deep-space exploration missions are known as particularly challenging with high risk and cost, as they operate in environments with high uncertainty. The fault of exploration robot can even cause the whole mission to failure. One of the solutions is to use swarm robots to operate missions collaboratively. Compared with a single capable robot, a swarm of less sophisticated robots can cooperate on multiple and complex tasks. Reinforcement learning (RL) has made a variety of progress in multi-agent system autonomous cooperative control domains. In this paper, we construct a collaborative exploration scenario, where a multi-robot system explores an unknown Mars surface. Tasks are assigned to robots by human scientists and each robot takes optimal policies autonomously. The method used to train policies is a multi-agent deep deterministic policy gradient algorithm (MADDPG) and we design an experience sample optimizer to improve this algorithm. The results show that, with the increase of robots and targets number, this method is more efficient than traditional deep RL algorithm in a multi-agent collaborative exploration environment.},
  keywords={Mars;Uncertainty;Costs;Collaboration;Swarm robotics;Reinforcement learning;Space exploration;space exploration;swarm robots;multi-agents;reinforcement learning;MADDPG},
  doi={10.1109/ICCAR49639.2020.9107997},
  ISSN={2251-2446},
  month={April},}@INPROCEEDINGS{11028685,
  author={Wang, Hua and Zhao, Chaoheng and He, Shuo and Li, Chaochao and Xu, Mingliang},
  booktitle={2024 International Conference on Virtual Reality and Visualization (ICVRV)}, 
  title={A Task-driven Group Cooperative Crowd Simulation Framework via Multi Agent Reinforcement Learning}, 
  year={2024},
  volume={},
  number={},
  pages={226-231},
  abstract={In recent years, the research has been ongoing about crowd simulation. In many situations, crowd are usually organized into several groups and proceed to their respective target location, to accomplish something or perform a task together for a duration, and this process may involve communication and cooperation within the group. We define such crowd as a task-driven group cooperative crowd, and focus on the crowd movement and group behavior during this process. The movement process of the crowd involves multiple stages and exhibits different behaviors at different stage. Within this framework, we consider a problem scenario where crowd within the group orderly gathering in front of objects for a duration to perform a task. We propose a data-driven framework based on Multi-Agent Reinforcement Learning(MARL), including task parameter policy learning and curriculum training strategy. We use a set of task parameters to describe the current task information and integrate them into the iterative optimization process of the policy. Based on this, we design agent observation and a weighted reward function, to make the agent recognize the current task stage and learn the corresponding behaviors. Utilizing curriculum training strategy, the training process is divided into multi steps, gradually increasing the training complexity. We conduct a series of experiments to verify the effectiveness and generalization of the framework and design a museum exhibition scenario to verify the practicality. We provide a demo to supplement the more details, https://github.com/ICSRC/2024ICVRV.},
  keywords={Training;Visualization;Focusing;Reinforcement learning;Virtual reality;Museums;Complexity theory;Iterative methods;Optimization;Crowd simulation;Reinforcement learning;Group behavior},
  doi={10.1109/ICVRV62410.2024.00048},
  ISSN={2473-571X},
  month={Dec},}@ARTICLE{10456565,
  author={Li, Kuo and Jia, Qing-Shan},
  journal={IEEE Transactions on Automation Science and Engineering}, 
  title={Multi-Agent Reinforcement Learning With Decentralized Distribution Correction}, 
  year={2025},
  volume={22},
  number={},
  pages={1684-1696},
  abstract={This work considers decentralized multi-agent reinforcement learning (MARL), where the global states and rewards are assumed to be fully observable, while the local behavior policy is preserved locally for resisting adversarial attack. In order to cooperatively accumulate more rewards, the agents exchange messages among a time-varying communication network to reach consensus. For these cooperative tasks, we propose a decentralized actor-critic algorithm, where the agents make individual decisions, but the joint behavior policy is optimized towards more cumulative rewards. We provide the theoretical analysis towards the convergence under the tabular setting and then expand it to nonlinear function approximations. Furthermore, by incorporating decentralized distribution correction, the agents are trained in an off-policy manner for higher sample efficiency. Finally, we conduct experiments to evaluate the algorithms, where the proposed algorithm performs competitively in both stability and asymptotic performance. Note to Practitioners—Fully decentralized MARL algorithms are widely applied in multi-agent systems for generating cooperative behaviors, e.g., multiple unmanned aerial vehicles (UAV) cooperatively performing search and rescue tasks, multiple vehicles efficiently passing a crowded intersection, and multiple robots cooperatively handling cargo or obstacles. Focusing on these potential applications, this work is motivated to improve the sample efficiency of recent decentralized MARL algorithms by incorporating off-policy training approaches. In this work, we reweight historical trajectories via a decentralized average consensus step and develop corresponding policy-optimization procedures, with which previous trajectories could be used to stabilize later iterations. Since the training materials are augmented by historical samples, the sample efficiency is significantly improved, and the training process is stabilized. With the fully decentralized training approach, the proposed algorithms are expected to be applied in large-scale systems, e.g., vehicle teams and UAV groups, for effective real-time control.},
  keywords={Behavioral sciences;Training;Multi-agent systems;Task analysis;Approximation algorithms;Trajectory;Convergence;Multi-agent system;reinforcement learning;consensus;distribution correction},
  doi={10.1109/TASE.2024.3369592},
  ISSN={1558-3783},
  month={},}@INPROCEEDINGS{9742006,
  author={Yu, Boyang and Cai, Zhaonian and He, Jingbo},
  booktitle={2021 2nd International Conference on Electronics, Communications and Information Technology (CECIT)}, 
  title={Fast-QMIX: Accelerating Deep Multi-Agent Reinforcement Learning with Virtual Weighted Q-values}, 
  year={2021},
  volume={},
  number={},
  pages={594-599},
  abstract={Cooperation between agents in a multi-agent system (MAS) is prevalent in real or virtual environments. In a fully cooperative environment, each agent learns its own policy by using the overall reward. QMIX adds an additional parametric network to replace the linear summation of values in Value-Decomposition Networks (VDNs), aiming to learn more complex relationships between agents. However, QMIX can not determine the best value for each agent during the process, resulting in slower convergence, instability of training, and poor performance. In this paper, we further dynamically assign virtual weighted Q-values based on QMIX with an additional network to further utilize the global information as an auxiliary guide and expand the ability to explore while also expand the robustness of the algorithm. Experiments show that our method not only outperforms original QMIX in different scenarios of the StarCraft Multi-Agent Challenge (SMAC) but also converges faster and more stable with the addition of calibration.},
  keywords={Training;Heuristic algorithms;Virtual environments;Collaboration;Reinforcement learning;Robustness;Information and communication technology;Deep reinforcement learning;Multi-agent;DRQN;QMIX;SMAC},
  doi={10.1109/CECIT53797.2021.00110},
  ISSN={},
  month={Dec},}@ARTICLE{10528314,
  author={Zhang, Shupei and Shi, Huapeng and Zhang, Wei and Pang, Ying and Sun, Pengju},
  journal={IEEE Access}, 
  title={Real-Time Multi-Vehicle Scheduling in Tasks With Dependency Relationships Using Multi-Agent Reinforcement Learning}, 
  year={2024},
  volume={12},
  number={},
  pages={81453-81470},
  abstract={With the advancement of technology in vehicle-road collaboration and autonomous driving, new commercial applications have surfaced. These include autonomous ride-hailing vehicles and unmanned delivery vehicles. As a result of the challenges presented by commercial applications, dispatching systems are moving towards being maintenance-free, centralized, multitasking, and real-time. Yet, most existing dispatching systems have been designed for single-task purposes and cannot tackle multitasking issues. Moreover, traditional optimization algorithms make it difficult to achieve timeliness in real-time changing traffic conditions. Therefore, this paper innovatively proposes a task allocation method based on Multi-Agent Reinforcement Learning (MARL). Firstly, this study introduces a classification model of task relationships through the binary assumption model of geographical areas and vehicles. Secondly, the study matches the classification model’s task cost state transition process with the Markov Decision Process, constructing a Multi-Agent Reinforcement Learning framework. Finally, the study constructs a simulation environment suitable for reinforcement learning based on Simulation of Urban Mobility (SUMO). Simulation results indicate that the task allocation system based on MARL can effectively improve the system’s overall efficiency by determining the order of task allocation and the matching relationships between tasks.},
  keywords={Task analysis;Reinforcement learning;Resource management;Costs;Heuristic algorithms;Robots;Real-time systems;Multi-agent systems;Urban areas;Combinatorial optimization;multi-agent system;reinforcement learning;simulation of urban mobility;task allocation},
  doi={10.1109/ACCESS.2024.3399610},
  ISSN={2169-3536},
  month={},}@ARTICLE{10616015,
  author={He, Guojun and Zhang, Shengyu and Feng, Mingjie and Li, Silan and Jiang, Tao},
  journal={IEEE Transactions on Wireless Communications}, 
  title={Age of Incorrect Information-Aware Data Dissemination for Distributed Multi-Agent Systems}, 
  year={2024},
  volume={23},
  number={10},
  pages={15705-15718},
  abstract={In this paper, we propose an age of incorrect information (AoII)-aware data dissemination scheme for distributed multi-agent systems (MASs). In the proposed scheme, AoII is utilized to measure the importance of data in terms of timeliness and content. We formulate the joint optimization of time slot allocation and agent selection as a decentralized partially observable Markov decision process (Dec-POMDP), with the objective of minimizing the AoII. To solve the Dec-POMDP, a novel multi-agent reinforcement learning algorithm (DV-MAPPO) is proposed. In particular, to tackle challenges posed by the partial observability of global system information, each agent estimates the global system state using variational inference. Moreover, to improve the accuracy of global system state estimation, each agent is given an intrinsic reward that is dominated by the accuracy of estimates. The proposed data dissemination scheme is implemented and evaluated in various missions. Simulation results show that the proposed data dissemination scheme outperforms traditional data dissemination schemes in terms of AoII. Furthermore, in typical multi-agent collaborative tasks, the proposed scheme facilitates more efficient cooperation among multiple agents compared to the data distribution mechanisms that ignore the importance of data.},
  keywords={Data dissemination;Ad hoc networks;Measurement;Task analysis;Optimization;Wireless communication;Sensors;Age of incorrect information;multi-agent reinforcement learning;data dissemination;distributed multi-agent system;task-oriented communication;artificial intelligence},
  doi={10.1109/TWC.2024.3432780},
  ISSN={1558-2248},
  month={Oct},}@ARTICLE{8865095,
  author={Zheng, Shangfei and Liu, Hong},
  journal={IEEE Access}, 
  title={Improved Multi-Agent Deep Deterministic Policy Gradient for Path Planning-Based Crowd Simulation}, 
  year={2019},
  volume={7},
  number={},
  pages={147755-147770},
  abstract={Deep reinforcement learning (DRL) has been proved to be more suitable than reinforcement learning for path planning in large-scale scenarios. In order to more effectively complete the DRL-based collaborative path planning in crowd evacuation, it is necessary to consider the space expansion problem brought by the increase of the number of agents. In addition, it is often faced with complicated circumstances, such as exit selection and congestion in crowd evacuation. However, few existing works have integrated these two aspects jointly. To solve this problem, we propose a planning approach for crowd evacuation based on the improved DRL algorithm, which will improve evacuation efficiency for large-scale crowd path planning. First, we propose a framework of congestion detection-based multi-agent reinforcement learning, the framework divides the crowd into leaders and followers and simulates leaders with a multi-agent system, it considers the congestion detection area is set up to evaluate the degree of congestion at each exit. Next, under the specification of this framework, we propose the improved Multi-Agent Deep Deterministic Policy Gradient (IMADDPG) algorithm, which adds the mean field network to maximize the returns of other agents, enables all agents to maximize the performance of a collaborative planning task in our training period. Then, we implement the hierarchical path planning method, which upper layer is based on the IMADDPG algorithm to solve the global path, and lower layer uses the reciprocal velocity obstacles method to avoid collisions in crowds. Finally, we simulate the proposed method with the crowd simulation system. The experimental results show the effectiveness of our method.},
  keywords={Path planning;Reinforcement learning;Planning;Collaboration;Multi-agent systems;Task analysis;Training;Deep reinforcement learning;multi-agent reinforcement learning;path planning;crowd simulation for evacuation;improved multi-agent deep deterministic policy gradient algorithm},
  doi={10.1109/ACCESS.2019.2946659},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10394147,
  author={Si, Ruiqi and Qiao, Ji and Wang, Xiaohui and Ji, Kaixuan and Wang, Zibo and Jun, Zhang and Pan, Xuanying and Zhang, Zhengyan},
  booktitle={2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={A Transferable Multi-Agent Reinforcement Learning Method for Distribution Service Restoration}, 
  year={2023},
  volume={},
  number={},
  pages={1866-1871},
  abstract={The occurrence of extreme events, which has increased the risk of major outages in the grid, makes the quick and efficient recovery of load in the distribution network become a key issue. The data-driven deep reinforcement learning method has great potential in providing fast decision-making. However, a large number of agents lead to the curse of dimensionality, making it inefficient to obtain effective control strategies. When tackling similar tasks in different power gird, the retraining of multiple agents will bring us great cost. To solve this problem, we propose a transferable multi-agent reinforcement learning framework that employs model reload and buffer reuse methods to transfer control strategies from small-scale simple scenes to large-scale complex scenes. It also utilizes attention mechanisms to aggregate observation features and handles the problem of variable observation dimensions. Finally, the distribution service restoration problem is modeled as a Markov decision process and solved using the QMIX algorithm. The performance of the proposed method has been verified in IEEE 34-node and IEEE 123-node distribution systems.},
  keywords={Deep learning;Decision making;Reinforcement learning;Distribution networks;Markov processes;Task analysis;Testing;Distribution network;service restoration;deep reinforcement learning;multi-agent system;transferability},
  doi={10.1109/SMC53992.2023.10394147},
  ISSN={2577-1655},
  month={Oct},}@INPROCEEDINGS{9358727,
  author={Shyalika, Chathurangi and Silva, Thushari},
  booktitle={2021 6th International Conference on Inventive Computation Technologies (ICICT)}, 
  title={Reinforcement learning based an Integrated Approach for Uncertainty Scheduling in Adaptive Environments using MARL}, 
  year={2021},
  volume={},
  number={},
  pages={1204-1211},
  abstract={Scheduling is a universal theme being conferred in technological areas like computing and strategic areas like operational management. The core idea behind scheduling is the distribution of shared resources across time for competitive tasks. Optimization, efficiency, productivity and performance are the major metrics evaluated in scheduling. Effective scheduling under uncertainty is tricky and unpredictable and its an interesting area to study. Environmental uncertainty is a challenging extent that effect scheduling based decision making in work environments where environment dynamics subject to numerous fluctuations frequently. Reinforcement Learning is an emerging field extensively research on environmental modelling under uncertainty. Optimization in dynamic scheduling can be effectively handled using Reinforcement learning. This paper presents a research that accompanies Multi-agent reinforcement learning (MARL) and extends Q-learning, which aims to provide a solution for the dynamic task scheduling in an uncertain environment. The proposed solution approaches as a model-based Deep Dyna-Q+ algorithm-based hybrid solution to solve the scheduling problem in an unexpected complex environment. The Dyna-Q+ based scheduler includes priority-indexing based task scheduler which allocates jobs to individual cooperative agents sequentially in adaptive mode. The proposed model-based solution is comparatively evaluated against benchmark methods; Q-Learning and Dyna-Q+ learning algorithms for dynamic task scheduling. The initial evaluation results verify that the approach would be an effective solution for scheduling optimization in a dynamic and complex environment.},
  keywords={Uncertainty;Processor scheduling;Heuristic algorithms;Reinforcement learning;Dynamic scheduling;Task analysis;Optimization;Reinforcement learning;Dynamic task scheduling;Model-based approach;Multi-agent RL;Uncertainty scheduling},
  doi={10.1109/ICICT50816.2021.9358727},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10363845,
  author={Zhao, Zhitong and Zhang, Ya and Wang, Siying and Qu, Hong},
  booktitle={2023 International Annual Conference on Complex Systems and Intelligent Science (CSIS-IAC)}, 
  title={A Learnable Noise Exploration Method for Multi-Agent Reinforcement Learning}, 
  year={2023},
  volume={},
  number={},
  pages={259-265},
  abstract={Multi-agent reinforcement learning (MARL) has demonstrated to be an attractive approach for addressing the challenge of multi-agent collaboration. Numerous existing MARL algorithms require hand-craft setting of exploration parameters to alleviate instability and low efficiency in the exploration-exploitation dilemma. However, the meticulous hand-craft exploration parameters limited the efficiency and adaptability of algorithms. In this paper, we propose a learnable noise exploration method for multi-agent reinforcement learning. By introducing noise into the individual value network, the proposed method brings about a level of randomness in agents to drive exploration. The parameters of noise are updated along with the network through gradient descent, avoiding the handcraft setting of exploration parameters. The experiment results demonstrate the effectiveness of our method in the single-state matrix game, and show the stability and significant performance improvement in the predator-prey game and hard multi-agent particle environment.},
  keywords={Collaboration;Reinforcement learning;Games;Probabilistic logic;Complex systems;Multi-agent systems;Multi-Agent System;Reinforcement Learning;Exploration},
  doi={10.1109/CSIS-IAC60628.2023.10363845},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10818085,
  author={Allred, Christopher and Haight, Jacob and Justice, Chandler and Peterson, Isaac and Scalise, Rosario and Hromadka, Ted and Pusey, Jason and Harper, Mario},
  booktitle={2024 Eighth IEEE International Conference on Robotic Computing (IRC)}, 
  title={Coordinating Search with Foundation Models and Multi-Agent Reinforcement Learning in Complex Environments}, 
  year={2024},
  volume={},
  number={},
  pages={225-230},
  abstract={We present a multi-agent system simulation designed for efficient coordination and collaboration among multiple robots, particularly suited for search operations. This simulation reflects unstructured and complex outdoor scenarios where significant obstruction and occluded terrain surfaces cause difficulties in search. The software integrates well with reinforcement learning (RL) and a centralized Multi-Agent Transformer (MAT) to enable autonomous robots to collect, process, and integrate data. Search, coverage, and complex mobility planning can be tested in this simulation with dynamic and unstructured environments. The project code and videos can be found at https://github.com/DIRECTLab/Coordinating-MAT-Env},
  keywords={Robot kinematics;Software algorithms;Collaboration;Reinforcement learning;Software systems;Transformers;Real-time systems;Robots;Surface treatment;Videos;Isaac Sim;Multi-Agent Simulation;Reinforcement Learning;Unstructured Environment;Simulation;Software},
  doi={10.1109/IRC63610.2024.00046},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9533636,
  author={Xu, Zhiwei and Li, Dapeng and Bai, Yunpeng and Fan, Guoliang},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={MMD-MIX: Value Function Factorisation with Maximum Mean Discrepancy for Cooperative Multi-Agent Reinforcement Learning}, 
  year={2021},
  volume={},
  number={},
  pages={1-7},
  abstract={In the real world, many tasks require multiple agents to cooperate with each other under the condition of local observations. To solve such problems, many multi-agent reinforcement learning methods based on Centralized Training with Decentralized Execution have been proposed. One representative class of work is value decomposition, which decomposes the global joint Q-value Qjtinto individual Q-values Qa to guide individuals' behaviors, e.g. VDN (Value-Decomposition Networks) and QMIX. However, these baselines often ignore the randomness in the situation. We propose MMD-MIX, a method that combines distributional reinforcement learning and value decomposition to alleviate the above weaknesses. Besides, to improve data sampling efficiency, we were inspired by REM (Random Ensemble Mixture) which is a robust RL algorithm to explicitly introduce randomness into the MMD-MIX. The experiments demonstrate that MMD-MIX outperforms prior baselines in the StarCraft Multi-Agent Challenge (SMAC) environment.},
  keywords={Training;Neural networks;Collaboration;Reinforcement learning;Task analysis;Multi-Agent System;Distributional Reinforcement Learning;Coordination and Collaboration},
  doi={10.1109/IJCNN52387.2021.9533636},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{11120591,
  author={Soulé, Julien and Jamont, Jean-Paul and Occello, Michel and Traonouez, Louis-Marie and Théron, Paul},
  booktitle={2025 IEEE 18th International Conference on Cloud Computing (CLOUD)}, 
  title={Streamlining Resilient Kubernetes Autoscaling with Multi-Agent Systems via an Automated Online Design Framework}, 
  year={2025},
  volume={},
  number={},
  pages={43-53},
  abstract={In cloud-native systems, Kubernetes clusters with interdependent services often face challenges to their operational resilience due to poor workload management issues such as resource blocking, bottlenecks, or continuous pod crashes. These vulnerabilities are further amplified in adversarial scenarios, such as Distributed Denial-of-Service attacks (DDoS). Conventional Horizontal Pod Autoscaling (HPA) approaches struggle to address such dynamic conditions, while reinforcement learning-based methods, though more adaptable, typically optimize single goals like latency or resource usage, neglecting broader failure scenarios. We propose decomposing the overarching goal of maintaining operational resilience into failure-specific sub-goals delegated to collaborative agents, collectively forming an HPA Multi-Agent System (MAS). We introduce an automated, four-phase online framework for HPA MAS design: 1) modeling a digital twin built from cluster traces; 2) training agents in simulation using roles and missions tailored to failure contexts; 3) analyzing agent behaviors for explainability; and 4) transferring learned policies to the real cluster. Experimental results demonstrate that the generated HPA MASs outperform three state-of-the-art HPA systems in sustaining operational resilience under various adversarial conditions in a proposed complex cluster.},
  keywords={Training;Cloud computing;Scalability;Reinforcement learning;Manuals;Aerodynamics;Robustness;Resilience;Multi-agent systems;Periodic structures;Adversarial;Horizontal Pod Autoscaling;Multi-Agent Reinforcement Learning;Multi-Agent System Design component;formatting;style;styling;insert},
  doi={10.1109/CLOUD67622.2025.00015},
  ISSN={2159-6190},
  month={July},}@INPROCEEDINGS{8028683,
  author={Fei, Ting and Chen, Xin and Wu, Min and Wang, Chi},
  booktitle={2017 36th Chinese Control Conference (CCC)}, 
  title={Experimental study on decentralized concurrent learning for multi-agent system with complex dynamics}, 
  year={2017},
  volume={},
  number={},
  pages={8373-8378},
  abstract={A cooperative multi-agent system entitles some independent agents to complete complex tasks through coordination and cooperation. Since the dynamics of physical agents are so complex that the environment of learning is indeed stochastic, the paper introduces the decentralized multi-agent reinforcement learning (MARL) algorithm, named as Decentralized Concurrent Learning with Cooperative Policy Exploration (DCL-CPE), in order to solve cooperative learning within stochastic environment. To investigate its feasibility in practical multi-agent systems, the box-pushing test with DCL-CPE is designed with a group of two-wheel driven robots acting as learning agents. Due to physical properties, such as nonholonomic dynamics, rolling and sliding frictions, unreliable sense, rigid body collision, etc., the cooperative learning is a high stochastic learning case. The simulation test in Webots shows that DCL-CPE is good at exploring best cooperative policy in a decentralized way, even as state transition and rewards are all stochastic.},
  keywords={Multi-agent systems;Learning (artificial intelligence);Robot kinematics;Heuristic algorithms;Robot sensing systems;Mobile robots;Multi-agent system;Decentralized cooperative learning;Stochastic environment;Webots},
  doi={10.23919/ChiCC.2017.8028683},
  ISSN={1934-1768},
  month={July},}@INPROCEEDINGS{10012942,
  author={Zhang, Hengxi and Tang, Huaze and Hu, Yuanquan and Wei, Xiaoli and Wu, Chenye and Ding, Wenbo and Zhang, Xiao-Ping},
  booktitle={2022 IEEE 96th Vehicular Technology Conference (VTC2022-Fall)}, 
  title={Heterogeneous Mean-Field Multi-Agent Reinforcement Learning for Communication Routing Selection in SAGI-Net}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={The utilization of heterogeneous end devices such as the low earth orbit (LEO) satellite, unmanned aerial vehicles (UAVs) and ground users (GUs) deployed at different altitudes, known as the space-air-ground integrated network (SAGI-Net), can be quite promising towards a bunch of advanced applications. Whereas, the energy efficiency of the SAGI-Net communication system is a key criterion needed to be improved urgently in consideration that the inappropriate communication routing will undoubtedly cause a huge communication energy cost of the system especially with a large number of communication devices inside. In this paper, we proposed a novel communication routing selection model for the SAGI-Net system and established a heterogeneous multi-agent reinforcement learning (HMF-MARL) framework to optimize the communication energy efficiency of this system, where the mean-field theory was introduced to enhance the ability of classic MARL method while still maintaining a relatively low computational complexity. The experiment results show that the capacity of the heterogeneous multi-agent system has been improved by nearly 80% using the proposed HMF-MARL method compared with the classic MARL one, which hopefully shows the potential value on the implementation of the SAGI-Net system in the future.},
  keywords={Vehicular and wireless technologies;Protocols;Satellites;Costs;Low earth orbit satellites;Reinforcement learning;Routing;SAGI-Net;heterogeneous mean field;MARL;communication routing selection;computational complexity},
  doi={10.1109/VTC2022-Fall57202.2022.10012942},
  ISSN={2577-2465},
  month={Sep.},}@INPROCEEDINGS{9921892,
  author={Song, Bing and Xiong, Gang and Zhu, Fenghua and Wu, Xuke and Lv, Yisheng and Ye, Peijun},
  booktitle={2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)}, 
  title={Empirical Learning of Decision Parameters for Agent-Based Model}, 
  year={2022},
  volume={},
  number={},
  pages={3194-3199},
  abstract={Agent-Based Model (ABM) is a widely used tool to analyze distributed systems. However, the decision-making parameters are difficult to determine, since ABM is a kind of micro model and such parameters, varying from person to person, cannot be measured conveniently in real traffic systems. For this problem, this paper introduces reinforcement learning to empirically and efficiently calculate the micro parameters of ABM. By a parameterization of the individual interactions, our new approach is able to decouple the dependence for a given agent upon his “social neighbors”, and thus can accelerate the learning process. Experiments on inter-city traveling of population indicate that the proposed method is effective for the micro parameter computation.},
  keywords={Computational modeling;Microscopy;Sociology;Reinforcement learning;Probability;Entropy;Calibration},
  doi={10.1109/ITSC55140.2022.9921892},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{11065849,
  author={Zhao, Xinyu and Liu, Jianxiang and Wu, Faguo and Zhang, Xiao},
  booktitle={2025 IEEE 14th Data Driven Control and Learning Systems (DDCLS)}, 
  title={Ensemble critic network with risk estimation for multi-agent reinforcement learning}, 
  year={2025},
  volume={},
  number={},
  pages={1486-1493},
  abstract={Reinforcement learning (RL) allows agents to optimize policies to maximize long-term cumulative rewards in an environment. However, the uncertainty embedded in the environment can lead to biased value estimation and potential risks. Especially in multi-agent systems, the increased complexity of interactions results in more widespread and difficult-to-quantify risk sources. Current risk-sensitive exploration principles for multi-agent scenarios often fail to meet the stability requirements of RL in high-dimensional complex spaces. To address these challenges, we propose the Ensemble Critic network with Risk Estimation for Agent Training framework (ECREAT), which enhances training stability and exploration safety. Focusing on the complex risk issues in multi-agent reinforcement learning (MARL), we introduce an ensemble critic algorithm based on the prediction of future Conditional Value at Risk (CVaR). By designing a novel Q-value update mechanism and multi-agent risk estimation metrics, ECREAT significantly improves safety and exploration efficiency in a wide range of complex tasks. The experimental results on the benchmark platforms indicate that ECREAT demonstrates outstanding overall performance in search tasks with both static and dynamic targets. Moreover, the occurrence of high-risk behaviors is significantly reduced, thereby validating its efficiency and practicality in real-world multi-agent scenarios.},
  keywords={Training;Uncertainty;Sensitivity;Scalability;Estimation;Reinforcement learning;Safety;Complexity theory;Numerical stability;Multi-agent systems;Reinforcement Learning;Multi-agent System;Risk Estimation},
  doi={10.1109/DDCLS66240.2025.11065849},
  ISSN={2767-9861},
  month={May},}@INPROCEEDINGS{9482235,
  author={Wang, Zihao and Zhang, Yanxin and Yin, Chenkun and Huang, Zhiqing},
  booktitle={2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)}, 
  title={Multi-agent Deep Reinforcement Learning based on Maximum Entropy}, 
  year={2021},
  volume={4},
  number={},
  pages={1402-1406},
  abstract={Deep reinforcement learning at the same time combines the perception of deep learning and the decision-making of reinforcement learning, is currently a hot research topic in the field of artificial intelligence. Multi-agent deep reinforcement learning applies the idea and algorithm of deep reinforcement learning to the learning and control of multi-agent system, which is an important method to develop multi-agent system with swarm agent. Multi-agent deep deterministic policy gradient(MADDPG) is the most popular model-free multi-agent reinforcement learning algorithm. To solve the problem of low learning and training efficiency and slow convergence speed of MADDPG due to the deterministic single action output of policy network, this paper combines the maximum reinforcement learning soft actor -critic algorithm to make each agent’s policy network output action with a random strategy and propose a multi-agent deep reinforcement learning algorithm MASAC based on maximum entropy. The experimental results show that the training speed of MASAC is better than that of MADDPG. At the same time, the learning agent has good performance, stable performance and strong anti-interference ability.},
  keywords={Training;Deep learning;Conferences;Decision making;Reinforcement learning;Control systems;Entropy;deep reinforcement learning;multi-agent;deep deterministic policy gradient;soft actor-critic},
  doi={10.1109/IMCEC51613.2021.9482235},
  ISSN={2693-2776},
  month={June},}@INPROCEEDINGS{10922345,
  author={Liu, Chuan and Wang, Tongxu and Chang, Xin and Tang, Henghai and Lv, Da},
  booktitle={2024 IEEE PES 16th Asia-Pacific Power and Energy Engineering Conference (APPEEC)}, 
  title={Power Supply Vehicle Scheduling Method Based on Multi-Agent Reinforcement Learning with Shared Attention}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={With the frequent occurrence of extreme weather events caused by climate change, cities are facing the risk of increasingly frequent large-scale power outages. Due to the lack of intelligence and collaborative dispatching capabilities, the traditional power supply vehicle dispatching method is difficult to quickly restore urban power supply after a disaster, resulting in serious economic losses and social impact. Therefore, improving the efficiency and response speed of power supply vehicle dispatching has become an urgent problem to be solved. This paper proposes an innovative method named multi-agent reinforcement learning method based on a shared attention mechanism (SARL) for efficient dispatching of urban power supply vehicles after disasters. This method comprehensively considers user priorities and actual needs by integrating multiple information sources, including meteorological conditions, geographic information system (GIS) data, user power demand, traffic conditions, etc. During the scheduling process, each power supply vehicle agent shares information and collaborates through the shared attention module to optimize the scheduling path and power supply strategy to maximize power supply efficiency and minimize power outage time. Experimental results show that the proposed method exhibits superior prediction accuracy and decision-making capabilities in complex and changeable environments, and significantly improves the recovery speed and efficiency of urban lifelines after disasters. This not only ensures the life safety of residents and the normal operation of the city, but also provides new ideas and technical support for intelligent emergency resource dispatching.},
  keywords={Attention mechanisms;Power supplies;Disasters;Urban areas;Reinforcement learning;Prediction algorithms;Dispatching;Resource management;Vehicle dynamics;Meteorology;Multi-agent system;reinforcement learning;shared attention mechanism;power supply vehicle scheduling},
  doi={10.1109/APPEEC61255.2024.10922345},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9860518,
  author={Vu, Hung V. and Farzanullah, Mohammad and Liu, Zheyu and Nguyen, Duy H. N. and Morawski, Robert and Le-Ngoc, Tho},
  booktitle={2022 IEEE 95th Vehicular Technology Conference: (VTC2022-Spring)}, 
  title={Multi-Agent Reinforcement Learning for Channel Assignment and Power Allocation in Platoon-Based C-V2X Systems}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={We consider the problem of joint channel assignment and power allocation in underlaid cellular vehicular-to-everything (C-V2X) systems where multiple vehicle-to-network (V2N) uplinks share the time-frequency resources with multiple vehicle-to-vehicle (V2V) platoons that enable groups of connected and autonomous vehicles to travel closely together. Due to the nature of high user mobility in vehicular environment, traditional centralized optimization approach relying on global channel information might not be viable in C-V2X systems with large number of users. Utilizing a multi-agent reinforcement learning (RL) approach, we propose a distributed resource allocation (RA) algorithm to overcome this challenge. Specifically, we model the RA problem as a multi-agent system. Based solely on the local channel information, each platoon leader, acting as an agent, collectively interacts with each other and accordingly selects the optimal combination of sub-band and power level to transmit its signals. Toward this end, we utilize the double deep Q-learning algorithm to jointly train the agents under the objectives of simultaneously maximizing the sum-rate of V2N links and satisfying the packet delivery probability of each V2V link in a desired latency limitation. Simulation results show that our proposed RL-based algorithm provides a close performance compared to that of the well-known exhaustive search algorithm.},
  keywords={Vehicular and wireless technologies;Time-frequency analysis;Q-learning;Simulation;Vehicle-to-infrastructure;Vehicular ad hoc networks;Channel allocation;Vehicle-to-everything;cellular networks;reinforcement learning;resource allocation},
  doi={10.1109/VTC2022-Spring54318.2022.9860518},
  ISSN={2577-2465},
  month={June},}@INPROCEEDINGS{11041166,
  author={Xu, Yungui and Wang, Ke and Yan, Jiahao and Li, Yaping and Mao, Wenbo},
  booktitle={2025 10th Asia Conference on Power and Electrical Engineering (ACPEE)}, 
  title={An Optimization Method for Grid Look-Ahead Dispatch Based on Adaptive Multi-Agent Reinforcement Learning}, 
  year={2025},
  volume={},
  number={},
  pages={400-404},
  abstract={This paper presents an adaptive look-ahead scheduling optimization method for power grids, combining adaptive multi-agent reinforcement learning (MARL) and adaptive K-means clustering. As the integration of renewable energy sources such as wind and solar power increases, their variability and uncertainty pose significant challenges to traditional scheduling methods, especially regarding the scalability and generalization of single-agent reinforcement learning approaches. To address this, the proposed method uses adaptive K-means clustering to classify large-scale input scenarios and assigns them to corresponding agents for parallel training, effectively addressing the generalization limitations of single-agent methods. Additionally, a global shared experience pool is introduced, enabling agents to share experiences, accelerate learning, and improve convergence. A case study based on the IEEE 118-bus system demonstrates that the proposed method outperforms single-agent approaches in terms of scheduling efficiency, renewable energy integration, and computational time.},
  keywords={Training;Electrical engineering;Renewable energy sources;Uncertainty;Processor scheduling;Scalability;Optimization methods;Power grids;Computational efficiency;Multi-agent systems;Look-ahead Dispatch;Multi-Agent System;Adaptive Clustering Model;Deep Reinforcement Learning},
  doi={10.1109/ACPEE64358.2025.11041166},
  ISSN={2996-2951},
  month={April},}@ARTICLE{9829316,
  author={Qin, Zeyu and Yao, Haipeng and Mai, Tianle and Wu, Di and Zhang, Ni and Guo, Song},
  journal={IEEE Transactions on Services Computing}, 
  title={Multi-Agent Reinforcement Learning Aided Computation Offloading in Aerial Computing for the Internet-of-Things}, 
  year={2023},
  volume={16},
  number={3},
  pages={1976-1986},
  abstract={LEO satellite networks have become a necessary supplement to terrestrial networks aiming to provide worldwide, ubiquitous connectivity, especially in complicated areas (e.g., mountains, oceans, and disaster areas) where terrestrial network infrastructures are typically sparingly distributed or unavailable. However, the increasing computation-intensive Internet-of-Things (IoT) applications (e.g., real-time remote monitoring, intelligent transportation) require not only efficient and reliable communication but also massive computing capabilities. Constrained by the battery and computing resources, the computing tasks and data of applications have to be transmitted to remote cloud servers. This bandwidth limitation and high transmission delay in LEO networks will reduce the quality-of-service (QoS) of IoT applications. Recently, the combination of LEO networks and edge computing (i.e., Satellite Mobile Edge Computing, SMEC) offers significant opportunities to address these problems. The IoT devices can directly get the computing resources directly from satellites rather than remote servers, thus avoiding long-distance transmission. Considering the resource constraints on satellites, offloading policy plays a crucial role in whole system performance. In this paper, we design a hybrid offloading architecture, which applies a centralized training and distributed execution framework. Also, we propose a multi-agent actor-critic reinforcement learning algorithm, where a centralized “critic” is augmented with the global network state to ease the training procedure of distributed user equipments (UE) by evaluating the benefits of their decisions, while the UEs can adjust their policies according to the critic's evaluation and choose their own decisions relying on their observations.},
  keywords={Satellites;Task analysis;Servers;Delays;Low earth orbit satellites;Computer architecture;Computational modeling;Aerial computing;computation offloading;deep reinforcement learning;mobile edge computing;multi-agent system},
  doi={10.1109/TSC.2022.3190562},
  ISSN={1939-1374},
  month={May},}@ARTICLE{7855760,
  author={Singh, Vijay Pratap and Kishor, Nand and Samuel, Paulson},
  journal={IEEE Transactions on Industrial Electronics}, 
  title={Distributed Multi-Agent System-Based Load Frequency Control for Multi-Area Power System in Smart Grid}, 
  year={2017},
  volume={64},
  number={6},
  pages={5151-5160},
  abstract={This paper presents an intelligent controller for “load frequency control (LFC)” application in “smart grid (SG)”environment having changes in communication topology (CT) via a multi-agent system (MAS) technology. In this study, network-induced effects, time delay, and change in CT have been addressed to examine the system performance in a closed loop. An event-triggered control method is used to reduce the communication burden in a network. An intelligent controller based on reinforcement learning consists of two levels, estimator agent and controller agent, in each multi-area system. Particle swarm optimization is used to tune the controller parameters. Furthermore, the proposed control strategy and system architecture as MAS for LFC in SG are analyzed in detail, verified for various load conditions and different network configurations. In addition, mean-square error of the power system states with CT is also analyzed. The results of this study validate the feasibility of the proposed control, as well as the capability of the MAS for the operation of LFC in SG with changes in CT.},
  keywords={Smart grids;Power system dynamics;Power system stability;Frequency control;Communication networks;Network topology;Communication topology (CT);load frequency control (LFC);multi-agent reinforcement learning (MARL);smart grid (SG)},
  doi={10.1109/TIE.2017.2668983},
  ISSN={1557-9948},
  month={June},}@INPROCEEDINGS{10761284,
  author={Lang, Junyi and Zheng, Xiaokang and Sun, Yimeng and Ding, Zhaohao},
  booktitle={2024 IEEE/IAS Industrial and Commercial Power System Asia (I&CPS Asia)}, 
  title={Online Job Scheduling for Energy Cost Optimization in Geo-Distributed Data Centers Considering Data Locality: A Multi-Agent Reinforcement Learning Approach}, 
  year={2024},
  volume={},
  number={},
  pages={748-753},
  abstract={With the rapid growth of cloud computing, geo-distributed data centers are becoming more energy-intensive. Given the unique characteristics of computing jobs, they can be executed across data centers in different geographic locations to better utilize lower electricity prices. However, data locality is a crucial factor to be considered in this process, as it is a precondition for the execution of jobs. In this paper, we propose a multi-agent deep reinforcement learning-based job scheduling algorithm that addresses constraints such as data locality and job deadlines to achieve real-time scheduling of jobs and data across data centers to optimize energy costs. First, we formulate the data locality-aware job scheduling problem across geo-distributed data centers as a Partially Observable Markov Decision Process (POMDP). Then, we develop a multi-agent system where agents handle job scheduling and data transfer respectively. The job scheduling agent needs to consider data transfer conditions when taking actions, as the necessary data for job execution must be available before executing. These agents collaborate by sharing the reward function to optimize the scheduling strategy, reducing the overall energy costs of job execution and data transfer. Finally, we conduct numerical experiments to validate the effectiveness of our proposed scheduling method in reducing data center energy costs while maintaining high service quality.},
  keywords={Data centers;Costs;Scheduling algorithms;Heuristic algorithms;Scalability;Reinforcement learning;Data transfer;Scheduling;Robustness;Optimization;Geo-distributed data centers;power consumption;reinforcement learning;data locality;job scheduling},
  doi={10.1109/ICPSAsia61913.2024.10761284},
  ISSN={},
  month={July},}
