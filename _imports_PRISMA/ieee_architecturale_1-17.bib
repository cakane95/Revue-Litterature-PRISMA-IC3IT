@INPROCEEDINGS{10805199,
  author={Kobayashi, Kunikazu and Kishi, Fumiya},
  booktitle={2024 SICE Festival with Annual Conference (SICE FES)}, 
  title={A Minecraft Agent Based on Hierarchical Deep Reinforcement Learning Model}, 
  year={2024},
  volume={},
  number={},
  pages={570-575},
  abstract={A deep reinforcement learning model named MineCLIP has been proposed for solving a given task in imperfect information game Minecraft by learning to associate gameplay videos corresponding to linguistic goals. The goal of the present research is to improve the success rate of the task by applying a hierarchical reinforcement learning framework to the linguistic guidance in MineCLIP, which decomposes the linguistic goal of a given task into steps. MineCLIP uses policy gradient PPO as a reinforcement learning algorithm. The proposed method uses a hierarchy of PPOs in MineCLIP to assign a single sub-policy to each language guidance, which allows step-by-step learning and facilitates task success. Using Minecraft simulation environment, i.e. MineDojo, we confirmed that the proposed method can improve the success rate of solving tasks compared to MineCLIP.},
  keywords={Computer simulation;Computational modeling;Games;Linguistics;Deep reinforcement learning;Iron;Time factors;Videos;Minecraft;Imperfect information game;Game AI;AI agent;Hierarchical reinforcement learning;Deep reinforcement learning;MineCLIP;PPO},
  doi={},
  ISSN={},
  month={Aug},}@ARTICLE{10547029,
  author={Lan, Xi and Qiao, Yuansong and Lee, Brian},
  journal={IEEE Access}, 
  title={Multiagent Hierarchical Reinforcement Learning With Asynchronous Termination Applied to Robotic Pick and Place}, 
  year={2024},
  volume={12},
  number={},
  pages={78988-79002},
  abstract={Recent breakthroughs in hierarchical multi-agent deep reinforcement learning (HMADRL) are propelling the development of sophisticated multi-robot systems, particularly in the realm of complex coordination tasks. These advancements hold significant potential for addressing the intricate challenges inherent in fast-evolving sectors such as intelligent manufacturing. In this study, we introduce an innovative simulator tailored for a multi-robot pick-and-place (PnP) operation, built upon the OpenAI Gym framework. Our aim is to demonstrate the efficacy of HMADRL algorithms for multi robot coordination in a manufacturing setting, concentrating on their influence on the gripping rate, a crucial indicator for gauging system performance and operational efficiency.},
  keywords={Robots;Robot kinematics;Task analysis;Training;Multi-robot systems;Productivity;Navigation;Multi-agent system;pick and place;multi-agent-hierarchical reinforcement learning;multi-robot system;asynchronous termination},
  doi={10.1109/ACCESS.2024.3409076},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10667129,
  author={Montgomery, Bennet and Muise, Christian and Givigi, Sidney},
  booktitle={2024 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)}, 
  title={Hierarchical Deep Reinforcement Learning with Cross-attention and Planning for Autonomous Roundabout Navigation}, 
  year={2024},
  volume={},
  number={},
  pages={417-423},
  abstract={Autonomous vehicle control is an important subfield of autonomous vehicle research. Many challenges remain to improve the safety and performance of autonomous vehicle control systems in urban driving environments. One such urban driving environment is the roundabout junction, which presents its own unique challenges to potential solutions to autonomous vehicle control. This paper proposes and tests a vehicle control agent as a candidate solution for urban roundabout navigation. The vehicle control agent is based on a hierarchical deep reinforcement learning architecture with a superior network selecting short-term lane-change behaviour and a subordinate network selecting longitudinal acceleration values. The road sequence followed by the agent is selected by a route planner based on Dijkstra’s algorithm. The proposed agent learns to navigate the roundabout environment safely, reaching the goal state in 100% of validation scenarios after training. The agent also outperforms an agent based on the Krauß-following model in 2 out of 5 tested metrics and matches the performance of the Krauß-following model in the remaining 3 metrics.},
  keywords={Training;Measurement;Navigation;Roads;Computer architecture;Deep reinforcement learning;Transformers;Deep Reinforcement Learning;Autonomous vehicles;Transformers;Neural Networks},
  doi={10.1109/CCECE59415.2024.10667129},
  ISSN={2576-7046},
  month={Aug},}@ARTICLE{10665939,
  author={Fu, Qingxu and Qiu, Tenghai and Yi, Jianqiang and Pu, Zhiqiang and Ai, Xiaolin},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, 
  title={Self-Clustering Hierarchical Multi-Agent Reinforcement Learning With Extensible Cooperation Graph}, 
  year={2025},
  volume={9},
  number={2},
  pages={1688-1698},
  abstract={Multi-Agent Reinforcement Learning (MARL) has been successful in solving many cooperative challenges. However, classic non-hierarchical MARL algorithms still cannot address various complex multi-agent problems that require hierarchical cooperative behaviors. The cooperative knowledge and policies learned in non-hierarchical algorithms are implicit and not interpretable, thereby restricting the integration of existing knowledge. This paper proposes a novel hierarchical MARL model called Hierarchical Cooperation Graph Learning (HCGL) for solving general multi-agent problems. HCGL has three components: a dynamic Extensible Cooperation Graph (ECG) for achieving self-clustering cooperation; a group of graph operators for adjusting the topology of ECG; and an MARL optimizer for training these graph operators. HCGL's key distinction from other MARL models is that the behaviors of agents are guided by the topology of ECG instead of policy neural networks. ECG is a three-layer graph consisting of an agent node layer, a cluster node layer, and a target node layer. To manipulate the ECG topology in response to changing environmental conditions, four graph operators are trained to adjust the edge connections of ECG dynamically. The hierarchical feature of ECG provides a unique approach to merge primitive actions (actions executed by the agents) and cooperative actions (actions executed by the clusters) into a unified action space, allowing us to integrate fundamental cooperative knowledge into an extensible interface. In our experiments, the HCGL model has shown outstanding performance in multi-agent benchmarks with sparse rewards. We also verify that HCGL can easily be transferred to large-scale scenarios with high zero-shot transfer success rates.},
  keywords={Electrocardiography;Topology;Reinforcement learning;Heuristic algorithms;Training;Network topology;Computational intelligence;Hierarchical MARL;multi-agent system;reinforcement learning},
  doi={10.1109/TETCI.2024.3449873},
  ISSN={2471-285X},
  month={April},}@INPROCEEDINGS{10650175,
  author={Fu, Kang and Zhao, Qingjie},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={A Multi-Agent Deep Reinforcement Learning Framework for the Stable Landing of a Flexibly Connected Three-Node Space Probe}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Exploring asteroids in the solar system is crucial for human survival and development in the future, since the asteroids may contain important clues about the origin of life which can bring important inspiration to humanity. However, due to the weak gravitational force of asteroids, as well as the limited knowledge about them, it is challenging for a space probe to land on these asteroids' surface steadily. To this end, we propose a flexible connected three-node probe to significantly boost the landing stability, in which each node is relatively independent in behaviors but constrained with each other. As a multi-agent system with constraints, a cooperative behavior planning and decision-making method is very necessary. In this paper, we propose a multi-agent deep reinforcement learning (MADRL) framework for addressing the stable landing of a flexibly connected three-node space probe. In order to overcome the uncertainty of the environment, a training paradigm of centralized training decentralized execution (CTDE) is adopted, where the information exchange between agents is taken into account. Moreover, we integrate gated recurrent unit (GRU) modules into actor and critic networks to preserve the historical information, so that the learned strategy can apply to uncertain asteroid environments and implement stable probe landing. Experimental results demonstrate that the proposed method outperforms other reinforcement learning-based methods in terms of convergence and stability.},
  keywords={Training;Uncertainty;Asteroids;Land surface;Feature extraction;Deep reinforcement learning;Solar system;Multi-agent deep reinforcement learning;centralized training decentralized execution;gated recurrent unit},
  doi={10.1109/IJCNN60899.2024.10650175},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{11108355,
  author={Ma, Yiheng and Jiang, Feng and Han, Kun and Zhu, Haiqi and Bie, Xiaofeng},
  booktitle={2025 IEEE International Conference on Pattern Recognition, Machine Vision and Artificial Intelligence (PRMVAI)}, 
  title={Multi-Strategy Distillation Based on CTCE and CEDE}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={This study proposes a multi-agent reinforcement learning algorithm that integrates the Centralized Training with Centralized Execution (CTCE) and Centralized Training with Decentralized Execution (CTDE) frameworks to enhance distributed agents' collaboration efficiency and training performance through customized knowledge distillation strategies. To address the challenges of partial observability and credit assignment in multi - agent systems, we design a distillation framework that employs the Multi - Agent Transformer (MAT) as the teacher model and QMIX as the student model, and incorporate dynamic intervention mechanisms (“Early Guidance”, “On Demand Assistance”, and “Key - Point Emphasis” strategies) to optimize the frequency and intensity of teacher guidance during critical decision - making phases. Additionally, we innovatively introduce distributional reinforcement learning methods. We measure the student model's confidence level through the multipeak concentration index (MCI) to facilitate effective teacher intervention. Experimental results demonstrate that our method significantly improves the success rate and reward values of student models in StarCraft II mini - games, validating the effectiveness of dynamic distillation strategies in balancing global collaboration with local adaptability. This approach provides novel insights for multi - agent coordination in complex tasks.},
  keywords={Training;Adaptation models;Collaboration;Reinforcement learning;Dynamic scheduling;Transformers;Robustness;Indexes;Resource management;Observability;CTCE;CTDE;distributional reinforcement learning;knowledge distillation;multi-agent;multi-peak concentration index;QR-QMIX},
  doi={10.1109/PRMVAI65741.2025.11108355},
  ISSN={},
  month={June},}@INPROCEEDINGS{8481888,
  author={Namba, Kazuhide},
  booktitle={2018 Portland International Conference on Management of Engineering and Technology (PICMET)}, 
  title={Structural Analysis for Diffusion using Simulation in Agent-Based Modeling with Multi Micro Factor}, 
  year={2018},
  volume={},
  number={},
  pages={1-8},
  abstract={This paper introduces analysis of structure for diffusion. Because of diffusion as individual level agent-based modeling and multi agent simulation about diffusion are effective. Previous research shows the factor related agent are the consumer behavior and the information network. In this research the other factor was added. As a result of addition, the factor is multi. The factor is micro not macro. Micro means the factor is changed in diffusion process. The factor both multi and micro are reflected to agent-based modeling. Multi agent simulation by agent-based modeling was done. As a result of simulation, we could produce diffusion phenomenon. Because of multi micro factor complicated phenomenon could be produced. Example of complicated phenomenon is chasm or inverse chasm. Complicated phenomenon depends on the factor. Originality about this research is that the structure for diffusion was revealed by consideration of simulation result. The structure for diffusion consists of the consumer behavior and the information network.},
  keywords={Consumer behavior;Simulation;Diffusion processes;Analytical models;Agent-based modeling;Technological innovation;Market research},
  doi={10.23919/PICMET.2018.8481888},
  ISSN={2159-5100},
  month={Aug},}@INPROCEEDINGS{11097641,
  author={Ratnabala, Lavanya and Fedoseev, Aleksey and Peter, Robinroy and Tsetserukou, Dzmitry},
  booktitle={2025 IEEE Intelligent Vehicles Symposium (IV)}, 
  title={MAGNNET: Multi-Agent Graph Neural Network-Based Efficient Task Allocation for Autonomous Vehicles with Deep Reinforcement Learning}, 
  year={2025},
  volume={},
  number={},
  pages={970-975},
  abstract={This paper addresses the challenge of decentralized task allocation within heterogeneous multiagent systems operating under communication constraints. We introduce a novel framework that integrates Graph Neural Networks (GNNs) with a centralized training and decentralized execution (CTDE) paradigm, further enhanced by a tailored Proximal Policy Optimization (PPO) algorithm for multi-agent deep reinforcement learning (MARL). Our approach enables unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) to dynamically allocate tasks efficiently without necessitating central coordination in a 3D grid environment. The framework minimizes total travel time while simultaneously avoiding conflicts in task assignments. For the cost calculation and routing, we employ reservation-based $A^{*}$ and $R^{*}$ path planners. Experimental results revealed that our method achieves a high 92.5% conflict-free success rate, with only a 7.49% performance gap compared to the centralized Hungarian method, while outperforming the heuristic decentralized baseline based on a greedy approach. Additionally, the framework exhibits scalability with up to 20 agents with allocation processing of 2.8 s and robustness in responding to dynamically generated tasks, underscoring its potential for real-world applications in complex multi-agent scenarios.},
  keywords={Training;Three-dimensional displays;Scalability;Deep reinforcement learning;Routing;Graph neural networks;Robustness;Resource management;Optimization;Multi-agent systems;Multi-agent system;Task Allocation;Multi-agent Deep Reinforcement Learning;Graph Neural Network;CTDE;Scalability},
  doi={10.1109/IV64158.2025.11097641},
  ISSN={2642-7214},
  month={June},}@INPROCEEDINGS{11193239,
  author={Zhang, Qi and Cui, He and Li, Xiran},
  booktitle={2025 6th International Conference on Clean Energy and Electric Power Engineering (ICCEPE)}, 
  title={Research on Deep Interactive Collaborative Control Strategy of Source Grid Load Storage for New Power Systems}, 
  year={2025},
  volume={},
  number={},
  pages={458-461},
  abstract={Driven by dual-carbon goals, the power system is shifting to a renewable-led, digital, and distributed architecture. Deep source-grid-load-storage coordination is key to handling renewable volatility and ensuring security. This paper proposes a multi-agent reinforcement learning-based control method and validates it via simulation. It analyzes the interaction characteristics and theoretical basis, constructs a multi-agent model, and designs a collaborative control framework using centralized training and decentralized execution (CTDE). Simulation on the IEEE 33-node system shows the proposed strategy improves renewable absorption, reduces costs, minimizes load peaks and valleys, and enhances voltage stability compared to traditional methods.},
  keywords={Costs;Collaboration;Reinforcement learning;Power system stability;Stability analysis;Robustness;Security;Voltage control;Qualifications;Multi-agent systems;New power system;Collaborative control of source network load storage;Multi agent reinforcement learning;New energy consumption},
  doi={10.1109/ICCEPE66357.2025.11193239},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10240132,
  author={Liu, Ziyi and Fang, Yongchun},
  booktitle={2023 42nd Chinese Control Conference (CCC)}, 
  title={Learning Diverse Control Strategies for Simulated Humanoid Combat via Motion Imitation and Multi-Agent Self-Play}, 
  year={2023},
  volume={},
  number={},
  pages={5595-5600},
  abstract={Human athletes can coordinate their bodies to achieve exquisite strategies with agile and diverse motions in multiplayer competitions, which is a long-standing challenge for robotics and physically embodied artificial agents. In this paper, we propose a hierarchical learning framework that generates diverse control strategies and agile motion for physically simulated humanoids, which can be divided into two stages: learning basic motion skills and learning high-level competitive strategies. The framework decouples low-level control and high-level strategy learning, where the low-level policy is trained via motion imitation and skill discovery objectives to generate agile motion skills, and the high-level policy is trained via prioritized fictitious self-play to generate diverse competitive strategies. Furthermore, we develop and release the world's first physically simulated humanoid combat environment. We evaluate our learning framework in this environment, and experimental results demonstrate that policies learned by our framework can generate both agile motion skills and diverse long-term competitive strategies.},
  keywords={Deep learning;Robot kinematics;Humanoid robots;Reinforcement learning;Aerospace electronics;Control systems;Behavioral sciences;Multi-Agent System;Motion Control;Hierarchical Reinforcement Learning},
  doi={10.23919/CCC58697.2023.10240132},
  ISSN={1934-1768},
  month={July},}@ARTICLE{11021570,
  author={Al-Habashna, Ala’a and Menard, Jon and Wainer, Gabriel and Boudreau, Gary},
  journal={IEEE Access}, 
  title={Decentralized and Joint Resource Allocation, Beamforming, and Beamcombining for 5G Networks With Heterogeneous MARL}, 
  year={2025},
  volume={13},
  number={},
  pages={101491-101506},
  abstract={In this paper, we propose a novel Multi-Agent Reinforcement Learning (MARL) -based paradigm for distributed and joint resource allocation, beamforming (BF), and beam combining of uplink transmissions in 5G networks. The proposed paradigm employs two types of heterogenous agents that learn to perform and optimize different tasks in order to achieve the main objective of the system, as well as the objective of the individual agents. In the proposed paradigm, UEs can be multi-agents that optimize their own resource allocation and BF. In addition to these multi agents (i.e., UEs), the BS is a different type of agent that optimizes the combining of UEs’ transmissions. We developed three different implementations of our proposal using three different MARL algorithms: Independent Q Learners (IQL), Multi-Agent Deep Deterministic Policy Gradient (MADDPG), and QTRAN. Various experiments were conducted to validate the usability of our proposal. Our results show that the proposed paradigm can successfully optimize the task of joint resource allocation, beamforming, and combining. Furthermore, we provide a comparative analysis of the three different implementations, highlighting noteworthy insights into the strengths and limitations of fully distributed algorithms, such as IQL, in comparison to algorithms employing the Centralized Training with Decentralized Execution (CTDE) framework, exemplified by QTRAN and MADDPG.},
  keywords={Resource management;5G mobile communication;Throughput;Interference;Array signal processing;Uplink;Training;Device-to-device communication;Computational complexity;Proposals;Deep reinforcement learning;multi-agent reinforcement learning;distributed resource allocation;beamforming;5G},
  doi={10.1109/ACCESS.2025.3576190},
  ISSN={2169-3536},
  month={},}@ARTICLE{10963677,
  author={Wang, Yu and Li, Huiping and Shen, Qingliang},
  journal={IEEE Transactions on Automation Science and Engineering}, 
  title={A Hierarchical Multi-Task and Multi-Agent Assignment Approach: Learning DQN Strategy From Execution}, 
  year={2025},
  volume={22},
  number={},
  pages={14712-14722},
  abstract={This article investigates the problem of real-time task assignment with heterogeneous agents while considering resource constraints. A hierarchical reinforcement learning-based(HRL) method to address the problem has been proposed. Unlike most existing studies, the method is to assign the tasks to agents such that the resource consumption is minimized while respecting the resource constraints and realizing task distribution balance among agents. At the high level, the real-time task assignment problem within a heterogeneous multi-agent system is formulated as a sequence optimization model which considers multiple constraints. At the low level, a novel reinforcement learning-based hierarchical framework that decomposes the large-scale task assignment problem into the target assignment layer and resource assignment layer is developed. In the first layer, a task assignment strategy closer to the optimal one in real-time by reducing the dimension of the DQN state-action space and learning from execution is produced. In the second layer, a novel resource assignment method is designed to minimize resource consumption and reserve as many agents as possible to handle new tasks by optimizing the resource distribution among agents. Simulation experiments in multi-UAV collaborative emergency material delivery demonstrate that the proposed method can generate high-quality solutions for various problem scales and greatly improve resource balance and real-time performance. Note to Practitioners—This work is motivated by the task assignment problem involving multiple heterogeneous agents in a highly real-time scenario, while taking into account various practical constraints. Each agent can carry various types and quantities of resources within its capacity, and the objective is to minimize resource consumption and travel distance for agents while reserving as many agents as possible to handle new tasks. The proposed approach can be utilized for securing the transportation of valuable assets, pesticide spraying using uncrewed aerial vehicles, and environmental monitoring. The emergency material delivery is used as an illustrative example to demonstrate how UAVs can ensure the rapid transportation of materials and safeguard the lives of affected individuals. To resolve the computationally challenging problem, this paper proposes a HRL method to efficiently solve the task assignment problem among multiple heterogeneous agents, which effectively enhances computational efficiency and improves task completion by optimizing the distribution of resources among agents.},
  keywords={Real-time systems;Automation;Training;Self-organizing feature maps;Neural networks;Multi-agent systems;Learning systems;Decision making;Transportation;Resource management;Task assignment;multi-agent system;reinforcement learning;resource balance;resource constraint},
  doi={10.1109/TASE.2025.3560121},
  ISSN={1558-3783},
  month={},}@ARTICLE{9784819,
  author={Zhang, Weijia and Liu, Hao and Xiong, Hui and Xu, Tong and Wang, Fan and Xin, Haoran and Wu, Hua},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={RLCharge: Imitative Multi-Agent Spatiotemporal Reinforcement Learning for Electric Vehicle Charging Station Recommendation}, 
  year={2023},
  volume={35},
  number={6},
  pages={6290-6304},
  abstract={Electric Vehicle (EV) has become a preferable choice in the modern transportation system due to its environmental and energy sustainability. However, in many large cities, EV drivers often fail to find the proper spots for charging, because of the limited charging infrastructures and the spatiotemporally unbalanced charging demands. Indeed, the recent emergence of deep reinforcement learning provides great potential to improve the charging experience from various aspects over a long-term horizon. In this paper, we propose an Imitative Multi-Agent Spatio-Temporal Reinforcement Learning (RlCharge) framework for intelligently recommending public accessible charging stations by jointly considering various long-term spatio-temporal factors. Specifically, by regarding each charging station as an individual agent, we formulate the problem as a multi-objective multi-agent reinforcement learning task. We first develop a multi-agent actor-critic framework with centralized training decentralized execution. Particularly, we propose a tailor-designed centralized attentive critic to coordinate the recommendation between geo-distributed agents, and introduce a delayed access strategy to exploit the knowledge of future charging competition during centralized training. Moreover, to handle the partial observability problem during decentralized execution in the large-scale multi-agent system, we propose the spatio-temporal heterogeneous graph convolution module, including (1) a dynamic graph convolution block to generate real-time representations for observable forthcoming EVs, and (2) a spatial graph convolution block to share the agent observations by message propagation between spatially adjacent agents. After that, to effectively optimize multiple divergent learning objectives, we extend the centralized attentive critic to multi-critics, and develop a dynamic gradient re-weighting strategy to adaptively guide the optimization direction. In addition, we propose an adaptive imitation learning scheme to further accelerate and stabilize the policy convergence. Finally, extensive experiments on two real-world datasets demonstrate that RlCharge achieves the best comprehensive performance compared with ten baseline approaches.},
  keywords={Charging stations;Task analysis;Convolution;Training;Reinforcement learning;Optimization;Continuous wavelet transforms;Electric vehicle charging station recommendation;multi-agent reinforcement learning;multi-objective optimization;graph neural networks;imitation learning},
  doi={10.1109/TKDE.2022.3178819},
  ISSN={1558-2191},
  month={June},}@INPROCEEDINGS{9892225,
  author={Chen, Hao and Yang, Guangkai and Zhang, Junge and Yin, Qiyue and Huang, Kaiqi},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)}, 
  title={RACA: Relation-Aware Credit Assignment for Ad-Hoc Cooperation in Multi-Agent Deep Reinforcement Learning}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={In recent years, reinforcement learning has faced several challenges in the multi-agent domain, such as the credit assignment issue. Value function factorization emerges as a promising way to handle the credit assignment issue under the centralized training with decentralized execution (CTDE) paradigm. However, existing value function factorization methods cannot deal with ad-hoc cooperation, that is, adapting to new configurations of teammates at test time. Specifically, these methods do not explicitly utilize the relationship between agents and cannot adapt to different sizes of inputs. To address these limitations, we propose a novel method, called Relation-Aware Credit Assignment (RACA), which achieves zero-shot generalization in ad-hoc cooperation scenarios. RACA takes advantage of a graph-based relation encoder to encode the topological structure between agents. Furthermore, RACA utilizes an attention-based observation abstraction mechanism that can generalize to an arbitrary number of teammates with a fixed number of parameters. Experiments demonstrate that our method outperforms baseline methods on the StarCraftII micromanagement benchmark and ad-hoc cooperation scenarios.},
  keywords={Training;Neural networks;Reinforcement learning;Benchmark testing;Ad hoc networks;Multi-Agent System;Deep Reinforcement Learning;Ad-Hoc Cooperation},
  doi={10.1109/IJCNN55064.2022.9892225},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{10191160,
  author={Hua, Hongzhi and Wen, Guixuan and Wu, Kaigui},
  booktitle={2023 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Building Decision Forest via Deep Reinforcement Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Ensemble learning methods whose base classifier is a decision tree usually belong to the bagging or boosting. It is widely used in all aspects of machine learning and has made great achievements in classification problems. However, no previous work has ever built the ensemble classifier by maximizing long-term returns to the best of our knowledge. This paper proposes a decision forest building method called MA-H-SAC-DF (Multi-agent Hybrid Soft Actor Critic based Decision Forest) for binary classification via deep reinforcement learning. First, the building process is modeled as a decentralized partial observable Markov decision process, and a set of cooperative agents jointly constructs all base classifiers. Second, the global state and local observations are defined based on information of the parent node and the current location. Last, the state-of-the-art deep reinforcement method Hybrid SAC (Hybrid Soft Actor Critic) with hybrid action space is extended to a multi-agent system under the CTDE (centralized training decentralized execution) architecture to find an optimal decision forest building policy. The experiments indicate that MA-H-SAC-DF has the same performance as random forest, Adaboost, and GBDT (Gradient Boosting Decision Tree) on balanced datasets and outperforms state-of-the-art ensemble learning algorithms on imbalanced datasets.},
  keywords={Deep learning;Training;Buildings;Neural networks;Forestry;Reinforcement learning;Boosting;multi-agent deep reinforcement learning;ensemble learning;decision tree},
  doi={10.1109/IJCNN54540.2023.10191160},
  ISSN={2161-4407},
  month={June},}@ARTICLE{10966452,
  author={Mendoza, Charmae Franchesca and Kaneko, Megumi and Rupp, Markus and Schwarz, Stefan},
  journal={IEEE Transactions on Cognitive Communications and Networking}, 
  title={Enhancing the Uplink of Cell-Free Massive MIMO Through Prioritized Sampling and Personalized Federated Deep Reinforcement Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Effective power control is key to solving the inter-user interference problem that degrades performance in cell-free massive multiple-input multiple-output (MIMO) systems. Motivated by its ability to operate online and model-free, without relying on training datasets, we leverage deep reinforcement learning (DRL) for uplink power control, aiming to maximize the guaranteed rate. We propose a fully centralized single-agent framework and two distributed schemes that employ several agents for improved scalability, leveraging prioritized experience replay to enable fast adaptation to the dynamic changes of the wireless environment. We investigate the performance of two multi-agent system architectures: (1) centralized training, decentralized execution (CTDE), where each agent forwards its RL experience to a central trainer, and (2) personalized federated learning (FedPer), where the training is performed locally at each agent, and only the base layer of the local deep neural network (DNN) model is forwarded periodically for aggregation at a server. We focus on the realistic scenario of dynamic device (de-)activation, combined with user mobility. Numerical evaluations demonstrate that the proposed FedPer with prioritized sampling achieves near-optimal rate and power performance while incurring the least amount of communication overhead.},
  keywords={Power control;Uplink;Training;Massive MIMO;Resource management;Optimization;Artificial neural networks;Convergence;Interference;Scalability;cell-free massive MIMO;power control;deep reinforcement learning;personalized federated learning;prioritized experience replay},
  doi={10.1109/TCCN.2025.3561289},
  ISSN={2332-7731},
  month={},}@INPROCEEDINGS{10128078,
  author={Gao, Zhenkun and Dai, Xiaoyan and Yao, Meibao and Xiao, Xueming},
  booktitle={2023 IEEE 6th International Conference on Industrial Cyber-Physical Systems (ICPS)}, 
  title={A Data Enhancement Strategy for Multi-Agent Cooperative Hunting based on Deep Reinforcement Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Cooperative hunting is a typical and significant scene to study multi-agent behaviors, where conventional control strategies are difficult to cope with, due to its high dimensionality of state space and locality of communication. Reinforcement learning provides a framework and a set of tools for this issue by trial-and-error interactions with the environment. Though promising, it often requires a large number of empirical sample data to learn effective hunting strategies, leading to low sample efficiency, understood as the training episodes required for the agent to learn effective behavior strategies. To improve the sampling efficiency, we propose a data enhancement strategy integrated in the execution (CTDE) training framework to train the multi-agent system. The data enhancement strategy is based on a state transfer dynamics model to generate additional predicted data, which we called dynamic prediction model, combined with the empirical data by interacting with the environment, for higher sample efficiency. The simulation results on the Webots platform show that our method outperforms some state-of-the-art methods, such as MAPPO, with high data sample efficiency.},
  keywords={Training;Service robots;Simulation;Training data;Reinforcement learning;Predictive models;Data models;Reinforcement learning;Multi robot systems;Data enhancement;Multi robot hunting},
  doi={10.1109/ICPS58381.2023.10128078},
  ISSN={2769-3899},
  month={May},}
