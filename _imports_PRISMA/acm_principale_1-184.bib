@inproceedings{10.1145/3746709.3746927,
author = {Wang, Yi and Chang, Shengbo and Tang, Hao and Wang, Cheng},
title = {Automating Satellite Collision Avoidance via Multi-Agent Reinforcement Learning},
year = {2025},
isbn = {9798400713163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746709.3746927},
doi = {10.1145/3746709.3746927},
abstract = {In recent decades, thousands of satellites have been launched by governments and companies for scientific and commercial purposes, as the near-earth communication has advanced. However, the risk of collision with debris or other satellites is increasing as space becomes more crowded. Traditionally, collision avoidance is performed by operators on the ground who calculate debris conjunctions using mathematics methods. Recently, some automated methods have also been proposed by researchers. However, for satellite constellations, it is necessary to take coordination and cooperation into consideration. In this paper, we employ multi-agent reinforcement learning, an advanced artificial intelligent method, to automate the collision avoidance of satellites. We propose a multi-agent reinforcement learning method with communication constrained, which learns coordination and communication simultaneously. We compare our method and baselines on the proposed environment with different settings. The experiments prove that our method is efficient and prior to the baselines.},
booktitle = {Proceedings of the 2025 6th International Conference on Computer Information and Big Data Applications},
pages = {1284–1290},
numpages = {7},
keywords = {Collision Avoidance, Multi-Agent Reinforcement Learning, Multi-Agent System, Satellite Constellation Simulation},
location = {
},
series = {CIBDA '25}
}

@inproceedings{10.5555/3709347.3743619,
author = {Hu, Kun and Wen, Muning and Wang, Xihuai and Zhang, Shao and Shi, Yiwei and Li, Minne and Li, Minglong and Wen, Ying},
title = {PMAT: Optimizing Action Generation Order in Multi-Agent Reinforcement Learning},
year = {2025},
isbn = {9798400714269},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-Agent Reinforcement Learning (MARL) faces challenges in coordinating agents due to complex interdependencies within multi-agent systems. Most MARL algorithms use the simultaneous decision-making paradigm but ignore the action-level dependencies among agents, which reduces coordination efficiency. In contrast, the sequential decision-making paradigm provides finer-grained supervision for agent decision order, presenting the potential for handling dependencies via better decision order management. However, determining the optimal decision order remains a challenge. In this paper, we introduce Action Generation with Plackett-Luce Sampling (AGPS), a novel mechanism for agent decision order optimization. We model the order determination task as a Plackett-Luce sampling process to address issues such as ranking instability and vanishing gradient during the network training process. AGPS realizes credit-based decision order determination by establishing a bridge between the significance of agents' local observations and their decision credits, thus facilitating order optimization and dependency management. Integrating AGPS with the Multi-Agent Transformer, we propose the Prioritized Multi-Agent Transformer (PMAT), a sequential decision-making MARL algorithm with decision order optimization. Experiments on benchmarks including StarCraft Multi-Agent Challenge, Google Research Football, and Multi-Agent MuJoCo show that PMAT outperforms state-of-the-art algorithms, greatly enhancing coordination efficiency.},
booktitle = {Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems},
pages = {997–1005},
numpages = {9},
keywords = {action generation order, multi-agent reinforcement learning},
location = {Detroit, MI, USA},
series = {AAMAS '25}
}

@article{10.1145/3706110,
author = {Gu, Tianlong and Zhi, Taihang and Bao, Xuguang and Chang, Liang},
title = {Credible Negotiation for Multi-agent Reinforcement Learning in Long-term Coordination},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {1556-4665},
url = {https://doi.org/10.1145/3706110},
doi = {10.1145/3706110},
abstract = {The coordination of multi-agent is one of the critical problems in Multi-agent Reinforcement Learning (MARL). The traditional methods of MARL focus on finding a stochastically acceptable solution called Nash Equilibrium (NE) for all agents from the Markov Game in which multiple equilibria exist. However, learning a fair equilibrium is crucial for the sustainability and stability of collaboration in the long-term coordination game, especially when the leadership competition exists. In this article, we propose the bi-level reinforcement learning method N-Bi-AC, whose solution is a Pareto improvement for traditional NE, to choose a fair Equilibrium. There are two parts in our method, the first is that we propose the Negotiator to determine the leader in stage game, and the other is to update the Q-value of agents in the game by using a bi-level actor-critic learning method based on the Joint Mixed Strategy Equilibrium Q-learning algorithm (JMSE Q-learning). The convergence proof is given, and the learning algorithm is compared with the state-of-the-art algorithms. We found that the proposed N-Bi-AC method successfully converged to a fair NE, which guarantees the fairness of agents in different matrix game environments.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = mar,
articleno = {1},
numpages = {27},
keywords = {Multi-agent Reinforcement Learning, Game Theory, Fair Coordination}
}

@inproceedings{10.5555/3709347.3743865,
author = {Yoo, Byunghyun and Shin, Younghwan and Kim, Hyunwoo and Chung, Euisok and Yang, Jeongmin},
title = {Adaptive Episode Length Adjustment for Multi-agent Reinforcement Learning},
year = {2025},
isbn = {9798400714269},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In standard reinforcement learning, an episode is defined as a sequence of interactions between agents and the environment, which terminates upon reaching a terminal state or a pre-defined episode length. Setting a shorter episode length enables the generation of multiple episodes with the same number of data samples, thereby facilitating an exploration of diverse states. While shorter episodes may limit the collection of long-term interactions, they may offer significant advantages when properly managed. For example, trajectory truncation in single-agent reinforcement learning has shown how the benefits of shorter episodes can be leveraged despite the trade-off of reduced long-term interaction experiences. However, this approach remains underexplored in MARL. This paper proposes a novel MARL approach, Adaptive Episode Length Adjustment (AELA), where the episode length is initially limited and gradually increased based on an entropy-based assessment of learning progress. By starting with shorter episodes, agents can focus on learning effective strategies for initial states and minimize time spent in dead-end states. The use of entropy as an assessment metric prevents premature convergence to suboptimal policies and ensures balanced training over varying episode lengths. We validate our approach using the StarCraft Multi-agent Challenge (SMAC) and a modified predator-prey environment, demonstrating significant improvements in both convergence speed and overall performance compared to existing methods. To the best of our knowledge, this is the first study to adaptively adjust episode length in MARL based on learning progress.},
booktitle = {Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems},
pages = {2253–2261},
numpages = {9},
keywords = {dead-end states, episode length adjustment, multi-agent reinforcement learning},
location = {Detroit, MI, USA},
series = {AAMAS '25}
}

@inproceedings{10.5555/3545946.3598669,
author = {Han, Shuai and Dastani, Mehdi and Wang, Shihan},
title = {Model-based Sparse Communication in Multi-agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Learning to communicate efficiently is central to multi-agent reinforcement learning (MARL). Existing methods often require agents to exchange messages intensively, which abuses communication channels and leads to high communication overhead. Only a few methods target on learning sparse communication, but they allow limited information to be shared, which affects the efficiency of policy learning. In this work, we propose model-based communication (MBC), a learning framework with a decentralized communication scheduling process. The MBC framework enables multiple agents to make decisions with sparse communication. In particular, the MBC framework introduces a model-based message estimator to estimate the up-to-date global messages using past local data. A decentralized message scheduling mechanism is also proposed to determine whether a message shall be sent based on the estimation. We evaluated our method in a variety of mixed cooperative-competitive environments. The experiment results show that the MBC method shows better performance and lower channel overhead than the state-of-art baselines.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {439–447},
numpages = {9},
keywords = {communication learning, message scheduling, multi-agent reinforcement learning, multi-agent system},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.5555/3709347.3743797,
author = {Peng, Kexing and Li, Pengyi and Hao, Jianye},
title = {Enhancing Graph-based Coordination with Evolutionary Algorithms for Episodic Multi-agent Reinforcement Learning},
year = {2025},
isbn = {9798400714269},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent Reinforcement Learning (MARL) has made significant progress in addressing coordination problems, but two key challenges persist in environments with partial observability: limited exploration and inaccurate evaluation of individual agents. To address these challenges, we propose a novel MARL framework that integrates Evolutionary Algorithms (EAs), episodic learning, and curiosity-driven exploration to optimize the coordination of joint policies using graph-based methods, named EECG. EAs are employed for their global optimization capabilities, particularly through population diversity and a gradient-free search mechanism, to enhance policy exploration. Initially, multiple agent teams explore and learn independently while sharing a common experience pool to enable data diversity. During the evolution phase, new joint policies are generated through crossover, mutation, and pareto-based selection. During the RL phase, diverse data is used to model and update the relationships among agents via Graph Neural Networks (GNNs), which help evaluate the effectiveness of individual agents' behaviors. GNNs treat agents as nodes and their interactions as edges, capturing coordination relationships effectively while dynamically assigning representations to nodes and edges. Furthermore, curiosity-based exploration motivates teams to discover new states, while a memory system stores high-reward experiences. We evaluated EECG on several benchmarks, including StarCraft II, SUMO autonomous driving, and the Multi-Agent Particle Environment. Our empirical results show that EECG consistently outperforms current baselines, with its components significantly contributing to faster convergence, especially by improving exploration and agent coordination. Our code is available: https://github.com/MercyM/EECG.},
booktitle = {Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems},
pages = {1623–1631},
numpages = {9},
keywords = {evolutionary algorithms, graph coordination, multi-agent reinforcement learning},
location = {Detroit, MI, USA},
series = {AAMAS '25}
}

@inproceedings{10.5555/3635637.3662937,
author = {Hu, Tianyi and Pu, Zhiqiang and Ai, Xiaolin and Qiu, Tenghai and Yi, Jianqiang},
title = {Measuring Policy Distance for Multi-Agent Reinforcement Learning},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Diversity plays a crucial role in improving the performance of multi-agent reinforcement learning (MARL). Currently, many diversity-based methods have been developed to overcome the drawbacks of excessive parameter sharing in traditional MARL. However, there remains a lack of a general metric to quantify policy differences among agents. Such a metric would not only facilitate the evaluation of the diversity evolution in multi-agent systems, but also provide guidance for the design of diversity-based MARL algorithms. In this paper, we propose the multi-agent policy distance (MAPD), a general tool for measuring policy differences in MARL. By learning the conditional representations of agents' decisions, MAPD can computes the policy distance between any pair of agents. Furthermore, we extend MAPD to a customizable version, which can quantify differences among agent policies on specified aspects. Based on the online deployment of MAPD, we design a multi-agent dynamic parameter sharing (MADPS) algorithm as an example of the MAPD's applications. Extensive experiments demonstrate that our method is effective in measuring differences in agent policies and specific behavioral tendencies. Moreover, in comparison to other methods of parameter sharing, MADPS exhibits superior performance.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {834–842},
numpages = {9},
keywords = {diversity measure, multi-agent system, reinforcement learning},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@inproceedings{10.5555/3545946.3598674,
author = {Zang, Yifan and He, Jinmin and Li, Kai and Fu, Haobo and Fu, Qiang and Xing, Junliang},
title = {Sequential Cooperative Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Cooperative multi-agent reinforcement learning (MARL) aims to coordinate the actions of multiple agents via a shared team reward. The complex interactions among agents make this problem extremely difficult. The mainstream of MARL methods often implicitly learn an inexplicable value decomposition from the shared reward into individual utilities, failing to give insights into how well each agent acts and lacking direct policy optimization guidance. This paper presents a sequential MARL framework that factorizes and simplifies the complex interaction analysis into a sequential evaluation process for more effective and efficient learning. We explicitly formulate this factorization via a novel sequential advantage function to evaluate each agent's actions, which achieves an explicable credit assignment and substantially facilitates policy optimization. We realize the sequential credit assignment (SeCA) by dynamically adjusting the sequence in light of agents' contributions to the team. Extensive experimental validations on a challenging set of StarCraft II micromanagement tasks verify SeCA's effectiveness.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {485–493},
numpages = {9},
keywords = {cooperative multi-agent reinforcement learning, sequential credit assignment, sequential evaluation},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.5555/3635637.3662972,
author = {Li, Xinran and Zhang, Jun},
title = {Context-aware Communication for Multi-agent Reinforcement Learning},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Effective communication protocols in multi-agent reinforcement learning (MARL) are critical to fostering cooperation and enhancing team performance. To leverage communication, many previous works have proposed to compress local information into a single message and broadcast it to all reachable agents. This simplistic messaging mechanism, however, may fail to provide adequate, critical, and relevant information to individual agents, especially in severely bandwidth-limited scenarios. This motivates us to develop context-aware communication schemes for MARL, aiming to deliver personalized messages to different agents. Our communication protocol, named CACOM, consists of two stages. In the first stage, agents exchange coarse representations in a broadcast fashion, providing context for the second stage. Following this, agents utilize attention mechanisms in the second stage to selectively generate messages personalized for the receivers. Furthermore, we employ the learned step size quantization (LSQ) technique for message quantization to reduce the communication overhead. To evaluate the effectiveness of CACOM, we integrate it with both actor-critic and value-based MARL algorithms. Empirical results on cooperative benchmark tasks demonstrate that CACOM provides evident performance gains over baselines under communication-constrained scenarios. The code is publicly available at https://github.com/LXXXXR/CACOM.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {1156–1164},
numpages = {9},
keywords = {communication, multi-agent systems, reinforcement learning},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@inproceedings{10.1145/3658644.3670293,
author = {Ma, Oubo and Pu, Yuwen and Du, Linkang and Dai, Yang and Wang, Ruo and Liu, Xiaolei and Wu, Yingcai and Ji, Shouling},
title = {SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3670293},
doi = {10.1145/3658644.3670293},
abstract = {Recent advancements in multi-agent reinforcement learning (MARL) have opened up vast application prospects, such as swarm control of drones, collaborative manipulation by robotic arms, and multi-target encirclement. However, potential security threats during the MARL deployment need more attention and thorough investigation. Recent research reveals that attackers can rapidly exploit the victim's vulnerabilities, generating adversarial policies that result in the failure of specific tasks. For instance, reducing the winning rate of a superhuman-level Go AI to around 20%. Existing studies predominantly focus on two-player competitive environments, assuming attackers possess complete global state observation.In this study, we unveil, for the first time, the capability of attackers to generate adversarial policies even when restricted to partial observations of the victims in multi-agent competitive environments. Specifically, we propose a novel black-box attack (SUB-PLAY) that incorporates the concept of constructing multiple subgames to mitigate the impact of partial observability and suggests sharing transitions among subpolicies to improve attackers' exploitative ability. Extensive evaluations demonstrate the effectiveness of SUB-PLAY under three typical partial observability limitations. Visualization results indicate that adversarial policies induce significantly different activations of the victims' policy networks. Furthermore, we evaluate three potential defenses aimed at exploring ways to mitigate security threats posed by adversarial policies, providing constructive recommendations for deploying MARL in competitive environments.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {645–659},
numpages = {15},
keywords = {adversarial policy, multi-agent reinforcement learning, partially observable},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inproceedings{10.1145/3746709.3746915,
author = {Hao, Jinling and Gao, Minghan and Han, Youyun and Gao, Qiang},
title = {Collaborative Multi-Agent Reinforcement Learning Model for Portfolio Management},
year = {2025},
isbn = {9798400713163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746709.3746915},
doi = {10.1145/3746709.3746915},
abstract = {The portfolio management problem refers to dynamically making investment decisions and allocating funds across multiple assets with a fixed principal, in order to maximize investment returns while effectively controlling risk. In recent years, deep learning and reinforcement learning have been introduced to better predict price trends and manage risks due to their powerful learning and generalization capabilities. However, how to solve the problem of cooperation and information sharing between assets to obtain good investment decisions and fund allocation at the same time, remains a challenging issue. In this paper, we propose a cooperative multi-agent reinforcement learning model for portfolio management, where each agent is responsible for making investment decisions for an asset, and funds are allocated based on the decisions of all agents. By designing a reward function that considers both individual agent's profit and global profit, we aim to optimize investment strategies and fund allocation, pursuing the maximum overall return on the system. The experimental results on futures indicate that the proposed model demonstrates superior performance. The study also analyzes the impact of the ratio between individual agent's profit and other agents' profits in the reward function, and proposes a dynamic adjustment strategy to optimize model performance. Additionally, the impact of the discreteness of the action space is studied in the paper.},
booktitle = {Proceedings of the 2025 6th International Conference on Computer Information and Big Data Applications},
pages = {1208–1214},
numpages = {7},
keywords = {Fund Allocation, Investment Strategy, Multi-Agent Learning, Portfolio Management, Reinforcement Learning},
location = {
},
series = {CIBDA '25}
}

@inproceedings{10.1145/3679240.3734602,
author = {de Mol, Barbera and Barbieri, Davide and Viebahn, Jan and Grossi, Davide},
title = {Centrally Coordinated Multi-Agent Reinforcement Learning for Power Grid Topology Control},
year = {2025},
isbn = {9798400711251},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3679240.3734602},
doi = {10.1145/3679240.3734602},
abstract = {Power grid operation is becoming more complex due to the increase in generation of renewable energy. The recent series of Learning To Run a Power Network (L2RPN) competitions have encouraged the use of artificial agents to assist human dispatchers in operating power grids. However, the combinatorial nature of the action space poses a challenge to both conventional optimizers and learned controllers. Action space factorization, which breaks down decision-making into smaller sub-tasks, is one approach to tackle the curse of dimensionality. In this study, we propose a centrally coordinated multi-agent (CCMA) architecture for action space factorization. In this approach, regional agents propose actions and subsequently a coordinating agent selects the final action. We investigate several implementations of the CCMA architecture, and benchmark in different experimental settings against various L2RPN baseline approaches. The CCMA architecture exhibits higher sample efficiency and superior final performance than the baseline approaches. The results suggest high potential of the CCMA approach for further application in higher-dimensional L2RPN as well as real-world power grid settings.},
booktitle = {Proceedings of the 16th ACM International Conference on Future and Sustainable Energy Systems},
pages = {460–475},
numpages = {16},
keywords = {Multi-Agent Reinforcment Learning (MARL), Hierarchical Reinforcement Learning (HRL), Consensus-Based Learning, Power Network Control (PNC)},
location = {
},
series = {E-Energy '25}
}

@inproceedings{10.5555/3545946.3598816,
author = {Gangopadhyay, Briti and Dasgupta, Pallab and Dey, Soumyajit},
title = {Counterexample-Guided Policy Refinement in Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-Agent Reinforcement Learning (MARL) policies are being incorporated into a wide range of safety-critical applications. It is important for these policies to be free of counterexamples and adhere to safety requirements. We present a methodology for the counterexample-guided refinement of an optimized MARL policy with respect to given safety specifications. The proposed algorithm refines a calibrated MARL policy to become safer by eliminating counterexamples found during testing, using targeted gradient updates. We empirically validate our method on different cooperative multi-agent tasks and demonstrate that targeted gradient updates induce safety in MARL policies.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1606–1614},
numpages = {9},
keywords = {counterexample-guided refinement, multi-agent proximal policy optimization, multi-agent reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@article{10.1145/3661147,
author = {Langerak, Thomas and Christen, Sammy and Albaba, Mert and Gebhardt, Christoph and Holz, Christian and Hilliges, Otmar},
title = {MARLUI: Multi-Agent Reinforcement Learning for Adaptive Point-and-Click UIs},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {EICS},
url = {https://doi.org/10.1145/3661147},
doi = {10.1145/3661147},
abstract = {As the number of selectable items increases, point-and-click interfaces rapidly become complex, leading to a decrease in usability. Adaptive user interfaces can reduce this complexity by automatically adjusting an interface to only display the most relevant items. A core challenge for developing adaptive interfaces is to infer user intent and chose adaptations accordingly. Current methods rely on tediously hand-crafted rules or carefully collected user data. Furthermore, heuristics need to be recrafted and data regathered for every new task and interface. To address this issue, we formulate interface adaptation as a multi-agent reinforcement learning problem. Our approach learns adaptation policies without relying on heuristics or real user data, facilitating the development of adaptive interfaces across various tasks with minimal adjustments needed. In our formulation, a user agent mimics a real user and learns to interact with an interface via point-and-click actions. Simultaneously, an interface agent learns interface adaptations, to maximize the user agent's efficiency, by observing the user agent's behavior. For our evaluation, we substituted the simulated user agent with actual users. Our study involved twelve participants and concentrated on automatic toolbar item assignment. The results show that the policies we developed in simulation effectively apply to real users. These users were able to complete tasks with fewer actions and in similar times compared to methods trained with real data. Additionally, we demonstrated our method's efficiency and generalizability across four different interfaces and tasks.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {253},
numpages = {27},
keywords = {Adaptive User Interfaces, Intelligent User Interfaces, Multi-Agent Reinforcement Learning}
}

@article{10.1145/3610300,
author = {Zhadan, Anastasia and Allahverdyan, Alexander and Kondratov, Ivan and Mikheev, Vikenty and Petrosian, Ovanes and Romanovskii, Aleksei and Kharin, Vitaliy},
title = {Multi-agent Reinforcement Learning-based Adaptive Heterogeneous DAG Scheduling},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3610300},
doi = {10.1145/3610300},
abstract = {Static scheduling of computational workflow represented by a directed acyclic graph (DAG) is an important problem in many areas of computer science. The main idea and novelty of the proposed algorithm is an adaptive heuristic or graph metric that uses a different heuristic rule at each scheduling step depending on local workflow. It is also important to note that multi-agent reinforcement learning is used to determine scheduling policy based on adaptive metrics. To prove the efficiency of the approach, a comparison with the state-of-the-art DAG scheduling algorithms is provided: DONF, CPOP, HCPT, HPS, and PETS. Based on the simulation results, the proposed algorithm shows an improvement of up to 30% on specific graph topologies and an average performance gain of 5.32%, compared to the best scheduling algorithm, DONF (suitable for large-scale scheduling), on a large number of random DAGs. Another important result is that using the proposed algorithm it was possible to cover 30.01% of the proximity interval from the best scheduling algorithm to the global optimal solution. This indicates that the idea of an adaptive metric for DAG scheduling is important and requires further research and development.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {87},
numpages = {26},
keywords = {Multi-agent deep reinforcement learning, scheduling, directed acyclic graph, deep learning, proximal policy optimization}
}

@inproceedings{10.1145/3658617.3697547,
author = {Fayyazi, Arya and Kamal, Mehdi and Pedram, Massoud},
title = {Dynamic Co-Optimization Compiler: Leveraging Multi-Agent Reinforcement Learning for Enhanced DNN Accelerator Performance},
year = {2025},
isbn = {9798400706356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658617.3697547},
doi = {10.1145/3658617.3697547},
abstract = {This paper introduces a novel Dynamic Co-Optimization Compiler (DCOC), which employs an adaptive Multi-Agent Reinforcement Learning (MARL) framework to enhance the efficiency of mapping machine learning (ML) models, particularly Deep Neural Networks (DNNs), onto diverse hardware platforms. DCOC incorporates three specialized actor-critic agents within MARL, each dedicated to different optimization facets: one for hardware and two for software. This cooperative strategy results in an integrated hardware/software co-optimization approach, improving the precision and speed of DNN deployments. By focusing on high-confidence configurations, DCOC effectively reduces the search space, achieving remarkable performance over existing methods. Our results demonstrate that DCOC enhances throughput by up to 37.95% while reducing optimization time by up to 42.2% across various DNN models, outperforming current state-of-the-art frameworks.},
booktitle = {Proceedings of the 30th Asia and South Pacific Design Automation Conference},
pages = {16–22},
numpages = {7},
keywords = {multi-agaent reinforcement learning, co-optimization, hardware accelerators, sampling, throughput},
location = {Tokyo, Japan},
series = {ASPDAC '25}
}

@inproceedings{10.5555/3635637.3662904,
author = {Evans, Benjamin Patrick and Ganesh, Sumitra},
title = {Learning and Calibrating Heterogeneous Bounded Rational Market Behaviour with Multi-agent Reinforcement Learning},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Agent-based models (ABMs) have shown promise for modelling various real world phenomena incompatible with traditional equilibrium analysis. However, a critical concern is the manual definition of behavioural rules in ABMs. Recent developments in multi-agent reinforcement learning (MARL) offer a way to address this issue from an optimisation perspective, where agents strive to maximise their utility, eliminating the need for manual rule specification. This learning-focused approach aligns with established economic and financial models through the use of rational utility-maximising agents. However, this representation departs from the fundamental motivation for ABMs: that realistic dynamics emerging from bounded rationality and agent heterogeneity can be modelled. To resolve this apparent disparity between the two approaches, we propose a novel technique for representing heterogeneous processing-constrained agents within a MARL framework. The proposed approach treats agents as constrained optimisers with varying degrees of strategic skills, permitting departure from strict utility maximisation. Behaviour is learnt through repeated simulations with policy gradients to adjust action likelihoods. To allow efficient computation, we use parameterised shared policy learning with distributions of agent skill levels. Shared policy learning avoids the need for agents to learn individual policies yet still enables a spectrum of bounded rational behaviours. We validate our model's effectiveness using real-world data on a range of canonical n-agent settings, demonstrating significantly improved predictive capability.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {534–543},
numpages = {10},
keywords = {agent-based modelling, bounded rationality, market simulation, multi-agent reinforcement learning, skill heterogeneity},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@inproceedings{10.5555/3635637.3662998,
author = {Mi, Qirui and Xia, Siyu and Song, Yan and Zhang, Haifeng and Zhu, Shenghao and Wang, Jun},
title = {TaxAI: A Dynamic Economic Simulator and Benchmark for Multi-agent Reinforcement Learning},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Taxation and government spending are crucial tools for governments to promote economic growth and maintain social equity. However, the difficulty in accurately predicting the dynamic strategies of diverse self-interested households presents a challenge for governments to implement effective tax policies. Given its proficiency in modeling other agents in partially observable environments and adaptively learning to find optimal policies, Multi-Agent Reinforcement Learning (MARL) is highly suitable for solving dynamic games between the government and numerous households. Although MARL shows more potential than traditional methods such as the genetic algorithm and dynamic programming, there is a lack of large-scale multi-agent reinforcement learning economic simulators. Therefore, we propose a MARL environment, named TaxAI, for dynamic games involving N households, government, firms, and financial intermediaries based on the Bewley-Aiyagari economic model. Our study benchmarks 2 traditional economic methods with 7 MARL methods on TaxAI, demonstrating the effectiveness and superiority of MARL algorithms. Moreover, TaxAI's scalability in simulating dynamic interactions between the government and 10,000 households, coupled with real-data calibration, grants it a substantial improvement in scale and reality over existing simulators. Therefore, TaxAI is the most realistic economic simulator for optimal tax policy, which aims to generate feasible recommendations for governments and individuals.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {1390–1399},
numpages = {10},
keywords = {benchmark, dynamic economic simulator, multi-agent reinforcement learning, optimal tax policy, tax evasion behavior},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@inproceedings{10.5555/3545946.3598667,
author = {Wang, Xuefeng and Li, Xinran and Shao, Jiawei and Zhang, Jun},
title = {AC2C: Adaptively Controlled Two-Hop Communication for Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Learning communication strategies in cooperative multi-agent reinforcement learning (MARL) has recently attracted intensive attention. Early studies typically assumed a fully-connected communication topology among agents, which induces high communication costs and may not be feasible. Some recent works have developed adaptive communication strategies to reduce communication overhead, but these methods cannot effectively obtain valuable information from agents that are beyond the communication range. In this paper, we consider a realistic communication model where each agent has a limited communication range, and the communication topology dynamically changes. To facilitate effective agent communication, we propose a novel communication protocol calledAdaptively Controlled Two-Hop Communication (AC2C). After an initial local communication round, AC2C employs an adaptive two-hop communication strategy to enable long-range information exchange among agents to boost performance, which is implemented by a communication controller. This controller determines whether each agent should ask for two-hop messages and thus helps to reduce the communication overhead during distributed execution. We evaluate AC2C on three cooperative multi-agent tasks, and the experimental results show that it outperforms relevant baselines with lower communication costs.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {427–435},
numpages = {9},
keywords = {adaptive controller, multi-agent system, reinforcement learning, two-hop communication},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3589334.3647982,
author = {Miyake, Kentaro and Ito, Hiroyoshi and Faloutsos, Christos and Matsumoto, Hirotomo and Morishima, Atsuyuki},
title = {NETEVOLVE: Social Network Forecasting using Multi-Agent Reinforcement Learning with Interpretable Features},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589334.3647982},
doi = {10.1145/3589334.3647982},
abstract = {Predicting how social networks change in the future is important in many applications. Results in social network research have shown that the change in the network can be explained by a small number of concepts, such as "homophily" and "transitivity". However, existing prediction methods require many latent features that are not connected to such concepts, making the methods' black boxes and their prediction results difficult to interpret, making them harder to derive scientific knowledge about social networks. In this study, we propose NetEvolve a novel multi-agent reinforcement learning-based method that predicts changes in a given social network. Given a sequence of changes as training data, NetEvolve learns the characteristics of the nodes with interpretable features, such as how the node feels rewards for connecting with similar people and the cost of the connection itself. Based on the learned feature, NetEvolve makes a forecast based on multi-agent simulation. The method achieves comparable or better accuracy than existing methods in predicting network changes in real-world social networks while keeping the prediction results interpretable.},
booktitle = {Proceedings of the ACM Web Conference 2024},
pages = {2542–2551},
numpages = {10},
keywords = {multi-agent system, network science, reinforcement learning, time-series},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3691620.3694983,
author = {Fan, Yujia and Wang, Sinan and Fei, Zebang and Qin, Yao and Li, Huaxuan and Liu, Yepang},
title = {Can Cooperative Multi-Agent Reinforcement Learning Boost Automatic Web Testing? An Exploratory Study},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3694983},
doi = {10.1145/3691620.3694983},
abstract = {Reinforcement learning (RL)-based web GUI testing techniques have attracted significant attention in both academia and industry due to their ability to facilitate automatic and intelligent exploration of websites under test. Yet, the existing approaches that leverage a single RL agent often struggle to comprehensively explore the vast state space of large-scale websites with complex structures and dynamic content. Observing this phenomenon and recognizing the benefit of multiple agents, we explore the use of Multi-Agent RL (MARL) algorithms for automatic web GUI testing, aiming to improve test efficiency and coverage. However, how to share information among different agents to avoid redundant actions and achieve effective cooperation is a non-trivial problem. To address the challenge, we propose the first MARL-based web GUI testing system, MARG, which coordinates multiple testing agents to efficiently explore a website under test. To share testing experience among different agents, we have designed two data sharing schemes: one centralized scheme with a shared Q-table to facilitate efficient communication, and another distributed scheme with data exchange to decrease the overhead of maintaining Q-tables. We have evaluated MARG on nine popular real-world websites. When configuring with five agents, MARG achieves an average increase of 4.34 and 3.89 times in the number of explored states, as well as a corresponding increase of 4.03 and 3.76 times in the number of detected failures, respectively, when compared to two state-of-the-art approaches. Additionally, compared to independently running the same number of agents, MARG can explore 36.42% more unique web states. These results demonstrate the usefulness of MARL in enhancing the efficiency and performance of web GUI testing tasks.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {14–26},
numpages = {13},
keywords = {web testing, multi-agent reinforcement learning, automatic GUI testing, information sharing},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3749566.3749610,
author = {Luo, Bing and Chen, Hui},
title = {Design of a Geographic Path Model for Dongguan Export Based on Multi-Agent Reinforcement Learning and Supply Chain Simulation},
year = {2025},
isbn = {9798400713927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3749566.3749610},
doi = {10.1145/3749566.3749610},
abstract = {In the context of global supply chain restructuring and the deep implementation of the Regional Comprehensive Economic Partnership (RCEP), this study addresses the geographic path optimization challenges faced by the Dongguan export manufacturing cluster. We propose a dynamic supply chain collaborative decision-making framework that integrates Multi-Agent Reinforcement Learning (MARL) with digital twin technology. Utilizing real-time customs clearance data from the Guangdong-Hong Kong-Macao Greater Bay Area and global shipping dynamics for 2025, we build a three-layer interactive model comprising manufacturers, logistics providers, and customs regulators, thereby overcoming the limitations of traditional static path planning.The innovations of this research are reflected in three aspects:Design of Heterogeneous Agent Reward Mechanisms: The manufacturer agent aims to minimize carbon tariff costs by embedding a real-time accounting module for the EU Carbon Border Adjustment Mechanism (CBAM).The experiments validate the model using real trade data from 300 Dongguan export enterprises for 2024-2025, showing that under the RCEP rules of origin constraints, the compliance rate of the generated pathways reaches 99.1%, avoiding potential tariff losses exceeding 230 million yuan. In response to the second blockage of the Suez Canal in 2025, the system reconstructs the entire network alternative solution within 43 seconds, improving decision-making efficiency by 270 times compared to manual processes. The interpretability AI module visualizes the agent gaming process, revealing the economic causes behind the emerging “Hong Kong transshipment - Vietnam land transport” route. This research provides algorithmic support for enhancing regional supply chain resilience under the “dual circulation” strategy, and the technical solution has been deployed in the Smart Port Brain System of Dongguan, contributing to the construction of a global logistics hub in the Guangdong-Hong Kong-Macao Greater Bay Area.},
booktitle = {Proceedings of the 2025 5th International Conference on Internet of Things and Machine Learning},
pages = {209–215},
numpages = {7},
keywords = {Carbon Tariff Path Optimization, Digital Twin Customs, Multi-Agent Reinforcement Learning, RCEP Rules of Origin, Supply Chain Simulation},
location = {
},
series = {IoTML '25}
}

@inproceedings{10.1145/3647750.3647765,
author = {Li, Zhuo and Yu, Jie and Liu, Xiaodong and Peng, Long},
title = {Load Balancing for Task Scheduling Based on Multi-Agent Reinforcement Learning in Cloud-Edge-End Collaborative Environments},
year = {2024},
isbn = {9798400716546},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3647750.3647765},
doi = {10.1145/3647750.3647765},
abstract = {With the increasing variety of computational scenarios and task types in cloud-edge-end collaborative networks, task scheduling in cloud-edge-end collaborative environments can better adapt to various task types and application scenarios, thereby enhancing the flexibility and adaptability of cloud-edge-end systems. This paper introduces a multi-agent reinforcement learning approach to conduct research on task load balancing scheduling in the context of cloud-edge-end collaboration, aiming to improve the efficiency of finding optimal task scheduling strategies in a distributed cloud-edge computing environment. In this paper, task scheduling is viewed as a competitive multi-agent system, where intelligent agents compete for a sufficient number of computing resources through the design of efficient task scheduling algorithms. This competition allows agents to reduce task completion latency and energy consumption while meeting task computational requirements. The paper employs Decentralized Partially Observable Markov Decision Process to model the reward maximization problem and designs a multi-agent reinforcement learning algorithm based on attention communication to solve it. Finally, experimental validation is conducted to evaluate the performance of the proposed task scheduling method.},
booktitle = {Proceedings of the 2024 8th International Conference on Machine Learning and Soft Computing},
pages = {94–100},
numpages = {7},
keywords = {Cloud-Edge-End collaborative, Collaborative task scheduling, Multi-Agent reinforcement learning},
location = {Singapore, Singapore},
series = {ICMLSC '24}
}

@inproceedings{10.5555/3463952.3464044,
author = {Li, Sheng and Gupta, Jayesh K. and Morales, Peter and Allen, Ross and Kochenderfer, Mykel J.},
title = {Deep Implicit Coordination Graphs for Multi-agent Reinforcement Learning},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent reinforcement learning (MARL) requires coordination to efficiently solve certain tasks. Fully centralized control is often infeasible in such domains due to the size of joint action spaces. Coordination graph based formalization allows reasoning about the joint action based on the structure of interactions. However,they often require domain expertise in their design. This paper introduces the deep implicit coordination graph (DICG) architecture for such scenarios. DICG consists of a module for inferring the dynamic coordination graph structure which is then used by a graph neural network based module to learn to implicitly reason about the joint actions or values. DICG allows learning the tradeoff between full centralization and decentralization via standard actor-critic methods to significantly improve coordination for domains with large number of agents. We apply DICG to both centralized-training-centralized-execution and centralized-training-decentralized-execution regimes. We demonstrate that DICG solves the relative over generalization pathology in predatory-prey tasks as well as outperforms various MARL baselines on the challenging StarCraft II Multi-agent Challenge (SMAC) and traffic junction environments.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {764–772},
numpages = {9},
keywords = {coordination, deep reinforcement learning, graph neural network, multi-agent reinforcement learning, multi-agent system},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/3543507.3583298,
author = {Sheng, Junjie and Wang, Lu and Yang, Fangkai and Qiao, Bo and Dong, Hang and Wang, Xiangfeng and Jin, Bo and Wang, Jun and Qin, Si and Rajmohan, Saravan and Lin, Qingwei and Zhang, Dongmei},
title = {Learning Cooperative Oversubscription for Cloud by Chance-Constrained Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583298},
doi = {10.1145/3543507.3583298},
abstract = {Oversubscription is a common practice for improving cloud resource utilization. It allows the cloud service provider to sell more resources than the physical limit, assuming not all users would fully utilize the resources simultaneously. However, how to design an oversubscription policy that improves utilization while satisfying some safety constraints remains an open problem. Existing methods and industrial practices are over-conservative, ignoring the coordination of diverse resource usage patterns and probabilistic constraints. To address these two limitations, this paper formulates the oversubscription for cloud as a chance-constrained optimization problem and proposes an effective Chance-Constrained Multi-Agent Reinforcement Learning (C2MARL) method to solve this problem. Specifically, C2MARL reduces the number of constraints by considering their upper bounds and leverages a multi-agent reinforcement learning paradigm to learn a safe and optimal coordination policy. We evaluate our C2MARL on an internal cloud platform and public cloud datasets. Experiments show that our C2MARL outperforms existing methods in improving utilization () under different levels of safety constraints.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {2927–2936},
numpages = {10},
keywords = {Cloud Computing, Multi-Agent System, Over Subscription, Reinforcement Learning},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.5555/3545946.3598817,
author = {Yu, Yang and Yin, Qiyue and Zhang, Junge and Huang, Kaiqi},
title = {Prioritized Tasks Mining for Multi-Task Cooperative Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-task learning improves data efficiency in cooperative multi-agent reinforcement learning, since agents can learn multiple related tasks simultaneously and the cooperation knowledge in a task can be utilized by others. However, existing methods mainly learn multiple cooperation tasks uniformly, regardless of their complexity and significance. In this paper, we propose a new framework called Prioritized Tasks Mining (PTM) for multi-task cooperation problems, which helps agents to identify and mine higher priority cooperation tasks, so as to learn more effective coordinated strategies for multiple cooperation tasks. Specially, agents will use the hindsight during training to identify the priority of different tasks, and make an exploration and exploitation in higher priority cooperative tasks to mine more sophisticated coordinated strategies. We evaluate PTM in challenging multi-task StarCraft micromanagement games with different scales, and results demonstrate that our method consistently outperforms all strong baselines.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1615–1623},
numpages = {9},
keywords = {multi-agent cooperation, multi-task learning, reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3744103.3744144,
author = {Wang, Xuejiao and Zhi, Guoqing and Tang, Zhihao and Jin, Hao and Zhang, Qianyue and Li, Nan},
title = {Self-Aware Intelligent Medical Rescue Unmanned Team via Large Language Model and Multi-Agent Reinforcement Learning},
year = {2025},
isbn = {9798400714429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3744103.3744144},
doi = {10.1145/3744103.3744144},
abstract = {Abstract: The evolution of medical rescue operations from traditional to future scenarios demands innovative approaches to intelligent automation. This paper proposes a self-award medical rescue team, leveraging cutting-edge technologies such as Large Language Models (LLMs), Multi-Agent Reinforcement Learning (MARL), and unmanned equipment to revolutionize frontline medical support. Our proposal of Unmanned and Digitalized Medical Rescue Team (UDMRT) emphasizes the decomposition of complex rescue tasks into manageable subtasks, facilitating improved learning and coordination among agents. This paper first implemented the rational assignment of roles for unmanned intelligent agents under complex tasks based on CMA-LLM (Cooperative Multi-Agent Large Language Models), forming combat teams with different functions. As for these teams, the paper proposed a novel hierarchical learning method designed for composite multi-agent tasks which addresses the challenge of coordination in complex domains by leveraging subtasks assignment. This method reduces observation spaces and encourages the reuse of subtask-specific policies, leading to more efficient learning and enhanced generalization capabilities. This architecture's modularity allows for better generalization to new environment configurations. Our work can adapt to new scenarios for practical applications in real-world multi-agent systems, where tasks are frequently comprised of discrete instances of localized interactions.},
booktitle = {Proceedings of the 2024 International Symposium on AI and Cybersecurity},
pages = {119–124},
numpages = {6},
keywords = {LLMs, MARL, Multi-Agent Collaboration, Task Assignment, UDMRT},
location = {
},
series = {ISAICS '24}
}

@inproceedings{10.1145/3511808.3557373,
author = {Ayala-Romero, Jose A. and Mernyei, P\'{e}ter and Shi, Bichen and Maz\'{o}n, Diego},
title = {KRAF: A Flexible Advertising Framework using Knowledge Graph-Enriched Multi-Agent Reinforcement Learning},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557373},
doi = {10.1145/3511808.3557373},
abstract = {Bidding optimization is one of the most important problems in online advertising. Auto-bidding tools are designed to address this problem and are offered by most advertising platforms for advertisers to allocate their budgets. In this work, we present a Knowledge Graph-enriched Multi-Agent Reinforcement Learning Advertising Framework (KRAF). It combines Knowledge Graph (KG) techniques with a Multi-Agent Reinforcement Learning (MARL) algorithm for bidding optimization with the goal of maximizing advertisers' return on ad spend (ROAS) and user-ad interactions, which correlates to the ad platform revenue. In addition, this proposal is flexible enough to support different levels of user privacy and the advent of new advertising markets with more heterogeneous data. In contrast to most of the current advertising platforms that are based on click-through rate models using a fixed input format and rely on user tracking, KRAF integrates the heterogeneous available data (e.g., contextual features, interest-based attributes, information about ads) as graph nodes to generate their dense representation (embeddings). Then, our MARL algorithm leverages the embeddings of the entities to learn efficient budget allocation strategies. To that end, we propose a novel coordination strategy based on a mean-field style to coordinate the learning agents and avoid the curse of dimensionality when the number of agents grows. Our proposal is evaluated on three real-world datasets to assess its performance and the contribution of each of its components, outperforming several baseline methods in terms of ROAS and number of ad clicks.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {47–56},
numpages = {10},
keywords = {bid optimization, knowledge graph, multi-agent reinforcement learning, online advertising},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3488933.3489029,
author = {Huang, Jiateng and Huang, Wanrong and Wu, Dan and Lan, Long},
title = {Meta Actor-Critic Framework for Multi-Agent Reinforcement Learning},
year = {2022},
isbn = {9781450384087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488933.3489029},
doi = {10.1145/3488933.3489029},
abstract = {In recent years, multi-agent reinforcement learning has received sustained attention in the last few years. The typical Actor-Critic methods learn mappings directly from observation to action without understanding the tasks themselves. In this paper, we present a meta actor-critic framework for meta-actor critique based on observational learning processes and the additional loss of meta-learning actors to accelerate and improve multi-agent learning across agents' experiences. Within our framework, all agents are deliberately designed to share the same meta-critic loss to achieve the optimum actor learning progress. Meanwhile, by minimizing the loss of meta-actors, the meta actor learns the features of the meta-observation, leading to better actions. We implemented the MADDPG and MATD3 algorithms in our proposed framework and empirically demonstrated the superiority of our framework on two kinds of multi-agent tasks. In addition, the framework can be flexibly incorporated into various contemporary multi-agent Actor-Critic methods.},
booktitle = {Proceedings of the 2021 4th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {636–643},
numpages = {8},
keywords = {Reinforcement learning, meta-learning, multi-agent system},
location = {Xiamen, China},
series = {AIPR '21}
}

@inproceedings{10.5555/3463952.3464053,
author = {Lyu, Xueguang and Xiao, Yuchen and Daley, Brett and Amato, Christopher},
title = {Contrasting Centralized and Decentralized Critics in Multi-Agent Reinforcement Learning},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Centralized Training for Decentralized Execution, where agents are trained offline using centralized information but execute in a decentralized manner online, has gained popularity in the multi-agent reinforcement learning community. In particular, actor-critic methods with a centralized critic and decentralized actors are a common instance of this idea. However, the implications of using a centralized critic in this context are not fully discussed and understood even though it is the standard choice of many algorithms. We therefore formally analyze centralized and decentralized critic approaches, providing a deeper understanding of the implications of critic choice. Because our theory makes unrealistic assumptions, we also empirically compare the centralized and decentralized critic methods over a wide set of environments to validate our theories and to provide practical advice. We show that there exist misconceptions regarding centralized critics in the current literature and show that the centralized critic design is not strictly beneficial, but rather both centralized and decentralized critics have different pros and cons that should be taken into account by algorithm designers.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {844–852},
numpages = {9},
keywords = {multi-agent system, policy gradient, reinforcement learning},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/3653804.3656282,
author = {Fu, Hongyi and Ji, Jianmin},
title = {Boosting Efficiency and Scalability of Multi-Agent Reinforcement Learning via Graph-Invariant Network},
year = {2024},
isbn = {9798400718199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3653804.3656282},
doi = {10.1145/3653804.3656282},
abstract = {Within a multiagent framework, an escalation in the count of agents leads to an exponential expansion of the state-action space. This expansion tends to diminish the scalability and reduce the sample efficiency of the multiagent reinforcement learning. To address the need for improved sample efficiency and scalability in multiagent algorithms, we characterize the multiagent system as a weighted directed graph and propose two related theorem: Graph Isomorphism Invariant (GII) and Graph Decomposition Invariant (GDI). These theorems provide significant insights into enhancing the sample efficiency and scalability of multiagent algorithms.The motivation behind GII arises from observing various "isomorphic" scenarios within a multiagent system, e.g., random shuffling of agent indices and identical topological structures. We demonstrate that the permutation invariant (PI), previously studied by others, is a specific case of our proposed GII. Implementing GII, rather than solely PI, can significantly reduce the state-action space and improve the sample efficiency of multiagent algorithms.Similarly, The motivation for GDI arises from noticing that as the number of agents in a multiagent system grows essentially introduces new vertices and edge sets, making the information of subgraph exploitable. Implementing GDI facilitates the efficient transfer of policies to larger-scale tasks. Mainstream MARL algorithms often overlook these properties, learning in the original state space, which leads to inefficient policy search and suppresses scalability.We have developed a network architecture that meets both GII and GDI, suitable for various multiagent algorithms. When implementing this architecture in the QTRAN algorithm, we refer to it as QTRAN-GI. By conducting comprehensive experiments in the SMAC environment, we confirm the superior sample efficiency and scalability of QTRAN-GI in transitioning to larger-scale multiagent tasks.},
booktitle = {Proceedings of the International Conference on Computer Vision and Deep Learning},
articleno = {79},
numpages = {8},
keywords = {Graph Decomposition, Graph Isomorphism, Multiagent Reinforcement Learning, Sample Efficiency},
location = {Changsha, China},
series = {CVDL '24}
}

@inbook{10.1145/3730436.3730525,
author = {Zhang, Yuan and Wang, Jiangnan and Li, Xuan and Li, Chao},
title = {Intelligent Decision Making in Dynamic Environments Based on Evolutionary Game Theory and Multi-Agent Reinforcement Learning},
year = {2025},
isbn = {9798400713637},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3730436.3730525},
abstract = {As dynamic environments become more complex, the need for autonomous decision making in high-dimensional state and action spaces increases. In this paper, we propose a multi-intelligent deep reinforcement learning (MADRL) framework based on evolutionary game theory and designed for parallel training in multiple environments. The framework addresses the challenges of algorithmic overfitting, slow convergence, and inefficient training by integrating multi-reward systems and introducing selection and mutation mechanisms inspired by collective intelligence. These enhancements improve learning efficiency and exploration, while ensuring robustness against adaptive adversaries through self-play training. Simulation result show that the proposed framework offers greater adaptability, faster convergence, and lower hyperparameter sensitivity than the classical approach. The results show that this approach enables agents to develop effective strategies and autonomous decision-making capabilities. This highlights its potential for applications in complex and dynamic systems such as robotics, autonomous navigation, and multi-agent coordination.},
booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Computational Intelligence},
pages = {538–547},
numpages = {10}
}

@inproceedings{10.5555/3535850.3536005,
author = {Xiao, Baicen and Ramasubramanian, Bhaskar and Poovendran, Radha},
title = {Agent-Temporal Attention for Reward Redistribution in Episodic Multi-Agent Reinforcement Learning},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This paper considers multi-agent reinforcement learning (MARL) tasks where agents receive a shared global reward at the end of an episode. The delayed nature of this reward affects the ability of the agents to assess the quality of their actions at intermediate time-steps. This paper focuses on developing methods to learn a temporal redistribution of the episodic reward to obtain a dense reward signal. Solving such MARL problems requires addressing two challenges: identifying (1) relative importance of states along the length of an episode (along time), and (2) relative importance of individual agents' states at any single time-step (among agents). In this paper, we introduce Agent-Temporal Attention for Reward Redistribution in Episodic Multi-Agent Reinforcement Learning (AREL) to address these two challenges. AREL uses attention mechanisms to characterize the influence of actions on state transitions along trajectories (temporal attention), and how each agent is affected by other agents at each time-step (agent attention). The redistributed rewards predicted by AREL are dense, and can be integrated with any given MARL algorithm. We evaluate AREL on challenging tasks from the Particle World environment and the StarCraft Multi-Agent Challenge. AREL results in higher rewards in Particle World, and improved win rates in StarCraft compared to three state-of-the-art reward redistribution methods. Our code is available at https://github.com/baicenxiao/AREL.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1391–1399},
numpages = {9},
keywords = {attention mechanism, credit assignment, episodic rewards, maulti-agent reinforcement learning},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.5555/3463952.3464054,
author = {Ma, Xiaoteng and Yang, Yiqin and Li, Chenghao and Lu, Yiwen and Zhao, Qianchuan and Yang, Jun},
title = {Modeling the Interaction between Agents in Cooperative Multi-Agent Reinforcement Learning},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Value-based methods of multi-agent reinforcement learning (MARL), especially the value decomposition methods, have been demonstrated on a range of challenging cooperative tasks. However, current methods pay little attention to the interaction between agents, which is essential to teamwork in games or real life. This limits the efficiency of value-based MARL algorithms in the two aspects: collaborative exploration and value function estimation. In this paper, we propose a novel cooperative MARL algorithm named as interactive actor-critic (IAC), which models the interaction of agents from the perspectives of policy and value function. On the policy side, a multi-agent joint stochastic policy is introduced by adopting a collaborative exploration module, which is trained by maximizing the entropy-regularized expected return. On the value side, we use the shared attention mechanism to estimate the value function of each agent, which takes the impact of the teammates into consideration. At the implementation level, we extend the value decomposition methods to continuous control tasks and evaluate IAC on benchmark tasks including classic control and multi-agent particle environments. Experimental results indicate that our method outperforms the state-of-the-art approaches and achieves better performance in terms of cooperation.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {853–861},
numpages = {9},
keywords = {collaborative exploration, maximum entropy learning, multi-agent reinforcement learning},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@article{10.1145/3447268,
author = {Zhu, Changxi and Leung, Ho-Fung and Hu, Shuyue and Cai, Yi},
title = {A Q-values Sharing Framework for Multi-agent Reinforcement Learning under Budget Constraint},
year = {2021},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/3447268},
doi = {10.1145/3447268},
abstract = {In a teacher-student framework, a more experienced agent (teacher) helps accelerate the learning of another agent (student) by suggesting actions to take in certain states. In cooperative multi-agent reinforcement learning (MARL), where agents must cooperate with one another, a student could fail to cooperate effectively with others even by following a teacher’s suggested actions, as the policies of all agents can change before convergence. When the number of times that agents communicate with one another is limited (i.e., there are budget constraints), an advising strategy that uses actions as advice could be less effective. We propose a partaker-sharer advising framework (PSAF) for cooperative MARL agents learning with budget constraints. In PSAF, each Q-learner can decide when to ask for and share its Q-values. We perform experiments in three typical multi-agent learning problems. The evaluation results indicate that the proposed PSAF approach outperforms existing advising methods under both constrained and unconstrained budgets. Moreover, we analyse the influence of advising actions and sharing Q-values on agent learning.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = apr,
articleno = {4},
numpages = {28},
keywords = {Multi-agent reinforcement learning, Q-learner, cooperative learning, knowledge sharing}
}

@inproceedings{10.1145/3442381.3449934,
author = {Zhang, Weijia and Liu, Hao and Wang, Fan and Xu, Tong and Xin, Haoran and Dou, Dejing and Xiong, Hui},
title = {Intelligent Electric Vehicle Charging Recommendation Based on Multi-Agent Reinforcement Learning},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449934},
doi = {10.1145/3442381.3449934},
abstract = {Electric Vehicle&nbsp;(EV) has become a preferable choice in the modern transportation system due to its environmental and energy sustainability. However, in many large cities, EV drivers often fail to find the proper spots for charging, because of the limited charging infrastructures and the spatiotemporally unbalanced charging demands. Indeed, the recent emergence of deep reinforcement learning provides great potential to improve the charging experience from various aspects over a long-term horizon. In this paper, we propose a framework, named Multi-Agent Spatio-Temporal Reinforcement Learning&nbsp;(Master), for intelligently recommending public accessible charging stations by jointly considering various long-term spatiotemporal factors. Specifically, by regarding each charging station as an individual agent, we formulate this problem as a multi-objective multi-agent reinforcement learning task. We first develop a multi-agent actor-critic framework with the centralized attentive critic to coordinate the recommendation between geo-distributed agents. Moreover, to quantify the influence of future potential charging competition, we introduce a delayed access strategy to exploit the knowledge of future charging competition during training. After that, to effectively optimize multiple learning objectives, we extend the centralized attentive critic to multi-critics and develop a dynamic gradient re-weighting strategy to adaptively guide the optimization direction. Finally, extensive experiments on two real-world datasets demonstrate that Master achieves the best comprehensive performance compared with nine baseline approaches.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {1856–1867},
numpages = {12},
keywords = {Charging station recommendation, multi-agent reinforcement learning, multi-objective optimization},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@article{10.1145/3582576,
author = {Vinitsky, Eugene and Lichtl\'{e}, Nathan and Parvate, Kanaad and Bayen, Alexandre},
title = {Optimizing Mixed Autonomy Traffic Flow with Decentralized Autonomous Vehicles and Multi-Agent Reinforcement Learning},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {2378-962X},
url = {https://doi.org/10.1145/3582576},
doi = {10.1145/3582576},
abstract = {We study the ability of autonomous vehicles to improve the throughput of a bottleneck using a fully decentralized control scheme in a mixed autonomy setting. We consider the problem of improving the throughput of a scaled model of the San Francisco–Oakland Bay Bridge: a two-stage bottleneck where four lanes reduce to two and then reduce to one. Although there is extensive work examining variants of bottleneck control in a centralized setting, there is less study of the challenging multi-agent setting where the large number of interacting AVs leads to significant optimization difficulties for reinforcement learning methods. We apply multi-agent reinforcement algorithms to this problem and demonstrate that significant improvements in bottleneck throughput, from 20% at a 5% penetration rate to 33% at a 40% penetration rate, can be achieved. We compare our results to a hand-designed feedback controller and demonstrate that our results sharply outperform the feedback controller despite extensive tuning. Additionally, we demonstrate that the RL-based controllers adopt a robust strategy that works across penetration rates whereas the feedback controllers degrade immediately upon penetration rate variation. We investigate the feasibility of both action and observation decentralization and demonstrate that effective strategies are possible using purely local sensing. Finally, we open-source our code at .},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = apr,
articleno = {13},
numpages = {22},
keywords = {Reinforcement learning, mixed autonomy, autonomous vehicles, traffic optimization}
}

@inproceedings{10.5555/3463952.3464010,
author = {Du, Yali and Liu, Bo and Moens, Vincent and Liu, Ziqi and Ren, Zhicheng and Wang, Jun and Chen, Xu and Zhang, Haifeng},
title = {Learning Correlated Communication Topology in Multi-Agent Reinforcement learning},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Communication improves the efficiency and convergence of multi-agent learning. Existing study of agent communication has been limited on predefined fixed connections. While an attention mechanism exists and is useful for scheduling the communication between agents, it, however, largely ignores the dynamical nature of communication and thus the correlation between agents' connections. In this work, we adopt a normalizing flow to encode correlation between agents interactions. The dynamical communication topology is directly learned by maximizing the agent rewards. In our end-to-end formulation, the communication structure is learned by considering it as a hidden dynamical variable. We realize centralized training of critics and graph reasoning policy, and decentralized execution from local observation and message that are received through the learned dynamical communication topology. Experiments on cooperative navigation in the particle world and adaptive traffic control tasks demonstrate the effectiveness of our method.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {456–464},
numpages = {9},
keywords = {communication topology, multi-agent systems, reinforcement learning},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.5555/3586210.3586243,
author = {Malhotra, Kanupriya and Lim, Zhi Jun and Alam, Sameer},
title = {A Multi-Agent Reinforcement Learning Approach for System-Level Flight Delay Absorption},
year = {2023},
publisher = {IEEE Press},
abstract = {With increasing air traffic, there is an ever-growing need for Air Traffic Controllers (ATCO) to efficiently manage traffic and congestion. Congestion often leads to increased delays in the Terminal Maneuvering Area (TMA), causing large amounts of fuel burn and detrimental environmental impacts. Approaches such as the Extended Arrival Manager (E-AMAN) propose solutions to absorb such delays, whereby flights are scheduled much before they enter the TMA. However, such an approach requires a speed management system where flights can coordinate to absorb system-level delays in their en-route phase. This paper proposes a Multi-Agent System (MAS) approach using Deep Reinforcement Learning to model and train flights as agents which can coordinate with each other to effectively absorb system-level delays. The simulations utilize Multi-Agent POsthumous Credit Assignment in Unity and test two reward approaches. Initial findings reveal an average of 3.3 minutes of system-level delay absorptions from a required delay of 4 minutes.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {406–417},
numpages = {12},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.1145/3357384.3357799,
author = {Zhou, Ming and Jin, Jiarui and Zhang, Weinan and Qin, Zhiwei and Jiao, Yan and Wang, Chenxi and Wu, Guobin and Yu, Yong and Ye, Jieping},
title = {Multi-Agent Reinforcement Learning for Order-dispatching via Order-Vehicle Distribution Matching},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357799},
doi = {10.1145/3357384.3357799},
abstract = {Improving the efficiency of dispatching orders to vehicles is a research hotspot in online ride-hailing systems. Most of the existing solutions for order-dispatching are centralized controlling, which require to consider all possible matches between available orders and vehicles. For large-scale ride-sharing platforms, there are thousands of vehicles and orders to be matched at every second which is of very high computational cost. In this paper, we propose a decentralized execution order-dispatching method based on multi-agent reinforcement learning to address the large-scale order-dispatching problem. Different from the previous cooperative multi-agent reinforcement learning algorithms, in our method, all agents work independently with the guidance from an evaluation of the joint policy since there is no need for communication or explicit cooperation between agents. Furthermore, we use KL-divergence optimization at each time step to speed up the learning process and to balance the vehicles (supply) and orders (demand). Experiments on both the explanatory environment and real-world simulator show that the proposed method outperforms the baselines in terms of accumulated driver income (ADI) and Order Response Rate (ORR) in various traffic environments. Besides, with the support of the online platform of Didi Chuxing, we designed a hybrid system to deploy our model.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2645–2653},
numpages = {9},
keywords = {deep reinforcement learning, multi-agent reinforcement learning, order-dispatching, ride-hailing},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.5555/3535850.3535976,
author = {Ruan, Jingqing and Du, Yali and Xiong, Xuantang and Xing, Dengpeng and Li, Xiyun and Meng, Linghui and Zhang, Haifeng and Wang, Jun and Xu, Bo},
title = {GCS: Graph-Based Coordination Strategy for Multi-Agent Reinforcement Learning},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Many real-world scenarios involve a team of agents that have to coordinate their policies to achieve a shared goal. Previous studies mainly focus on decentralized control to maximize a common reward and barely consider the coordination among control policies, which is critical in dynamic and complicated environments. In this work, we propose factorizing the joint team policy into graph generator and graph-based coordinated policy to enable coordinated behaviours among agents. The graph generator adopts an encoder-decoder framework that outputs directed acyclic graphs (DAGs) to capture the underlying dynamic decision structure. We also apply the DAGness-constrained and DAG depth-constrained optimization in the graph generator to balance efficiency and performance. The graph-based coordinated policy exploits the generated decision structure. The graph generator and coordinated policy are trained simultaneously to maximize the discounted return. Empirical evaluations on Collaborative Gaussian Squeeze, Cooperative Navigation, and Google Research Football demonstrate the superiority of the proposed method. The code is available at urlhttps://github.com/Amanda-1997/GCS_aamas337.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1128–1136},
numpages = {9},
keywords = {action coordination graph, multi-agent systems, reinforcement learning},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.5555/3535850.3536006,
author = {Xu, Zhiwei and Bai, Yunpeng and Li, Dapeng and Zhang, Bin and Fan, Guoliang},
title = {SIDE: State Inference for Partially Observable Cooperative Multi-Agent Reinforcement Learning},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {As one of the solutions to the decentralized partially observable Markov decision process (Dec-POMDP) problems, the value decomposition method has achieved significant results recently. However, most value decomposition methods require the fully observable state of the environment during training, but this is not feasible in some scenarios where only incomplete and noisy observations can be obtained. Therefore, we propose a novel value decomposition framework, named State Inference for value DEcomposition (SIDE), which eliminates the need to know the global state by simultaneously seeking solutions to the two problems of optimal control and state inference. SIDE can be extended to any value decomposition method to tackle partially observable problems. By comparing with the performance of different algorithms in StarCraft II micromanagement tasks, we verified that though without accessible states, SIDE can infer the current state that contributes to the reinforcement learning process based on past local observations and even achieve superior results to many baselines in some complex scenarios.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1400–1408},
numpages = {9},
keywords = {graph neural networks, multi-agent learning, reinforcement learning, variational inference},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/3385032.3385041,
author = {Clark, Tony and Barn, Balbir and Kulkarni, Vinay and Barat, Souvik},
title = {Language Support for Multi Agent Reinforcement Learning},
year = {2020},
isbn = {9781450375948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385032.3385041},
doi = {10.1145/3385032.3385041},
abstract = {Software Engineering must increasingly address the issues of complexity and uncertainty that arise when systems are to be deployed into a dynamic software ecosystem. There is also interest in using digital twins of systems in order to design, adapt and control them when faced with such issues. The use of multi-agent systems in combination with reinforcement learning is an approach that will allow software to intelligently adapt to respond to changes in the environment. This paper proposes a language extension that encapsulates learning-based agents and system building operations and shows how it is implemented in ESL. The paper includes examples the key features and describes the application of agent-based learning implemented in ESL applied to a real-world supply chain.},
booktitle = {Proceedings of the 13th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {7},
numpages = {12},
keywords = {Agents, Reinforcement Learning},
location = {Jabalpur, India},
series = {ISEC '20}
}

@article{10.5555/3586589.3586718,
author = {Mondal, Washim Uddin and Agarwal, Mridul and Aggarwal, Vaneet and Ukkusuri, Satish V.},
title = {On the approximation of cooperative heterogeneous multi-agent reinforcement learning (MARL) using Mean Field Control (MFC)},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {Mean field control (MFC) is an effective way to mitigate the curse of dimensionality of cooperative multi-agent reinforcement learning (MARL) problems. This work considers a collection of Npop heterogeneous agents that can be segregated into K classes such that the k-th class contains Nk homogeneous agents. We aim to prove approximation guarantees of the MARL problem for this heterogeneous system by its corresponding MFC problem. We consider three scenarios where the reward and transition dynamics of all agents are respectively taken to be functions of (1) joint state and action distributions across all classes, (2) individual distributions of each class, and (3) marginal distributions of the entire population. We show that, in these cases, the K-class MARL problem can be approximated by MFC with errors given as $e_1=mathcal{O}(frac{sqrt{|mathcal{X}|}+sqrt{|mathcal{U}|}}{N_{mathrm{pop}}}sum_{k}sqrt{N_k})$, $e_2=mathcal{O}(left[sqrt{|mathcal{X}|}+sqrt{|mathcal{U}|}right]sum_{k}frac{1}{sqrt{N_k}})$ and $e_3=mathcal{O}left(left[sqrt{|mathcal{X}|}+sqrt{|mathcal{U}|}right]left[frac{A}{N_{mathrm{pop}}}sum_{kin[K]}sqrt{N_k}+frac{B}{sqrt{N_{mathrm{pop}}}}right]right)$, respectively, where A, B are some constants and |χ|, |U| are the sizes of state and action spaces of each agent. Finally, we design a Natural Policy Gradient (NPG) based algorithm that, in the three cases stated above, can converge to an optimal MARL policy within O(ej) error with a sample complexity of O(ej-3), j ∈ {1, 2, 3}, respectively.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {129},
numpages = {46},
keywords = {multi-agent learning, heterogeneous systems, mean-field control, approximation guarantees, policy gradient algorithm}
}

@inproceedings{10.1145/3650212.3680376,
author = {Ma, Xuyan and Wang, Yawen and Wang, Junjie and Xie, Xiaofei and Wu, Boyu and Li, Shoubin and Xu, Fanjiang and Wang, Qing},
title = {Enhancing Multi-agent System Testing with Diversity-Guided Exploration and Adaptive Critical State Exploitation},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680376},
doi = {10.1145/3650212.3680376},
abstract = {Multi-agent systems (MASs) have achieved remarkable success in multi-robot control, intelligent transportation, and multiplayer games, etc.        Thorough testing for MAS is urgently needed to ensure its robustness in the face of constantly changing and unexpected scenarios.        Existing methods mainly focus on single-agent system testing and cannot be directly applied to MAS testing due to the complexity of MAS. To our best knowledge, there are fewer studies on MAS testing.        While several studies have focused on adversarial attacks on MASs, they primarily target failure detection from an attack perspective, i.e., discovering failure scenarios, while ignoring the diversity of scenarios.        In this paper, to highlight a typical balance between exploration (diversifying behaviors) and exploitation (detecting failures), we propose an advanced testing framework for MAS called  with diversity-guided exploration and adaptive critical state exploitation.        It incorporates both individual diversity and team diversity, and designs an adaptive perturbation mechanism to perturb the action at the critical states, so as to trigger more and more diverse failure scenarios of the system.        We evaluate MASTest on two popular MAS simulation environments: Coop Navi and StarCraft II.        Results show that the average distance of the resulting failure scenarios is increased by 29.55%-103.57% and 74.07%-370.00% on two environments compared to the baselines. Also, the failure patterns found by MASTest are improved by 71.44%-300.00% and 50%-500.00% on two experimental environments compared to the baselines.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1491–1503},
numpages = {13},
keywords = {Adaptive Perturbation Exploitation, Diversity-guided Exploration, Multi-agent System Testing},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{10.5555/3455716.3455894,
author = {Rashid, Tabish and Samvelyan, Mikayel and De Witt, Christian Schroeder and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
title = {Monotonic value function factorisation for deep multi-agent reinforcement learning},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {In many real-world settings, a team of agents must coordinate its behaviour while acting in a decentralised fashion. At the same time, it is often possible to train the agents in a centralised fashion where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a mixing network that estimates joint action-values as a monotonic combination of per-agent values. We structurally enforce that the joint-action value is monotonic in the per-agent values, through the use of non-negative weights in the mixing network, which guarantees consistency between the centralised and decentralised policies. To evaluate the performance of QMIX, we propose the StarCraft Multi-Agent Challenge (SMAC) as a new benchmark for deep multi-agent reinforcement learning. We evaluate QMIX on a challenging set of SMAC scenarios and show that it significantly outperforms existing multi-agent reinforcement learning methods.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {178},
numpages = {51},
keywords = {reinforcement learning, multi-agent learning, multi-agent coordination}
}

@inproceedings{10.5555/3709347.3743879,
author = {Zhu, Fengming and Lin, Fangzhen},
title = {Single-Agent Planning in a Multi-Agent System: A Unified Framework for Type-Based Planners},
year = {2025},
isbn = {9798400714269},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We consider a general problem where an agent is in a multi-agent environment and must plan for herself without any prior information about her opponents. At each moment, this pivotal agent is faced with a trade-off between exploiting her currently accumulated information about the other agents and exploring further to improve future (re-)planning. We propose a theoretic framework that unifies a spectrum of planners for the pivotal agent to address this trade-off. The planner at one end of this spectrum aims to find exact solutions, while those towards the other end yield approximate solutions as the problem scales up. Beyond theoretical analysis, we also implement 13 planners and conduct experiments in a specific domain called multi-agent route planning with the number of agents up to 50, to compare their performaces in various scenarios. One interesting observation comes from a class of planners that we call safe-agents and their enhanced variants by incorporating domain-specific knowledge, which is a simple special case under the proposed general framework, but performs sufficiently well in most cases. Our unified framework, as well as those induced planners, provides new insights on multi-agent decision-making, with potential applications to related areas such as mechanism design.},
booktitle = {Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems},
pages = {2382–2391},
numpages = {10},
keywords = {multi-agent planning, opponent modelling, tree search},
location = {Detroit, MI, USA},
series = {AAMAS '25}
}

@inproceedings{10.1145/3652628.3652739,
author = {Duan, Minghan and Wang, Junsong and Jiang, Feng},
title = {Research on Reinforcement Learning Method Based on Multi-agent Dynamic GoalsResearch on sparse rewards based on dynamic target multi-agent reinforcement learning},
year = {2024},
isbn = {9798400708831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652628.3652739},
doi = {10.1145/3652628.3652739},
abstract = {Generally speaking, sparse reward algorithms are usually only applicable to static target tasks. In order to solve the sparse reward problem in multi-agent dynamic target scenarios, this paper proposes the Multi-agent Dynamic Hindsight Experience Replay (MADHER) algorithm. The main idea of MADHER algorithm is to fully utilize two failed trajectories to generate experience with rewards, in order to solve the problem of sample inefficiency caused by sparse rewards for dynamic targets. In order to meet the characteristics of post experience playback for MADHER dynamic targets, this paper designs an efficient experience playback buffer structure. This design can greatly save time and expenses. The research results indicate that the MADHER algorithm is an effective method for solving the dynamic multi-objective sparse reward problem of multi-agent systems. Compared to other methods, it has faster convergence speed and higher performance.},
booktitle = {Proceedings of the 4th International Conference on Artificial Intelligence and Computer Engineering},
pages = {665–670},
numpages = {6},
location = {Dalian, China},
series = {ICAICE '23}
}

@inproceedings{10.1145/3672919.3672955,
author = {Yao, Wenyan and Zhang, Tianbao},
title = {Design of Human Resources Management Decision System Based on Multi-Agent System and Reinforcement Learning Algorithm},
year = {2024},
isbn = {9798400718212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672919.3672955},
doi = {10.1145/3672919.3672955},
abstract = {This study aims to address the lack of scientific and systematic decision systems in the field of Human Resources Management (HRM). By designing a HRM decision support system based on Multi-Agent systems and reinforcement learning algorithms, effective tools are provided to HR managers to assist them in making more scientific and systematic HRM decisions. The research analyzes the current issues in HRM practices and proposes comprehensive solutions. Through the optimization of Multi-Agent reinforcement learning algorithms, experiments validate the effectiveness of the system in supporting decision-making in HRM. The results demonstrate that the improved algorithms outperform traditional methods, confirming the efficacy of the system's design and optimization. This HRM decision support system, based on Multi-Agent systems and reinforcement learning algorithms, holds the potential to drive organizational development and enhance the efficiency of HRM. However, further research and practical application are needed to refine and optimize the system to adapt to the constantly evolving HRM environment.},
booktitle = {Proceedings of the 2024 3rd International Conference on Cyber Security, Artificial Intelligence and Digital Economy},
pages = {183–187},
numpages = {5},
location = {Nanjing, China},
series = {CSAIDE '24}
}

@inproceedings{10.5555/3306127.3331794,
author = {Li, Xihan and Zhang, Jia and Bian, Jiang and Tong, Yunhai and Liu, Tie-Yan},
title = {A Cooperative Multi-Agent Reinforcement Learning Framework for Resource Balancing in Complex Logistics Network},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Resource balancing within complex transportation networks is one of the most important problems in real logistics domain. Traditional solutions on these problems leverage combinatorial optimization with demand and supply forecasting. However, the high complexity of transportation routes, severe uncertainty of future demand and supply, together with non-convex business constraints make it extremely challenging in the traditional resource management field. In this paper, we propose a novel sophisticated multi-agent reinforcement learning approach to address these challenges. In particular, inspired by the externalities especially the interactions among resource agents, we introduce an innovative cooperative mechanism for state and reward design resulting in more effective and efficient transportation. Extensive experiments on a simulated ocean transportation service demonstrate that our new approach can stimulate cooperation among agents and lead to much better performance. Compared with traditional solutions based on combinatorial optimization, our approach can give rise to a significant improvement in terms of both performance and stability.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {980–988},
numpages = {9},
keywords = {logistics network, multi-agent, reinforcement learning, resource balancing},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@article{10.1145/3070861,
author = {Marinescu, Andrei and Dusparic, Ivana and Clarke, Siobh\'{a}n},
title = {Prediction-Based Multi-Agent Reinforcement Learning in Inherently Non-Stationary Environments},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/3070861},
doi = {10.1145/3070861},
abstract = {Multi-agent reinforcement learning (MARL) is a widely researched technique for decentralised control in complex large-scale autonomous systems. Such systems often operate in environments that are continuously evolving and where agents’ actions are non-deterministic, so called inherently non-stationary environments. When there are inconsistent results for agents acting on such an environment, learning and adapting is challenging. In this article, we propose P-MARL, an approach that integrates prediction and pattern change detection abilities into MARL and thus minimises the effect of non-stationarity in the environment. The environment is modelled as a time-series, with future estimates provided using prediction techniques. Learning is based on the predicted environment behaviour, with agents employing this knowledge to improve their performance in realtime. We illustrate P-MARL’s performance in a real-world smart grid scenario, where the environment is heavily influenced by non-stationary power demand patterns from residential consumers. We evaluate P-MARL in three different situations, where agents’ action decisions are independent, simultaneous, and sequential. Results show that all methods outperform traditional MARL, with sequential P-MARL achieving best results.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = may,
articleno = {9},
numpages = {23},
keywords = {Multi-agent systems, environment prediction, reinforcement learning, smart grids}
}

@inproceedings{10.1145/3447548.3467124,
author = {Han, Benjamin and Arndt, Carl},
title = {Budget Allocation as a Multi-Agent System of Contextual &amp; Continuous Bandits},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467124},
doi = {10.1145/3447548.3467124},
abstract = {Budget allocation for online advertising suffers from multiple complications, including significant delay between the initial ad impression to the call to action as well as cold-start prediction problems for ad campaigns with limited or no historical performance data. To address these issues, we introduce the Contextual Budgeting System (CBS ), a budget allocation framework using a multi-agent system of contextual &amp; continuous Multi-Armed Bandits. Our proposed solution decomposes the problem into a convex optimization problem whose objective is drawn using Thompson Sampling. In order to efficiently deal with context and cold-start, we propose a transfer learning mechanism using supervised learning methods that augment simple parametric models.We apply an implementation of this algorithm to all spending for new driver acquisition at Lyft and measure a (22 ± 10)% improvement in the mean Cost Per user Acquisition (CPA) over a previous non-contextual model, based on Markov Chain Monte-Carlo, generating tens of millions of dollars annually in efficiency improvements while also increasing total user acquisition.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {2937–2945},
numpages = {9},
keywords = {transfer learning, reinforcement learning, portfolio management, multi-agent learning, continuous multi-armed bandits, contextual bandit, advertising, Thompson sampling},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.5555/3709347.3744052,
author = {Geng, Minghong},
title = {Hierarchical Frameworks for Scaling-up Multi-agent Coordination},
year = {2025},
isbn = {9798400714269},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent reinforcement learning has emerged as a powerful framework for developing collaborative behaviors in autonomous systems. However, existing MARL methods often struggle with scalability in terms of both the number of agents and decision-making horizons. My research focuses on developing hierarchical approaches to scale up MARL systems through two complementary directions: structural scaling by increasing the number of coordinated agents and temporal scaling by extending planning horizons. My initial work introduced HiSOMA, a hierarchical framework integrating self-organizing neural networks with MARL for long-horizon planning, and MOSMAC, a benchmark for evaluating MARL methods on multi-objective MARL scenarios. Building on these foundations, my recent work studies L2M2, a novel framework that leverages large language models for high-level planning in hierarchical multi-agent systems. My ongoing research explores complex bimanual control tasks, specifically investigating multi-agent approaches for coordinated dual-hand manipulation.},
booktitle = {Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems},
pages = {2932–2934},
numpages = {3},
keywords = {benchmark, hierarchical multi-agent system, large language model, multi-agent reinforcement learning},
location = {Detroit, MI, USA},
series = {AAMAS '25}
}

@article{10.5555/3648699.3648877,
author = {Li, Wenhao and Jin, Bo and Wang, Xiangfeng and Yan, Junchi and Zha, Hongyuan},
title = {F2A2: flexible fully-decentralized approximate actor-critic for cooperative multi-agent reinforcement learning},
year = {2023},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {Traditional centralized multi-agent reinforcement learning (MARL) algorithms are sometimes unpractical in complicated applications due to non-interactivity between agents, the curse of dimensionality, and computation complexity. Hence, several decentralized MARL algorithms are motivated. However, existing decentralized methods only handle the fully cooperative setting where massive information needs to be transmitted in training. The block coordinate gradient descent scheme they used for successive independent actor and critic steps can simplify the calculation, but it causes serious bias. This paper proposes a exible fully decentralized actor-critic MARL framework, which can combine most of the actor-critic methods and handle large-scale general cooperative multi-agent settings. A primal-dual hybrid gradient descent type algorithm framework is designed to learn individual agents separately for decentralization. From the perspective of each agent, policy improvement and value evaluation are jointly optimized, which can stabilize multi-agent policy learning. Furthermore, the proposed framework can achieve scalability and stability for the large-scale environment. This framework also reduces information transmission by the parameter sharing mechanism and novel modeling-other-agents methods based on theory-of-mind and online supervised learning. Sufficient experiments in cooperative Multi-agent Particle Environment and StarCraft II show that the proposed decentralized MARL instantiation algorithms perform competitively against conventional centralized and decentralized methods.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {178},
numpages = {75},
keywords = {cooperative MARL, decentralized, actor-critic, primal-dual method}
}

@inproceedings{10.1145/3711896.3737198,
author = {Azim, Ehtesamul and Wang, Dongjie and Hwang, Tae Hyun and Fu, Yanjie and Zhang, Wei},
title = {Biological Pathway Guided Gene Selection Through Collaborative Reinforcement Learning},
year = {2025},
isbn = {9798400714542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711896.3737198},
doi = {10.1145/3711896.3737198},
abstract = {Gene selection in high-dimensional genomic data is essential for understanding disease mechanisms and improving therapeutic outcomes. Traditional feature selection methods effectively identify predictive genes but often ignore complex biological pathways and regulatory networks, leading to unstable and biologically irrelevant signatures. Prior approaches, such as Lasso-based methods and statistical filtering, either focus solely on individual gene-outcome associations or fail to capture pathway-level interactions, presenting a key challenge: how to integrate biological pathway knowledge while maintaining statistical rigor in gene selection? To address this gap, we propose a novel two-stage framework that integrates statistical selection with biological pathway knowledge using multi-agent reinforcement learning (MARL). First, we introduce a pathway-guided pre-filtering strategy that leverages multiple statistical methods alongside KEGG pathway information for initial dimensionality reduction. Next, for refined selection, we model genes as collaborative agents in a MARL framework, where each agent optimizes both predictive power and biological relevance. Our framework incorporates pathway knowledge through Graph Neural Network-based state representations, a reward mechanism combining prediction performance with gene centrality and pathway coverage, and collaborative learning strategies using shared memory and a centralized critic component. Extensive experiments on multiple gene expression datasets demonstrate that our approach significantly improves both prediction accuracy and biological interpretability compared to traditional methods.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
pages = {4250–4260},
numpages = {11},
keywords = {automated gene selection, biological pathway, disease outcome prediction, multi-agent reinforcement learning},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@article{10.1145/3742479,
author = {Peng, Kexing and Zhu, Shihao and Ma, Tinghuai},
title = {STPE-MARL: Spatio-Temporal Multi-Agent Population Evolution Reinforcement Learning},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3742479},
doi = {10.1145/3742479},
abstract = {Achieving joint goals efficiently in complex real-world tasks demands effective collaboration among multiple agents. Multi-Agent Reinforcement Learning (MARL) faces two interrelated challenges: limited exploration leads to early convergence on suboptimal behaviors, which in turn exacerbates non-stationarity under partial observability. To address these issues, we propose a novel framework, Spatio-Temporal Multi-agent Population Evolution (STPE-MARL). By integrating Evolutionary Algorithms (EAs) with MARL, our method enhances exploration diversity and facilitates global policy optimization. We further incorporate Graph Neural Networks (GNNs) to mitigate partial observability by encoding permutation symmetry through graph-based message passing. Two GNN-based training modes, Graph Relation and Graph Decomposition, are introduced to extend agents’ receptive fields and capture spatio-temporal dependencies through time-series trajectory sampling. We evaluate STPE-MARL in two complex environments: micromanagement tasks in StarCraft II and large-scale traffic simulations in SUMO (Simulation of Urban MObility). Experimental results demonstrate that STPE-MARL significantly improves policy convergence and outperforms baseline methods, highlighting the complementary roles of EAs in exploration and GNNs in addressing observation limitations.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {86},
numpages = {24},
keywords = {Multi-agent Reinforcement Learning, Graph Neural Networks, Evolutionary Algorithms, Permutation Symmetry}
}

@inproceedings{10.1145/3711896.3737017,
author = {Li, Junjun and Ma, Zeyuan and Huang, Ting and Gong, Yue-Jiao},
title = {Learn to Refine: Synergistic Multi-Agent Path Optimization for Lifelong Conflict-Free Navigation of Autonomous Vehicles},
year = {2025},
isbn = {9798400714542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711896.3737017},
doi = {10.1145/3711896.3737017},
abstract = {Lifelong Multi-Agent Path Finding (LMAPF) focuses on planning conflict-free paths for agents, like autonomous vehicles, that are continuously assigned new tasks. The synergy of search-based and learning-based methods holds promise for striking a balance in-between effectiveness and efficiency but still faces several challenges such as inferior initial paths, weak search-learning synergy and low sample utilization rate. To address these issues, this paper proposes a new synergized LMAPF approach, named Synergistic Multi-Agent Path Optimization (SMAPO), which consists of two tightly-coupled phases: Primordial Planning and Decision Refinement. In the Primordial Planning phase, we introduce a novel load-balanced A* algorithm that integrates planned and perceived congestion costs, which enhances initial solution quality by evenly distributing spatiotemporal traffic loads, thereby mitigating potential conflicts. In the Decision Refinement phase, we propose a novel Encoder-Decoder based neural network to learn a collaborative optimization policy through multi-agent reinforcement learning. In addition, we leverage dual transformations to augment trajectory samples during online learning, enhancing both the sample utilization rate and overall learning stability. Extensive experiments reveal that our SMAPO is superior to the state-of-the-art baselines in effectiveness, efficiency, and generalization capability. Source code is available at https://github.com/ByteUser-blues/SMAPO.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
pages = {1400–1411},
numpages = {12},
keywords = {autonomous vehicles, data-driven optimization, lifelong multi-agent path finding, multi-agent reinforcement learning},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@inproceedings{10.5555/3635637.3663308,
author = {Zhu, Changxi and Dastani, Mehdi and Wang, Shihan},
title = {A Survey of Multi-Agent Deep Reinforcement Learning with Communication},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Communication is an effective mechanism for coordinating the behaviors of multiple agents, broadening their views of the environment, and to support their collaborations. In the field of multi-agent deep reinforcement learning (MADRL), agents can improve the overall learning performance and achieve their objectives through communication. Agents can communicate various types of messages, either to all agents or to specific agent groups, or conditioned on specific constraints. With the growing body of research work in MADRL with communication (Comm-MADRL), there is a lack of a systematic and structural approach to distinguish and classify existing Comm-MADRL approaches. In this paper, we survey recent works in the Comm-MADRL field and consider various aspects of communication that can play a role in designing and developing multi-agent reinforcement learning systems. With these aspects in mind, we propose 9 dimensions along which Comm-MADRL approaches can be analyzed, developed, and compared. By projecting existing works into the multi-dimensional space, we discover interesting trends. We also propose some novel directions for designing future Comm-MADRL systems through exploring possible combinations of the dimensions.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {2845–2847},
numpages = {3},
keywords = {communication, deep reinforcement learning, multi-agent reinforcement learning, survey},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@inproceedings{10.5555/3635637.3663073,
author = {Zhang, Zhicheng and Liang, Yancheng and Wu, Yi and Fang, Fei},
title = {MESA: Cooperative Meta-Exploration in Multi-Agent Learning through Exploiting State-Action Space Structure},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent reinforcement learning (MARL) algorithms often struggle to find strategies close to Pareto optimal Nash Equilibrium, owing largely to the lack of efficient exploration. The problem is exacerbated in sparse-reward settings, caused by the larger variance exhibited in policy learning. This paper introduces MESA, a novel meta-exploration method for cooperative multi-agent learning. It learns to explore by first identifying the agents' high-rewarding joint state-action subspace from training tasks and then learning a set of diverse exploration policies to "cover" the subspace. These trained exploration policies can be integrated with any off-policy MARL algorithm for test-time tasks. We first showcase MESA's advantage in a multi-step matrix game. Furthermore, experiments show that with learned exploration policies, MESA achieves significantly better performance in sparse-reward tasks in several multi-agent particle environments and multi-agent MuJoCo environments, and exhibits the ability to generalize to more challenging tasks at test time.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {2085–2093},
numpages = {9},
keywords = {exploration strategy, meta-learning, multi-agent reinforcement learning},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@inproceedings{10.5555/3709347.3743784,
author = {Nagpal, Kartik and Dong, Dayi and Mehr, Negar},
title = {Leveraging Large Language Models for Effective and Explainable Multi-Agent Credit Assignment},
year = {2025},
isbn = {9798400714269},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Recent work, spanning from autonomous vehicle coordination to in-space assembly, has shown the importance of learning collaborative behavior for enabling robots to achieve shared goals. A common approach for learning this cooperative behavior is to utilize the centralized-training decentralized-execution paradigm. However, this approach also introduces a new challenge: how do we evaluate the contributions of each agent's actions to the overall success or failure of the team. This ''credit assignment'' problem has remained open, and has been extensively studied in the Multi-Agent Reinforcement Learning (MARL) literature. In fact, humans manually inspecting agent behavior often generate better credit evaluations than existing methods. We combine this observation with recent works which show Large Language Models (LLMs) demonstrate human-level performance at many pattern recognition tasks. Our key idea is to reformulate credit assignment to the two pattern recognition problems of sequence improvement and attribution, which motivates our novel Large Language Model Multi-agent Credit Assignment (LLM-MCA) method. Our approach utilizes a centralized LLM reward-critic which numerically decomposes the environment reward based on the individualized contribution of each agent in the scenario. We then update the agents' policy networks based on this feedback. We also propose an extension LLM-TACA where our LLM critic performs explicit task assignment by passing an intermediary goal directly to each agent policy in the scenario. Both our methods far outperform the state-of-the-art on a variety of benchmarks, including Level-Based Foraging, Robotic Warehouse, and our new ''Spaceworld'' benchmark which incorporates collision-related safety constraints. As an artifact of our methods, we generate large trajectory datasets with each timestep annotated with per-agent reward information, as sampled from our LLM critics. By making this dataset available, we aim to enable future works which can directly train a set of collaborative, decentralized policies offline.},
booktitle = {Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems},
pages = {1501–1510},
numpages = {10},
keywords = {credit assignment, foundation models, large language models, multi-agent reinforcement learning, task allocation},
location = {Detroit, MI, USA},
series = {AAMAS '25}
}

@inproceedings{10.1145/3396851.3397682,
author = {Kell, Alexander J. M. and Forshaw, Matthew and McGough, A. Stephen},
title = {Long-term electricity market agent based model validation using genetic algorithm based optimization},
year = {2020},
isbn = {9781450380096},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396851.3397682},
doi = {10.1145/3396851.3397682},
abstract = {Electricity market modelling is often used by governments, industry and agencies to explore the development of scenarios over differing timeframes. For example, how would the reduction in cost of renewable energy impact investments in gas power plants or what would be an optimum strategy for carbon tax or subsidies?Cost optimization based solutions are the dominant approach for understanding different long-term energy scenarios. However, these types of models have certain limitations such as the need to be interpreted in a normative manner, and the assumption that the electricity market remains in equilibrium throughout. Through this work, we show that agent-based models are a viable technique to simulate decentralised electricity markets. The aim of this paper is to validate an agent-based modelling framework to increase confidence in its ability to be used in policy and decision making.Our framework can model heterogeneous agents with imperfect information. The model uses a rules-based approach to approximate the underlying dynamics of a real world, decentralised electricity market. We use the UK as a case study, however, our framework is generalisable to other countries. We increase the temporal granularity of the model by selecting representative days of electricity demand and weather using a k-means clustering approach.We show that our framework can model the transition from coal to gas observed in the UK between 2013 and 2018. We are also able to simulate a future scenario to 2035 which is similar to the UK Government, Department for Business and Industrial Strategy (BEIS) projections. We show a more realistic increase in nuclear power over this time period. This is due to the fact that with current nuclear technology, electricity is generated almost instantaneously and has a low short-run marginal cost [13].},
booktitle = {Proceedings of the Eleventh ACM International Conference on Future Energy Systems},
pages = {1–13},
numpages = {13},
keywords = {agent-based modelling, energy market simulation, energy models, genetic algorithm, optimization, policy, simulation, validation},
location = {Virtual Event, Australia},
series = {e-Energy '20}
}

@inproceedings{10.1145/3726302.3729942,
author = {Zhang, Yi and Qiu, Ruihong and Xu, Xuwei and Liu, Jiajun and Wang, Sen},
title = {DARLR: Dual-Agent Offline Reinforcement Learning for Recommender Systems with Dynamic Reward},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3729942},
doi = {10.1145/3726302.3729942},
abstract = {Model-based offline reinforcement learning (RL) has emerged as a promising approach for recommender systems, enabling effective policy learning by interacting with frozen world models. However, the reward functions in these world models, trained on sparse offline logs, often suffer from inaccuracies. Specifically, existing methods face two major limitations in addressing this challenge: (1) deterministic use of reward functions as static look-up tables, which propagates inaccuracies during policy learning, and (2) static uncertainty designs that fail to effectively capture decision risks and mitigate the impact of these inaccuracies. In this work, a dual-agent framework, DARLR, is proposed to dynamically update world models to enhance recommendation policies. To achieve this, a selector is introduced to identify reference users by balancing similarity and diversity so that the recommender can aggregate information from these users and iteratively refine reward estimations for dynamic reward shaping. Further, the statistical features of the selected users guide the dynamic adaptation of an uncertainty penalty to better align with evolving recommendation requirements. Extensive experiments on four benchmark datasets demonstrate the superior performance of DARLR, validating its effectiveness. The code is available at this address.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2192–2202},
numpages = {11},
keywords = {multi-agent reinforcement learning, offline reinforcement learning, recommendation systems},
location = {Padua, Italy},
series = {SIGIR '25}
}

@inproceedings{10.5555/3545946.3598809,
author = {Goel, Harsh and Zhang, Yifeng and Damani, Mehul and Sartoretti, Guillaume},
title = {SocialLight: Distributed Cooperation Learning towards Network-Wide Traffic Signal Control},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Many recent works have turned to multi-agent reinforcement learning (MARL) for adaptive traffic signal control to optimize the travel time of vehicles over large urban networks. However, achieving effective and scalable cooperation among junctions (agents) remains an open challenge, as existing methods often rely on extensive, non-generalizable reward shaping or on non-scalable centralized learning. To address these problems, we propose a new MARL method for traffic signal control, SocialLight, which learns cooperative traffic control policies by distributedly estimating the individual marginal contribution of agents on their local neighborhood. SocialLight relies on the Asynchronous Actor Critic (A3C) framework, and makes learning scalable by learning a locally-centralized critic conditioned over the states and actions of neighboring agents, used by agents to estimate individual contributions by counterfactual reasoning. We further introduce important modifications to the advantage calculation that help stabilize policy updates. These modifications decouple the impact of the neighbors' actions on the computed advantages, thereby reducing the variance in the gradient updates. We benchmark our trained network against state-of-the-art traffic signal control methods on standard benchmarks in two traffic simulators, SUMO and CityFlow. Our results show that SocialLight exhibits improved scalability to larger road networks and better performance across usual traffic metrics.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1551–1559},
numpages = {9},
keywords = {adaptive traffic signal control, autonomous signal control, multi-agent reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3408308.3427604,
author = {Vazquez-Canteli, Jose R. and Henze, Gregor and Nagy, Zoltan},
title = {MARLISA: Multi-Agent Reinforcement Learning with Iterative Sequential Action Selection for Load Shaping of Grid-Interactive Connected Buildings},
year = {2020},
isbn = {9781450380614},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408308.3427604},
doi = {10.1145/3408308.3427604},
abstract = {We demonstrate that multi-agent reinforcement learning (RL) controllers can cooperate to provide more effective load shaping in a model-free, decentralized, and scalable way with very limited sharing of anonymous information. Rapid urbanization, increasing electrification, the integration of renewable energy resources, and the potential shift towards electric vehicles create new challenges for the planning and control of energy systems in smart cities. Energy storage resources can help better align peaks of renewable energy generation with peaks of electricity consumption and flatten the curve of electricity demand. Model-based controllers, such as MPC, require developing models of the systems controlled, which is often not cost-effective or scalable. Model-free controllers, such as RL, have the potential to provide good control policies cost-effectively and leverage the use of historical data for training. However, it is unclear how RL algorithms can control a multitude of energy systems in a scalable coordinated way. In this paper, we introduce MARLISA, a controller that combines multi-agent RL with our proposed iterative sequential action selection algorithm for load shaping in urban energy systems. This approach uses a reward function with individual and collective goals, and the agents predict their own future electricity consumption and share this information with each other following a leader-follower schema. The RL agents are tested in four groups of nine simulated buildings, with each group located in a different climate. The buildings have diverse load and domestic hot water profiles, PV panels, thermal storage devices, heat pumps, and electric heaters. The agents are evaluated on the average of five normalized metrics: annual net electric consumption, 1 -- load factor, average daily peak demand, annual peak demand, and ramping. MARLISA achieves superior results over multiple independent/uncooperative RL agents using the same reward function. Our results outperformed a manually optimized rule-based controller (RBC) benchmark by reducing the average daily peak load by 15%, ramping by 35%, and increasing the load factor by 10%. A multi-year case study on real weather data shows that MARLISA significantly outperforms the RBC in within a year and converges in less than 2 years. Combining MARLISA and the RBC for the first year improves overall initial performance by learning from the RBC rather than random exploration.},
booktitle = {Proceedings of the 7th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {170–179},
numpages = {10},
keywords = {Reinforcement learning, demand response, microgrid, multi-agent coordination},
location = {Virtual Event, Japan},
series = {BuildSys '20}
}

@inproceedings{10.5555/3545946.3598672,
author = {Mei, Yongsheng and Zhou, Hanhan and Lan, Tian and Venkataramani, Guru and Wei, Peng},
title = {MAC-PO: Multi-Agent Experience Replay via Collective Priority Optimization},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Experience replay is crucial for off-policy reinforcement learning (RL) methods. By remembering and reusing the experiences from past different policies, experience replay significantly improves the training efficiency and stability of RL algorithms. Many decision-making problems in practice naturally involve multiple agents and require multi-agent reinforcement learning (MARL) under centralized training decentralized execution paradigm. Nevertheless, existing MARL algorithms often adopt standard experience replay where the transitions are uniformly sampled regardless of their importance. Finding prioritized sampling weights that are optimized for MARL experience replay has yet to be explored. To this end, we propose MAC-PO, which formulates optimal prioritized experience replay for multi-agent problems as a regret minimization over the sampling weights of transitions. Such optimization is relaxed and solved using the Lagrangian multiplier approach to obtain the close-form optimal sampling weights. By minimizing the resulting policy regret, we can narrow the gap between the current policy and a nominal optimal policy, thus acquiring an improved prioritization scheme for multi-agent tasks. Our experimental results on Predator-Prey and StarCraft Multi-Agent Challenge environments demonstrate the effectiveness of our method, having a better ability to replay important transitions and outperforming other state-of-the-art baselines.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {466–475},
numpages = {10},
keywords = {experience replay, multi-agent reinforcement learning, priority optimization},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.5555/3635637.3662932,
author = {Hairi and Zhang, Zifan and Liu, Jia},
title = {Sample and Communication Efficient Fully Decentralized MARL Policy Evaluation via a New Approach: Local TD Update},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In actor-critic framework for fully decentralized multi-agent reinforcement learning (MARL), one of the key components is the MARL policy evaluation (PE) problem, where a set of N agents work cooperatively to evaluate the value function of the global states for a given policy through communicating with their neighbors. In MARL-PE, a critical challenge is how to lower the sample and communication complexities, which are defined as the number of training samples and communication rounds needed to converge to some ε-stationary point. To lower communication complexity in MARL-PE, a ''natural'' idea is to perform multiple local TD-update steps between each consecutive rounds of communication to reduce the communication frequency. However, the validity of the local TD-update approach remains unclear due to the potential ''agent-drift'' phenomenon resulting from heterogeneous rewards across agents in general. This leads to an interesting open question: Can the local TD-update approach entail low sample and communication complexities? In this paper, we make the first attempt to answer this fundamental question. We focus on the setting of MARL-PE with average reward, which is motivated by many multi-agent network optimization problems. Our theoretical and experimental results confirm that allowing multiple local TD-update steps is indeed an effective approach in lowering the sample and communication complexities of MARL-PE compared to consensus-based MARL-PE algorithms. Specifically, the local TD-update steps between two consecutive communication rounds can be as large as O (1/ε1/2 log(1/ε)) in order to converge to an ε-stationary point of MARL-PE. Moreover, we show theoretically that in order to reach the optimal sample complexity, the communication complexity of local TD-update approach is O (1/ε1/2 log(1/ε)).},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {789–797},
numpages = {9},
keywords = {multi-agent reinforcement learning, policy evaluation, sample and communication complexities, td learning},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@inbook{10.1145/3728725.3728825,
author = {Zhou, Ziye and Guo, Yanbin and Tang, Xin and Li, Kehan and You, Heng},
title = {Dynamic Decision Correction Framework Integrating Path Optimization and Incomplete Information Handling},
year = {2025},
isbn = {9798400713453},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3728725.3728825},
abstract = {Despite significant advancements in multi-agent systems, solving the optimal path problem and addressing incomplete information remain challenging in multi-agent reinforcement learning (MARL). Traditional MARL algorithms often struggle with these two critical issues. To address these challenges, this paper proposes a novel framework called DDCF(Dynamic Decision Correction Framework) that integrates deep reinforcement learning (DRL) with Long Short-Term Memory (LSTM) networks to optimize the decision-making process for agents navigating complex environments. The first key innovation of this framework is the application of LSTM networks, which enable agents to learn optimal paths over time while considering dynamic, temporal dependencies within the environment. The second key innovation is the introduction of a dynamic decision-making approach, which effectively handles the problem of incomplete information by enabling agents to make decisions based on both observable and inferred states. This decision method ensures that agents can intelligently handle uncertain environments by incorporating both certain and fuzzy information. Experimental results demonstrate that the proposed framework significantly outperforms traditional methods in terms of path optimization and decision-making performance, providing a robust solution for agents in environments characterized by incomplete information and complex decision tasks.},
booktitle = {Proceedings of the 2025 2nd International Conference on Generative Artificial Intelligence and Information Security},
pages = {632–642},
numpages = {11}
}

@inproceedings{10.5555/3535850.3536010,
author = {Yang, Jiachen and Wang, Ethan and Trivedi, Rakshit and Zhao, Tuo and Zha, Hongyuan},
title = {Adaptive Incentive Design with Multi-Agent Meta-Gradient Reinforcement Learning},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Critical sectors of human society are progressing toward the adoption of powerful artificial intelligence (AI) agents, which are trained individually on behalf of self-interested principals but deployed in a shared environment. Short of direct centralized regulation of AI, which is as difficult an issue as regulation of human actions, one must design institutional mechanisms that indirectly guide agents' behaviors to safeguard and improve social welfare in the shared environment. Our paper focuses on one important class of such mechanisms: the problem of adaptive incentive design, whereby a central planner intervenes on the payoffs of an agent population via incentives in order to optimize a system objective. To tackle this problem in high-dimensional environments whose dynamics may be unknown or too complex to model, we propose a model-free meta-gradient method to learn an adaptive incentive function in the context of multi-agent reinforcement learning. Via the principle of online cross-validation, the incentive designer explicitly accounts for its impact on agents' learning and, through them, the impact on future social welfare. Experiments on didactic benchmark problems show that the proposed method can induce selfish agents to learn near-optimal cooperative behavior and significantly outperform learning-oblivious baselines. When applied to a complex simulated economy, the proposed method finds tax policies that achieve better trade-off between economic productivity and equality than baselines, a result that we interpret via a detailed behavioral analysis.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1436–1445},
numpages = {10},
keywords = {incentive design, multi-agent reinforcement learning},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@article{10.1145/3731246,
author = {Han, Weizhen and Liu, Bingyi and Dong, Mianxiong and Ota, Kaoru and Shao, Xun and Ji, Yusheng},
title = {Edge Intelligence Enabled Data Transmission in IoV:  Integrating Link Optimization and Packet Routing},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1550-4859},
url = {https://doi.org/10.1145/3731246},
doi = {10.1145/3731246},
abstract = {In the Internet of Vehicles (IoV), effective data transmission between vehicles enhances awareness of potential risks and traffic anomalies, significantly improving traffic safety and efficiency. However, in complex urban environments, existing routing protocols face challenges such as severe attenuation of wireless signals, inefficiency of the three-handshake mechanism, and low reusability of multi-hop links, These issues drastically affect the performance of routing protocols. To address these problems, this paper develops an edge intelligence-enabled routing protocol (EIRP), integrating link optimization and packet routing. The proposed EIRP consists of three key components. Firstly, we propose a traffic awareness-based multi-hop link construction scheme to construct and maintain links among interactions according to traffic conditions. The primary goal of this scheme is to establish stable links before message transmission to improve its efficiency. Secondly, we design a multi-hop link scheduling scheme to enhance data transmission performance and improve the utility of the constructed multi-hop links. Thirdly, we propose a cooperative multi-agent reinforcement learning (MARL)-based packet routing scheme, leveraging individual rewards to address the sparse rewards challenge. Extensive experiments are conducted to evaluate the performance of the proposed EIRP. The results demonstrate that EIRP significantly improves the packet delivery rate and ensures low communication delay in various urban scenarios.},
note = {Just Accepted},
journal = {ACM Trans. Sen. Netw.},
month = jun,
keywords = {Edge Intelligence, link optimization, multi-agent reinforcement learning, multi-hop link, packet routing.}
}

@inproceedings{10.1145/3594739.3612912,
author = {Zhang, Hengxi and Tang, Huaze and Ding, Wenbo and Zhang, Xiao-Ping},
title = {Cooperative Multi-Type Multi-Agent Deep Reinforcement Learning for Resource Management in Space-Air-Ground Integrated Networks},
year = {2023},
isbn = {9798400702006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594739.3612912},
doi = {10.1145/3594739.3612912},
abstract = {The Space-Air-Ground Integrated Network (SAGIN), integrating heterogeneous devices including low earth orbit (LEO) satellites, unmanned aerial vehicles (UAVs), and ground users (GUs), holds significant promise for the advancing smart city applications. However, resource management of the SAGIN is a challenge requiring urgent study in that inappropriate resource management will cause poor data transmission, and hence affect the services in smart cities. In this paper, we develop a comprehensive SAGIN system that encompasses five distinct communication links and propose an efficient cooperative multi-type multi-agent deep reinforcement learning (CMT-MARL) method to address the resource management issue. The experimental results highlight the efficacy of proposed CMT-MARL, as evidenced by key performance indicators such as the overall transmission rate and transmission success rate. These results underscore the potential value and feasibility of future implementation of the SAGIN.},
booktitle = {Adjunct Proceedings of the 2023 ACM International Joint Conference on Pervasive and Ubiquitous Computing &amp; the 2023 ACM International Symposium on Wearable Computing},
pages = {712–717},
numpages = {6},
keywords = {SAGIN, multi-agent reinforcement learning, resource management},
location = {Cancun, Quintana Roo, Mexico},
series = {UbiComp/ISWC '23 Adjunct}
}

@article{10.1145/3728466,
author = {Zhang, Bo and Tan, Wen Jun and Cai, Wentong and Zhang, Allan N.},
title = {Privacy Meets Performance: Enhancing Distributed Simulation-based Federated Multi-agent Learning with Privacy-preserving Surrogate Model},
year = {2025},
issue_date = {October 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1049-3301},
url = {https://doi.org/10.1145/3728466},
doi = {10.1145/3728466},
abstract = {In recent years, model-free multi-agent reinforcement learning (MaRL) has become a powerful tool for learning effective policies to solve optimization problems. However, individual agents may raise concerns about sharing their internal data, simulation models, and decision models in collaborative optimization. Distributed simulation (DS) and federated learning have been widely used as privacy-preserving methods to hide simulation details and maintain data and model privacy. Despite their benefits, these methods often require large amounts of interaction and data to converge, which leads to a high communication time, especially if the agents are distributed around the world. To address this issue, we propose a distributed surrogate model for DS-based federated MaRL to utilize the surrogate model instead of DS during the training. This can enhance data efficiency and effectiveness to accelerate agent learning while maintaining data and model privacy. An aerospace supply chain (SC) is used as the experimental scenario to evaluate the performance of our proposed approach, in terms of SC profits, training convergence, and execution time. Experimental results show that our proposed approach can achieve higher SC profits with the same number of simulation runs, converge faster, and reduce execution time to gain the same level of SC profits.},
journal = {ACM Trans. Model. Comput. Simul.},
month = sep,
articleno = {33},
numpages = {16},
keywords = {Distributed surrogate model, multi-agent reinforcement learning, data-driven decision making, privacy preserving supply chain inventory management}
}

@inproceedings{10.5555/3635637.3662876,
author = {Chai, Jiajun and Fu, Yuqian and Zhao, Dongbin and Zhu, Yuanheng},
title = {Aligning Credit for Multi-Agent Cooperation via Model-based Counterfactual Imagination},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Recent years have witnessed considerable progress in model-based reinforcement learning research. Inspired by the significant improvement in sample efficiency, researchers have explored its application in multi-agent scenarios to mitigate the huge demands in training data of multi-agent reinforcement learning (MARL) approaches. However, existing methods retain the training framework designed for single-agent settings, resulting in inadequate promotion of multi-agent cooperation. In this work, we propose a novel model-based MARL method called Multi-Agent Counterfactual Dreamer (MACD). MACD introduces a centralized imagination with decentralized execution (CIDE) framework to generate higher-quality pseudo data for policy learning, thus further improving the algorithm's sample efficiency. Moreover, we address the credit assignment and non-stationary challenges by performing an additional counterfactual trajectory based on the learned world model. We provide a theoretical proof that this counterfactual policy update rule maximizes the multi-agent learning objective. Empirical studies validate the superiority of our method in terms of sample efficiency, training stability, and final cooperation performance when compared with several state-of-the-art model-free and model-based MARL algorithms. Ablation studies and visualization demonstration further underscore the significance of both the CIDE framework and the counterfactual module in our approach.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {281–289},
numpages = {9},
keywords = {counterfactual advantage, credit assignment, model-based reinforcement learning, multi-agent reinforcement learning},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@inproceedings{10.5555/3709347.3743809,
author = {Raissouni, Hassan and Bekhti, Wissal and El Khamlichi, Btissam and El Fallah Seghrouchni, Amal},
title = {Reputation-Filtered Reward Reshaping: Encouraging Cooperation in High Dimensional Semi-Cooperative Multi-agent Settings},
year = {2025},
isbn = {9798400714269},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In semi-cooperative settings, cooperation is induced by appropriate incentives that align individual agents' goals with a common objective. The primary challenge is balancing personal and collective goals, which introduces new complications. A key issue is that cooperating with all agents equally can result in poor decisions, suboptimal cooperation, and inefficiencies in task execution. Furthermore, agents must manage the trade-off between staying connected to share cooperation-related information and pursuing their own objectives. To tackle these issues, we propose a novel framework incorporating a filtered reward-reshaping mechanism with two main components: (1) a reputation system that evaluates trust and competency, allowing agents to assess and filter peers' contributions, collaborate with reliable partners, and improve learning efficiency, and (2) a density-focused Potential-Based Reward Shaping (PBRS) mechanism that promotes connectivity and encourages exploration by adjusting rewards based on the density of agents in the observable space. Our approach was tested against PED-DQN and Independent Q-Learners, demonstrating enhanced performance in high-dimensional semi-cooperative environments. Additionally, theoretical stability analysis confirmed the system's convergence to a desirable equilibrium, ensuring long-term stability.},
booktitle = {Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems},
pages = {1736–1744},
numpages = {9},
keywords = {cooperation, multi-agent system, reinforcement learning, reputation, reward reshaping, trust},
location = {Detroit, MI, USA},
series = {AAMAS '25}
}

@inproceedings{10.5555/3545946.3598673,
author = {Zhang, Shaowei and Cao, Jiahan and Yuan, Lei and Yu, Yang and Zhan, De-Chuan},
title = {Self-Motivated Multi-Agent Exploration},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In cooperative multi-agent reinforcement learning (CMARL), it is critical for agents to achieve a balance between self-exploration and team collaboration. However, agents can hardly accomplish the team task without coordination and they would be trapped in a local optimum where easy cooperation is accessed without enough individual exploration. Recent works mainly concentrate on agents' coordinated exploration, which brings about the exponentially grown exploration of the state space. To address this issue, we propose Self-Motivated Multi-Agent Exploration (SMMAE), which aims to achieve success in team tasks by adaptively finding a trade-off between self-exploration and team cooperation. In SMMAE, we train an independent exploration policy for each agent to maximize their own visited state space. Each agent learns an adjustable exploration probability based on the stability of the joint team policy. The experiments on highly cooperative tasks in StarCraft II micromanagement benchmark (SMAC) demonstrate that SMMAE can explore task-related states more efficiently, accomplish coordinated behaviours and boost the learning performance.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {476–484},
numpages = {9},
keywords = {multi-agent cooperation, multi-agent reinforcement learning, self-motivated exploration},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@article{10.1145/3639371,
author = {Xu, Meng and Chen, Xinhong and She, Yechao and Jin, Yang and Zhao, Guanyi and Wang, Jianping},
title = {Strengthening Cooperative Consensus in Multi-Robot Confrontation},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3639371},
doi = {10.1145/3639371},
abstract = {Multi-agent reinforcement learning (MARL) has proven effective in training multi-robot confrontation, such as StarCraft and robot soccer games. However, the current joint action policies utilized in MARL have been unsuccessful in recognizing and preventing actions that often lead to failures on our side. This exacerbates the cooperation dilemma, ultimately resulting in our agents acting independently and being defeated individually by their opponents. To tackle this challenge, we propose a novel joint action policy, referred to as the consensus action policy (CAP). Specifically, CAP records the number of times each joint action has caused our side to fail in the past and computes a cooperation tendency, which is integrated with each agent’s Q-value and Nash bargaining solution to determine a joint action. The cooperation tendency promotes team cooperation by selecting joint actions that have a high tendency of cooperation and avoiding actions that may lead to team failure. Moreover, the proposed CAP policy can be extended to partially observable scenarios by combining it with Deep Q network or actor-critic–based methods. We conducted extensive experiments to compare the proposed method with seven existing joint action policies, including four commonly used methods and three state-of-the-art methods, in terms of episode rewards, winning rates, and other metrics. Our results demonstrate that this approach holds great promise for multi-robot confrontation scenarios.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {30},
numpages = {27},
keywords = {Multi-robot confrontation, multi-agent reinforcement learning, cooperation dilemma, consensus action policy}
}

@inproceedings{10.5555/3709347.3743849,
author = {Villin, Victor and Buening, Thomas Kleine and Dimitrakakis, Christos},
title = {A Minimax Approach to Ad Hoc Teamwork},
year = {2025},
isbn = {9798400714269},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We propose a minimax-Bayes approach to Ad Hoc Teamwork (AHT) that optimizes policies against an adversarial prior over partners, explicitly accounting for uncertainty about partners at time of deployment. Unlike existing methods that assume a specific distribution over partners, our approach improves worst-case performance guarantees. Extensive experiments, including evaluations on coordinated cooking tasks from the Melting Pot suite, show our method's superior robustness compared to self-play, fictitious play, and best response learning. Our work highlights the importance of selecting an appropriate training distribution over teammates to achieve robustness in AHT.},
booktitle = {Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems},
pages = {2105–2114},
numpages = {10},
keywords = {ad hoc teamwork, multi-agent reinforcement learning},
location = {Detroit, MI, USA},
series = {AAMAS '25}
}

@inproceedings{10.1145/3678717.3691262,
author = {Wonsak, Shimon and Henke, Nils and Al-Rifai, Mohammad and Nolting, Michael and Nejdl, Wolfgang},
title = {Adaptive Dispatching of Mobile Charging Stations using Multi-Agent Graph Convolutional Cooperative-Competitive Reinforcement Learning},
year = {2024},
isbn = {9798400711077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678717.3691262},
doi = {10.1145/3678717.3691262},
abstract = {Battery electric vehicles (BEV) offer an opportunity to decrease transportation and mobility emissions significantly. The availability of charging station networks and infrastructure is crucial for the proliferation of BEVs. While the expansion of the charging networks is still slow, optimal utilization of the existing infrastructure and dispatching of mobile charging stations can serve as a bypass while more charging stations are built. In this work, we propose a novel multi-agent reinforcement learning - AdapMCS - approach for optimizing the adaptive dispatching of mobile charging stations to maximize the number of served charging requests by a charging station operator while improving the customer experience. By combining graph neural networks with reinforcement learning our approach is able to adapt to dynamic spatio-temporal changes in the demand distribution, for example, during big events such as concerts or fairs. Furthermore, we conduct a thorough evaluation using a publicly available real-world dataset and simulation of dynamic demand distribution changes. The results show that our adaptive dispatching approach is able to deal with the demand shifts and achieve significant gains for both customers, in terms of reducing waiting and charging times, and operators, in terms of increasing their profit.},
booktitle = {Proceedings of the 32nd ACM International Conference on Advances in Geographic Information Systems},
pages = {410–420},
numpages = {11},
keywords = {Graph Neural Networks, Mobile Charging Stations, Multi-Agent, Reinforcement Learning},
location = {Atlanta, GA, USA},
series = {SIGSPATIAL '24}
}

@inproceedings{10.5555/3545946.3598620,
author = {Lin, Fanqi and Huang, Shiyu and Pearce, Tim and Chen, Wenze and Tu, Wei-Wei},
title = {TiZero: Mastering Multi-Agent Football with Curriculum Learning and Self-Play},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent football poses an unsolved challenge in AI research. Existing work has focused on tackling simplified scenarios of the game, or else leveraging expert demonstrations. In this paper, we develop a multi-agent system to play the full 11 vs. 11 game mode, without demonstrations. This game mode contains aspects that present major challenges to modern reinforcement learning algorithms; multi-agent coordination, long-term planning, and non-transitivity. To address these challenges, we present TiZero; a self-evolving, multi-agent system that learns from scratch. TiZero introduces several innovations, including adaptive curriculum learning, a novel self-play strategy, and an objective that optimizes the policies of multiple agents jointly. Experimentally, it outperforms previous systems by a large margin on the Google Research Football environment, increasing win rates by over 30%. To demonstrate the generality of TiZero's innovations, they are assessed on several environments beyond football; Overcooked, Multi-agent Particle-Environment, Tic-Tac-Toe and Connect-Four.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {67–76},
numpages = {10},
keywords = {google research football, large-scale training, multi-agent reinforcement learning, self-play},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.5555/3635637.3662952,
author = {Jusup, Matej and P\'{a}sztor, Barna and Janik, Tadeusz and Zhang, Kenan and Corman, Francesco and Krause, Andreas and Bogunovic, Ilija},
title = {Safe Model-Based Multi-Agent Mean-Field Reinforcement Learning},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Many applications, e.g., in shared mobility, require coordinating a large number of agents. Mean-field reinforcement learning addresses the resulting scalability challenge by optimizing the policy of a representative agent interacting with the infinite population of identical agents instead of considering individual pairwise interactions. In this paper, we address an important generalization where there exist global constraints on the distribution of agents (e.g., requiring capacity constraints or minimum coverage requirements to be met). We propose Safe-M3-UCRL, the first model-based mean-field reinforcement learning algorithm that attains safe policies even in the case of unknown transitions. As a key ingredient, it uses epistemic uncertainty in the transition model within a log-barrier approach to ensure pessimistic constraints satisfaction with high probability. Beyond the synthetic swarm motion benchmark, we showcase Safe-M3-UCRL on the vehicle repositioning problem faced by many shared mobility operators and evaluate its performance through simulations built on vehicle trajectory data from a service provider in Shenzhen. Our algorithm effectively meets the demand in critical areas while ensuring service accessibility in regions with low demand.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {973–982},
numpages = {10},
keywords = {epistemic uncertainty, global safety, mean-field control, multi-agent reinforcement learning, probabilistic neural network ensemble, shared mobility, vehicle repositioning},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@article{10.1145/3691326,
author = {Castro, Pablo and Desai, Ajit and Du, Han and Garratt, Rodney and Rivadeneyra, Francisco},
title = {Estimating Policy Functions in Payment Systems Using Reinforcement Learning},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {2167-8375},
url = {https://doi.org/10.1145/3691326},
doi = {10.1145/3691326},
abstract = {This article uses reinforcement learning (RL) to approximate the policy rules of banks participating in a high-value payment system (HVPS). The objective of the RL agents is to learn a policy function for the choice of amount of liquidity provided to the system at the beginning of the day and the rate at which to pay intraday payments. Individual choices have complex strategic effects precluding a closed form solution of the optimal policy, except in simple cases. We show that, in a stylized two-agent setting, RL agents learn the optimal policy that minimizes the cost of processing their individual payments—without complete knowledge of the environment. We further demonstrate that, in more complex settings, both agents learn to reduce the cost of processing their payments and effectively respond to liquidity-delay tradeoff. Our results show the potential of RL to solve liquidity management problems in HVPS and provide new tools to assist policymakers in their mandates of ensuring safety and improving the efficiency of payment systems.},
journal = {ACM Trans. Econ. Comput.},
month = feb,
articleno = {1},
numpages = {31},
keywords = {Artificial intelligence, reinforcement learning, high-value payment systems}
}

@inproceedings{10.5555/3306127.3331848,
author = {Chen, Yujie and Qian, Yu and Yao, Yichen and Wu, Zili and Li, Rongqi and Zhou, Yinzhi and Hu, Haoyuan and Xu, Yinghui},
title = {Can Sophisticated Dispatching Strategy Acquired by Reinforcement Learning?},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In this paper, we study a courier dispatching problem (CDP) raised from an online pickup-service platform of Alibaba. The CDP aims to assign a set of couriers to serve pickup requests with stochastic spatial and temporal arrival rate among urban regions. The objective is to maximize the revenue of served requests given a limited number of couriers over a period of time. Many online algorithms such as dynamic matching and vehicle routing strategy from existing literature could be applied to tackle this problem. However, these methods rely on appropriately predefined optimization objectives at each decision point, which is hard in dynamic situations. This paper formulates the CDP as a Markov decision process (MDP) and proposes a data-driven approach to derive the optimal dispatching rule-set under different scenarios. Our method stacks multi-layer images of the spatial-and-temporal map and apply multi-agent reinforcement learning (MARL) techniques to evolve dispatching models. This method solves the learning inefficiency caused by traditional centralized MDP modeling. Through comprehensive experiments on both artificial dataset and real-world dataset, we show: 1) By utilizing historical data and considering long-term revenue gains, MARL achieves better performance than myopic online algorithms; 2) MARL is able to construct the mapping between complex scenarios to sophisticated decisions such as the dispatching rule. 3) MARL has the scalability to adopt in large-scale real-world scenarios.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1395–1403},
numpages = {9},
keywords = {courier dispatching problem, multi-agent reinforcement learning, smart cities},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.5555/3635637.3663009,
author = {Oesterle, Michael and Grams, Tim and Bartelt, Christian and Stuckenschmidt, Heiner},
title = {RAISE the Bar: Restriction of Action Spaces for Improved Social Welfare and Equity in Traffic Management},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Restriction-based governance has recently been proposed as an alternative to reward shaping for achieving system-level goals in competitive multi-agent systems. In this work, we apply these two approaches to the domain of traffic management, specifically investigating their efficacy and fairness. Our results show that edge restrictions in congested traffic networks are superior to dynamic pricing with regard to equity (i.e., equal treatment of agents) while achieving comparable travel-time improvements. We argue that the former metric, as an adequate proxy for fairness, can be crucial for the quality and acceptance of a governance scheme, particularly when human agents are affected.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {1492–1500},
numpages = {9},
keywords = {equity, fairness, governance, multi-agent system, restriction, traffic management},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@inproceedings{10.1145/3665065.3665068,
author = {D\'{\i}ez, \'{A}lvaro and Aznar, Fidel},
title = {Deep Reinforcement Learning for Swarm Navigation Using Different Evolution Strategies},
year = {2024},
isbn = {9798400717291},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665065.3665068},
doi = {10.1145/3665065.3665068},
abstract = {At present, swarm robotics represents a very interesting option for solving complex problems in dynamic and changing environments. To do so, the agents that form a robot swarm must be able to deploy one or more collective behaviors. In order to learn the latter, in the last few years, different automatic design methods have become very popular. In this paper, we propose a hybrid method to automatically design an autonomous navigation behavior for swarm robotics. The idea is to combine multi-agent reinforcement learning and neuro-evolution. Furthermore, we present and compare two evolutionary algorithms whose aim is to implement the learning process of the aforementioned behavior. In particular, they are the well-known cross-entropy method and covariance matrix adaptation evolution strategy. Finally, we include some experiments in which we demonstrate that the covariance matrix adaptation evolution strategy is more appropriate than the cross-entropy method, when performing a vision-based collision-free exploration task in a simulated indoor corridor.},
booktitle = {Proceedings of the 2024 8th International Conference on Intelligent Systems, Metaheuristics &amp; Swarm Intelligence},
pages = {14–19},
numpages = {6},
keywords = {autonomous navigation, evolution strategies, reinforcement learning, swarm robotics},
location = {Singapore, Singapore},
series = {ISMSI '24}
}

@inproceedings{10.1145/3357384.3357978,
author = {Jin, Jiarui and Zhou, Ming and Zhang, Weinan and Li, Minne and Guo, Zilong and Qin, Zhiwei and Jiao, Yan and Tang, Xiaocheng and Wang, Chenxi and Wang, Jun and Wu, Guobin and Ye, Jieping},
title = {CoRide: Joint Order Dispatching and Fleet Management for Multi-Scale Ride-Hailing Platforms},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357978},
doi = {10.1145/3357384.3357978},
abstract = {How to optimally dispatch orders to vehicles and how to trade off between immediate and future returns are fundamental questions for a typical ride-hailing platform. We model ride-hailing as a large-scale parallel ranking problem and study the joint decision-making task of order dispatching and fleet management in online ride-hailing platforms. This task brings unique challenges in the following four aspects. First, to facilitate a huge number of vehicles to act and learn efficiently and robustly, we treat each region cell as an agent and build a multi-agent reinforcement learning framework. Second, to coordinate the agents from different regions to achieve long-term benefits, we leverage the geographical hierarchy of the region grids to perform hierarchical reinforcement learning. Third, to deal with the heterogeneous and variant action space for joint order dispatching and fleet management, we design the action as the ranking weight vector to rank and select the specific order or the fleet management destination in a unified formulation. Fourth, to achieve the multi-scale ride-hailing platform, we conduct the decision-making process in a hierarchical way where a multi-head attention mechanism is utilized to incorporate the impacts of neighbor agents and capture the key agent in each scale. The whole novel framework is named as CoRide. Extensive experiments based on multiple cities real-world data as well as analytic synthetic data demonstrate that CoRide provides superior performance in terms of platform revenue and user experience in the task of city-wide hybrid order dispatching and fleet management over strong baselines.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {1983–1992},
numpages = {10},
keywords = {fleet management, hierarchical reinforcement learning, multi-agent reinforcement learning, order dispatching, ride-hailing},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3356464.3357707,
author = {Zhou, Ming and Chen, Yong and Wen, Ying and Yang, Yaodong and Su, Yufeng and Zhang, Weinan and Zhang, Dell and Wang, Jun},
title = {Factorized Q-learning for large-scale multi-agent systems},
year = {2019},
isbn = {9781450376563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356464.3357707},
doi = {10.1145/3356464.3357707},
abstract = {Deep Q-learning has achieved significant success in single-agent decision making tasks. However, it is challenging to extend Q-learning to large-scale multi-agent scenarios, due to the explosion of action space resulting from the complex dynamics between the environment and the agents. In this paper, we propose to make the computation of multi-agent Q-learning tractable by treating the Q-function (w.r.t. state and joint-action) as a high-order high-dimensional tensor and then approximate it with factorized pairwise interactions. Furthermore, we utilize a composite deep neural network architecture for computing the factorized Q-function, share the model parameters among all the agents within the same group, and estimate the agents' optimal joint actions through a coordinate descent type algorithm. All these simplifications greatly reduce the model complexity and accelerate the learning process. Extensive experiments on two different multi-agent problems demonstrate the performance gain of our proposed approach in comparison with strong baselines, particularly when there are a large number of agents.},
booktitle = {Proceedings of the First International Conference on Distributed Artificial Intelligence},
articleno = {7},
numpages = {7},
keywords = {large-scale multi-agent systems, multi-agent reinforcement learning},
location = {Beijing, China},
series = {DAI '19}
}

@inproceedings{10.5555/3545946.3598743,
author = {Wang, Jing and Song, Meichen and Gao, Feng and Liu, Boyi and Wang, Zhaoran and Wu, Yi},
title = {Differentiable Arbitrating in Zero-sum Markov Games},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We initiate the study of how to perturb the reward in a zero-sum Markov game with two players to induce a desirable Nash equilibrium, namely arbitrating. Such a problem admits a bi-level optimization formulation. The lower level requires solving the Nash equilibrium under a given reward function, which makes the overall problem challenging to optimize in an end-to-end way. We propose a backpropagation scheme that differentiates through the Nash equilibrium, which provides the gradient feedback for the upper level. In particular, our method only requires a black-box solver for the (regularized) Nash equilibrium (NE). We develop the convergence analysis for the proposed framework with proper black-box NE solvers and demonstrate the empirical successes in two multi-agent reinforcement learning (MARL) environments. Supplementary for all the proofs in this paper could be found in: https://arxiv.org/abs/2302.10058.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1034–1043},
numpages = {10},
keywords = {bi-level optimization, equilibrium refinement, zero-sum game},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.5555/3709347.3743758,
author = {Lee, Sunbowen and Lyu, Hongqin and Gong, Yicheng and Sun, Yingying and Deng, Chao},
title = {MacLight: Multi-scene Aggregation Convolutional Learning for Traffic Signal Control},
year = {2025},
isbn = {9798400714269},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Reinforcement learning methods have proposed promising traffic signal control policy that can be trained on large road networks. Current SOTA methods model road networks as topological graph structures, incorporate graph attention into deep Q-learning, and merge local and global embeddings to improve policy. However, graph-based methods are difficult to parallelize, resulting in huge time overhead. Moreover, none of the current peer studies have deployed dynamic traffic systems for experiments, which is far from the actual situation.In this context, we propose Multi-Scene Aggregation Convolutional Learning for traffic signal control (MacLight), which offers faster training speeds and more stable performance. Our approach consists of two main components. The first is the global representation, where we utilize variational autoencoders to compactly compress and extract the global representation. The second component employs the proximal policy optimization algorithm as the backbone, allowing value evaluation to consider both local features and global embedding representations. This backbone model significantly reduces time overhead and ensures stability in policy updates. We validated our method across multiple traffic scenarios under both static and dynamic traffic systems. Experimental results demonstrate that, compared to general and domian SOTA methods, our approach achieves superior stability, optimized convergence levels and the highest time efficiency. The code is under https://github.com/Aegis1863/MacLight.},
booktitle = {Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems},
pages = {1263–1271},
numpages = {9},
keywords = {multi-agent reinforcement learning, multi-scene convolution, traffic signal control, variational autoencoder},
location = {Detroit, MI, USA},
series = {AAMAS '25}
}

@inproceedings{10.1145/3543507.3583459,
author = {Li, Kaiyuan and Wang, Pengfei and Wang, Haitao and Liu, Qiang and Wang, Xingxing and Wang, Dong and Wang, Shangguang},
title = {Communicative MARL-based Relevance Discerning Network for Repetition-Aware Recommendation},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583459},
doi = {10.1145/3543507.3583459},
abstract = {The repeated user-item interaction now is becoming a common phenomenon in the e-commerce scenario. Due to its potential economic profit, various models are emerging to predict which item will be re-interacted based on the user-item interactions. In this specific scenario, item relevance is a critical factor that needs to be concerned, which tends to have different effects on the succeeding re-interacted one&nbsp;(i.e., stimulating or delaying its emergence). It is necessary to make a detailed discernment of item relevance for a better repetition-aware recommendation. Unfortunately, existing works usually mixed all these types, which may disturb the learning process and result in poor performance. In this paper, we introduce a novel Communicative MARL-based Relevance Discerning Network&nbsp;(CARDfor short) to automatically discern the item relevance for a better repetition-aware recommendation. Specifically, CARDformalizes the item relevance discerning problem into a communication selection process in MARL. CARDtreats each unique interacted item as an agent and defines three different communication types over agents, which are stimulative, inhibitive, and noisy respectively. After this, CARDutilizes a Gumbel-enhanced classifier to distinguish the communication types among agents, and an attention-based Reactive Point Process is further designed to transmit the well-discerned stimulative and inhibitive incentives separately among all agents to make an effective collaboration for repetition decisions. Experimental results on two real-world e-commerce datasets show that our proposed method outperforms the state-of-the-art recommendation methods in terms of both sequential and repetition-aware recommenders. Furthermore, CARDis also deployed in the online sponsored search advertising system in Meituan, obtaining a performance improvement of over 1.5% and 1.2% in CTR and effective Cost Per Mille&nbsp;(eCPM) respectively, which is significant to the business.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1231–1239},
numpages = {9},
keywords = {Communication Selection, Communication in Multi-agent Reinforcement Learning, Reactive Point Process, Repetition-aware Recommendation},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3534678.3539416,
author = {Zhang, Weijia and Liu, Hao and Han, Jindong and Ge, Yong and Xiong, Hui},
title = {Multi-Agent Graph Convolutional Reinforcement Learning for Dynamic Electric Vehicle Charging Pricing},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539416},
doi = {10.1145/3534678.3539416},
abstract = {Electric Vehicles (EVs) have been emerging as a promising low-carbon transport target. While a large number of public charging stations are available, the use of these stations is often imbalanced, causing many problems to Charging Station Operators (CSOs). To this end, in this paper, we propose a Multi-Agent Graph Convolutional Reinforcement Learning (MAGC) framework to enable CSOs to achieve more effective use of these stations by providing dynamic pricing for each of the continuously arising charging requests with optimizing multiple long-term commercial goals. Specifically, we first formulate this charging station request-specific dynamic pricing problem as a mixed competitive-cooperative multi-agent reinforcement learning task, where each charging station is regarded as an agent. Moreover, by modeling the whole charging market as a dynamic heterogeneous graph, we devise a multi-view heterogeneous graph attention networks to integrate complex interplay between agents induced by their diversified relationships. Then, we propose a shared meta generator to generate individual customized dynamic pricing policies for large-scale yet diverse agents based on the extracted meta characteristics. Finally, we design a contrastive heterogeneous graph pooling representation module to learn a condensed yet effective state action representation to facilitate policy learning of large-scale agents. Extensive experiments on two real-world datasets demonstrate the effectiveness of MAGC and empirically show that the overall use of stations can be improved if all the charging stations in a charging market embrace our dynamic pricing policy.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2471–2481},
numpages = {11},
keywords = {charging station dynamic pricing, graph contrastive learning, graph neural networks, multi-agent reinforcement learning},
location = {Washington DC, USA},
series = {KDD '22}
}

@inbook{10.1145/3727648.3727813,
author = {Yan, Shengming and Ye, Nan and Tan, Qijing and Zhong, Weibo},
title = {USVs Formation Generation Method Using MADDPG},
year = {2025},
isbn = {9798400712647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3727648.3727813},
abstract = {In this paper, we propose the generation of Unmanned Surface Vehicle(USV) formations using the MADDPG algorithm. In this algorithm, each USV updates its actor and critic networks based on the global state of the environment and other USVs' actions, which allowing for efficient collaboration and the generation of formations. Different initial positions were set in these experiments, by which the simulations test the performances of formation generation. It is observed from the results that starting from different initial placements, with successful generation of the desired formations, the targets of position may be reached by USVs. More specifically, with different initial placements, the course angle error and final position accuracy are slightly different. In general, the formation success rate reaches 88.1% with a course angle error of 15.3%. The results reflect that the MADDPG algorithm is effective and robust for multi-agent formation control.},
booktitle = {Proceedings of the 4th International Conference on Computer, Artificial Intelligence and Control Engineering},
pages = {1002–1005},
numpages = {4}
}

@inproceedings{10.5555/3306127.3331992,
author = {Han, Dongge and Boehmer, Wendelin and Wooldridge, Michael and Rogers, Alex},
title = {Multi-Agent Hierarchical Reinforcement Learning with Dynamic Termination},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In a multi-agent system, an agent's optimal policy will typically depend on the policies of other agents. Predicting the behaviours of others, and responding promptly to changes in such behaviours, is therefore a key issue in multi-agent systems research. One obvious possibility is for each agent to broadcast their current intention, for example, the currently executed option in a hierarchical RL framework. However, this approach results in inflexible agents when options have an extended duration. While adjusting the executed option at each step improves flexibility from a single-agent perspective, frequent changes in options can induce inconsistency between an agent's actual behaviour and its broadcasted intention. In order to balance flexibility and predictability, we propose a dynamic termination Bellman equation that allows the agents to flexibly terminate their options.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2006–2008},
numpages = {3},
keywords = {hierarchical reinforcement learning, multi-agent reinforcement learning},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@article{10.1145/3058592,
author = {Wang, Hongbign and Chen, Xin and Wu, Qin and Yu, Qi and Hu, Xingguo and Zheng, Zibin and Bouguettaya, Athman},
title = {Integrating Reinforcement Learning with Multi-Agent Techniques for Adaptive Service Composition},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/3058592},
doi = {10.1145/3058592},
abstract = {Service-oriented architecture is a widely used software engineering paradigm to cope with complexity and dynamics in enterprise applications. Service composition, which provides a cost-effective way to implement software systems, has attracted significant attention from both industry and research communities. As online services may keep evolving over time and thus lead to a highly dynamic environment, service composition must be self-adaptive to tackle uninformed behavior during the evolution of services. In addition, service composition should also maintain high efficiency for large-scale services, which are common for enterprise applications. This article presents a new model for large-scale adaptive service composition based on multi-agent reinforcement learning. The model integrates reinforcement learning and game theory, where the former is to achieve adaptation in a highly dynamic environment and the latter is to enable agents to work for a common task (i.e., composition). In particular, we propose a multi-agent Q-learning algorithm for service composition, which is expected to achieve better performance when compared with the single-agent Q-learning method and multi-agent SARSA (State-Action-Reward-State-Action) method. Our experimental results demonstrate the effectiveness and efficiency of our approach.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = may,
articleno = {8},
numpages = {42},
keywords = {Service composition, game theory, multi-agent system, reinforcement learning}
}

@inproceedings{10.1109/ASONAM49781.2020.9381448,
author = {Chen, Yang and Liu, Jiamou},
title = {Social capital games as a framework for social structural pattern emergence},
year = {2021},
isbn = {9781728110561},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASONAM49781.2020.9381448},
doi = {10.1109/ASONAM49781.2020.9381448},
abstract = {Prominent structural patterns such as small-world and core-periphery structures amount to some of the most important emergent characteristics of a social network. Yet little work is done to interpret these emergent phenomena in a unified way. Towards a unified interpretation framework, we connect the establishment of social patterns with social capital. Social capital captures the benefits that an individual gains from its social surrounding. We argue that individuals' desire to gaining higher social capital may give rise to important network properties. To validate this claim, we propose social capital game that mathematically conceptualizes bonding and bridging social capital. This framework allows us to regard individuals in a social network as learning agents who gain social capital through iteratively building interpersonal ties. The link-building decisions of these agents are guided by a multi-agent reinforcement learning (MARL) algorithm which improves agents' capability through repeated game plays. We conduct a series of experiments which demonstrate (1) the collective behaviors of the agents give rise to salient social patterns, and (2) by varying agents' preferences to different forms of social capital, different types of social patterns emerge. In particular, bonding social capital plays a pivotal role in the formation of a community structure in the network while bridging social capital is instrumental to the emergence of core-periphery structure. Our work sheds light on the formation of complex network phenomena.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {309–316},
numpages = {8},
keywords = {multi-agent reinforcement learning, network formation, social capital, social structures},
location = {Virtual Event, Netherlands},
series = {ASONAM '20}
}

@inproceedings{10.1145/3307772.3335321,
author = {Kell, Alexander and Forshaw, Matthew and McGough, A. Stephen},
title = {ElecSim: Monte-Carlo Open-Source Agent-Based Model to Inform Policy for Long-Term Electricity Planning},
year = {2019},
isbn = {9781450366717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307772.3335321},
doi = {10.1145/3307772.3335321},
abstract = {Due to the threat of climate change, a transition from a fossil-fuel based system to one based on zero-carbon is required. However, this is not as simple as instantaneously closing down all fossil fuel energy generation and replacing them with renewable sources -- careful decisions need to be taken to ensure rapid but stable progress. To aid decision makers, we present a new tool, ElecSim, which is an open-sourced agent-based modelling framework used to examine the effect of policy on long-term investment decisions in electricity generation. ElecSim allows non-experts to rapidly prototype new ideas.Different techniques to model long-term electricity decisions are reviewed and used to motivate why agent-based models will become an important strategic tool for policy. We motivate why an open-source toolkit is required for long-term electricity planning.Actual electricity prices are compared with our model and we demonstrate that the use of a Monte-Carlo simulation in the system improves performance by 52.5%. Further, using ElecSim we demonstrate the effect of a carbon tax to encourage a low-carbon electricity supply. We show how a £40 ($50) per tonne of CO2 emitted would lead to 70% renewable electricity by 2050.},
booktitle = {Proceedings of the Tenth ACM International Conference on Future Energy Systems},
pages = {556–565},
numpages = {10},
location = {Phoenix, AZ, USA},
series = {e-Energy '19}
}

@inproceedings{10.1145/3278721.3278759,
author = {Sun, Fan-Yun and Chang, Yen-Yu and Wu, Yueh-Hua and Lin, Shou-De},
title = {Designing Non-greedy Reinforcement Learning Agents with Diminishing Reward Shaping},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278759},
doi = {10.1145/3278721.3278759},
abstract = {This paper intends to address an issue in RL that when agents possessing varying capabilities, most resources may be acquired by stronger agents, leaving the weaker ones "starving". We introduce a simple method to train non-greedy agents in multi-agent reinforcement learning scenarios with nearly no extra cost. Our model can achieve the following goals in designing the non-greedy agent:non-homogeneous equality, only need local information, cost-effective, generalizable and configurable. We propose the idea of diminishing reward that makes the agent feel less satisfied for consecutive rewards obtained. This idea allows the agents to behave less greedy with-out the need to explicitly coding any ethical pattern nor monitor other agents' status. Given our framework, resources distributed more equally without running the risk of reaching homogeneous equality. We designed two games, Gathering Game and Hunter Prey to evaluate the quality of the model.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {297–302},
numpages = {6},
keywords = {multi-agent reinforcement learning, non-greedy, reward shaping},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3688671.3688757,
author = {Liapis, Georgios and Vordou, Anna and Vlahavas, Ioannis},
title = {Machine Learning Methods for Emulating Personality Traits in a Gamified Environment},
year = {2024},
isbn = {9798400709821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3688671.3688757},
doi = {10.1145/3688671.3688757},
abstract = {Personality traits are regarded as a significant factor of competency for job candidates, for example, evaluating the capacity to work efficiently within a team. However, there is a gap in the traditional assessment system for these cases since they typically rely on self-answered questionnaires that are biased or easily exploitable. Artificial Intelligence techniques can fill this gap by generating objective data to define standard personality template profiles, utilizing trained Reinforcement Learning agents. In this paper, we propose a gamified framework that employs Machine Learning methods to emulate personality traits based on the players’ play styles, with the purpose of creating standard team profiles. The OCEAN Five personality model is used as a basis for this attempt, which characterizes personality as a synthesis of the five components: openness, conscientiousness, extraversion, agreeableness, and neuroticism. After generating gameplay data through self-play, we examine how various personality qualities, actions, and modes of communication impact the team performance of the agents, with respect to the different personality traits. Results indicate that the personality traits of the agents individually and as a team do impact their performance and efficiency. This can be used as a methodology for creating efficient individual bot agents or teams of agents in many game environments.},
booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence},
articleno = {5},
numpages = {8},
keywords = {Machine Learning, OCEAN 5, Gamified Environment},
location = {
},
series = {SETN '24}
}

@inproceedings{10.5555/3635637.3663072,
author = {Zhang, Mingyue and Li, Nianyu and Li, Jialong and Liao, Jiachun and Liu, Jiamou},
title = {Memory-Based Resilient Control Against Non-cooperation in Multi-agent Flocking},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Inspired by natural flocking behaviors, researchers aim to develop a distributed control approach for artificial agents to mimic these behaviors. The main challenge lies in maintaining the resilience of the artificial flock, as some agents inevitably display non-cooperative behavior, thereby deviating from the flocking objective. Existing control approaches, especially those based on learning algorithm, are susceptible to forgetting issues that non-cooperative agents can exploit to disrupt the flock formation. To address this problem, this study introduces a memory-based resilient control approach that strategically analyzes historical data across three distinct time scales (long, short, and periodic). The implementation of a long short periodic-term memory (LSP) algorithm employs accumulative discounted credibility evaluated by Q-learning to recognize long-term non-cooperation, utilizes a filtering rule to establish a trusted set excluding short-term non-cooperation, and integrates fast Fourier transform to refine the trusted set against periodic inconsistency. We assess the effectiveness of this approach through extensive experiments. The results highlight the potential and advantages of using LSP in flocking, enhancing the resilience of multi-agent flocking against complex non-cooperative threats.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {2075–2084},
numpages = {10},
keywords = {collective behaviour, flocking, multi-agent system, resilient consensus control},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@inproceedings{10.1145/3488560.3498373,
author = {Wen, Chao and Xu, Miao and Zhang, Zhilin and Zheng, Zhenzhe and Wang, Yuhui and Liu, Xiangyu and Rong, Yu and Xie, Dong and Tan, Xiaoyang and Yu, Chuan and Xu, Jian and Wu, Fan and Chen, Guihai and Zhu, Xiaoqiang and Zheng, Bo},
title = {A Cooperative-Competitive Multi-Agent Framework for Auto-bidding in Online Advertising},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498373},
doi = {10.1145/3488560.3498373},
abstract = {In online advertising, auto-bidding has become an essential tool for advertisers to optimize their preferred ad performance metrics by simply expressing high-level campaign objectives and constraints. Previous works designed auto-bidding tools from the view of single-agent, without modeling the mutual influence between agents. In this paper, we instead consider this problem from a distributed multi-agent perspective, and propose a general underlineM ulti-underlineA gent reinforcement learning framework for underlineA uto-underlineB idding, namely MAAB, to learn the auto-bidding strategies. First, we investigate the competition and cooperation relation among auto-bidding agents, and propose a temperature-regularized credit assignment to establish a mixed cooperative-competitive paradigm. By carefully making a competition and cooperation trade-off among agents, we can reach an equilibrium state that guarantees not only individual advertiser's utility but also the system performance (i.e., social welfare). Second, to avoid the potential collusion behaviors of bidding low prices underlying the cooperation, we further propose bar agents to set a personalized bidding bar for each agent, and then alleviate the revenue degradation due to the cooperation. Third, to deploy MAAB in the large-scale advertising system with millions of advertisers, we propose a mean-field approach. By grouping advertisers with the same objective as a mean auto-bidding agent, the interactions among the large-scale advertisers are greatly simplified, making it practical to train MAAB efficiently. Extensive experiments on the offline industrial dataset and Alibaba advertising platform demonstrate that our approach outperforms several baseline methods in terms of social welfare and revenue.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {1129–1139},
numpages = {11},
keywords = {auto-bidding, bid optimization, e-commerce advertising, multi-agent reinforcement learning},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@article{10.1145/3715701,
author = {Saleem, Kiran and Wang, Lei and Ahmed, Rana Zeeshan and Gadekallu, Thippa Reddy and Almadhor, Ahmad and Li, Yang},
title = {VoI-based Situation-Aware Routing Protocol for Non-linear Underwater Communication Networks},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1556-4665},
url = {https://doi.org/10.1145/3715701},
doi = {10.1145/3715701},
abstract = {One of the main challenges for underwater applications, such as environmental monitoring and disaster management, is achieving efficient data transmission in environments where conditions change rapidly, and resources need for data transport are scarce. The capability of evaluating the Value of information (VoI) enables us to assess these problems by proposing a Value of Information-based Situation-Aware Non-Linear Routing (VoI SANLR/VoI SANL) method. It aims to deal with critical event scenarios using BDI (Belief-Desire-Intention) logic criteria and prioritizing the timely uploading of data-driven information towards the destination. SANLR of VoI is developed to reduce energy consumption, end-to-end latency, jitter, and improve Packet Delivery Ratio (PDR) in underwater communication networks. VoI SANLR introduces principles of priority-based methods and intends to address challenges in terms of underwater environment such as varying channel conditions, lack energy resources, and real-time decision requirements by using SANLR. Energy optimization analysis reveals consistent outperformance, achieving a remarkable 95% reduction in energy consumption compared to other techniques. Low latency is maintained, ranging from 2.5 to 0.5 seconds, showcasing enhanced efficiency and scalability. VoI SANLR demonstrates exceptional performance in both throughput and jitter. It achieves the highest data transfer rates, ranging from 100 kbps to 110 kbps, indicating outstanding efficiency. Additionally, the jitter remains consistently low, between 1.8 ms and 2 ms, ensuring minimal delay variability and improved communication stability. PDR consistently surpasses other techniques, reaching a maximum of 99%. Additionally, network lifetime analysis demonstrates VoI SANLR's superiority, exhibiting the highest network lifetime at each node and a significant 31.25% improvement at Node 100 compared to other methods.},
note = {Just Accepted},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = jan,
keywords = {Situation-awareness, underwater communication network, intelligent decision support system, multi-agent system, emergent situation, non-linear environment}
}

@inproceedings{10.1145/3411295.3411317,
author = {Singh, Rohit and Sicker, Douglas},
title = {Ultra-dense low data rate (UDLD) communication in the THz},
year = {2020},
isbn = {9781450380836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411295.3411317},
doi = {10.1145/3411295.3411317},
abstract = {In the future, with the advent of the Internet of Things (IoT), wireless sensors, and multiple 5G applications yet to be developed, an indoor room might be filled with 1000s of devices. These devices will have different Quality of Service (QoS) demands and resource constraints, such as mobility, hardware, and efficiency requirements. The THz band has a massive greenfield spectrum and is envisioned to cater to these dense-indoor deployments. However, THz has multiple caveats, such as high absorption rate, limited coverage range, low transmit power, sensitivity to mobility, and frequent outages, making it challenging to deploy. THz might compel networks to be dependent on additional infrastructure, which might not be profitable for network operators and can even result in inefficient resource utilization for devices demanding low to moderate data rates. Using distributed Device-to-Device (D2D) communication in the THz, we can cater to these ultra-dense low data rate type applications in a constrained resource situation. We propose a 2-Layered distributed D2D model, where devices use coordinated multi-agent reinforcement learning (MARL) to maximize efficiency and user coverage for dense-indoor deployment. We explore the choice of features required to train the algorithms and how it impacts the system efficiency. We show that densification and mobility in a network can be used to further the limited coverage range of THz devices, without the need for extra infrastructure or resources.},
booktitle = {Proceedings of the 7th ACM International Conference on Nanoscale Computing and Communication},
articleno = {21},
numpages = {7},
keywords = {Beyond 5G (B5G), Terahertz (THz), densification, device-to-device (D2D) communication, indoor deployment, multi-agent reinforcement learning (MARL)},
location = {Virtual Event, USA},
series = {NanoCom '20}
}

@inproceedings{10.5555/3709347.3743853,
author = {Wang, Tonghan and Dong, Heng and Jiang, Yanchen and Parkes, David C. and Tambe, Milind},
title = {On Diffusion Models for Multi-Agent Partial Observability: Shared Attractors, Error Bounds, and Composite Flow},
year = {2025},
isbn = {9798400714269},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multiagent systems grapple with partial observability (PO), and the decentralized POMDP (Dec-POMDP) model highlights the fundamental nature of this challenge. Whereas recent approaches to addressing PO have appealed to deep learning models, providing a rigorous understanding of how these models and their approximation errors affect agents' handling of PO and their interactions remain a challenge. In addressing this challenge, we investigate reconstructing global states from local action-observation histories in Dec-POMDPs using diffusion models. We first find that diffusion models conditioned on local history represent possible states as stable fixed points. In collectively observable (CO) Dec-POMDPs, individual diffusion models conditioned on agents' local histories share a unique fixed point corresponding to the global state, while in non-CO settings, shared fixed points yield a distribution of possible states given joint history. We further find that, with deep learning approximation errors, fixed points can deviate from true states and the deviation is negatively correlated to the Jacobian rank. Inspired by this low-rank property, we bound a deviation by constructing a surrogate linear regression model that approximates the local behavior of a diffusion model. With this bound, we propose a composite diffusion process iterating over agents with theoretical convergence guarantees to the true state.},
booktitle = {Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems},
pages = {2143–2152},
numpages = {10},
keywords = {Dec-POMDP, diffusion model, fixed point, multi-agent, partial observability, state reconstruction from observation},
location = {Detroit, MI, USA},
series = {AAMAS '25}
}

@inproceedings{10.1145/3610419.3610499,
author = {Rangu, Gayathri and Nair, Shivashankar B.},
title = {On Mobile-Agent-based Swarm Reinforcement Learning in a Heterogeneous Robotic Network},
year = {2023},
isbn = {9781450399807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610419.3610499},
doi = {10.1145/3610419.3610499},
abstract = {Multiple robots constituting a swarm and cooperating with one another can greatly speed up learning processes. Sharing learned information amongst such entities could be performed in a centralized or decentralized manner. The former, though easy to implement, suffers from the single-node dependency issue. Swarm Learning (SL), a comparatively new approach, constitutes a decentralized Federated Learning paradigm, that integrates edge computing, peer-to-peer networking, coordination and secrecy. In SL, locally learned models are aggregated and shared amongst the nodes in the network without the use of a central entity. Implementing SL, on networked robotic applications, requires a group of connected robots to perform and learn tasks either individually or cooperatively. In this paper, we describe an approach for implementing SL on a set of robots that individually learn a task, using mobile agents. Since a swarm of connected real robots may not be affordable, we portray this SL approach on a heterogeneous set of connected simulated and real robots. The mobile agent migrates seamlessly through the network of simulated and real robots and performs the aggregation and sharing of the locally learned models thereby decentralizing the learning mechanism. Experiments conducted with simulated, real, and mixed sets of such robots, using Reinforcement Learning locally, have shown the efficacy and viability of SL in a heterogeneous network of robots.},
booktitle = {Proceedings of the 2023 6th International Conference on Advances in Robotics},
articleno = {80},
numpages = {6},
keywords = {Real-to-Sim Transfer, Sim-to-Real Transfer, Swarm Learning},
location = {Ropar, India},
series = {AIR '23}
}

@inproceedings{10.1145/3378065.3378071,
author = {YeeWai, Sim and WaiShiang, Cheah and bin Khairuddin, Muhammad Asyraf},
title = {Multi Agent Object Recognition: A Preliminary Study},
year = {2020},
isbn = {9781450361910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378065.3378071},
doi = {10.1145/3378065.3378071},
abstract = {A multi-agent system is used in various domain application. To what extent it is useful for image processing is yet to be reported. From the review, there are not many works that discuss the development experience in adopting the MAS platform in image processing. This paper reveals the potential of MAS and challenge in adopting MAS-Jade platform in object recognition. We present the experience in designing JADE based object recognition, especially how agent communication is used to coordinate different recognition agents to work together in object recognition. From the findings, multi agent system is able to improve the accuracy of object recognition. However, challenges like complexity in agent modelling; agent population; human agent teamwork; human in the loop in interrupting the intelligent architecture, having an "agent marketplace" for video surveillance are worth to futher explore.},
booktitle = {Proceedings of the 4th International Conference on Intelligent Information Processing},
pages = {29–35},
numpages = {7},
keywords = {Object recognition, multi agent system},
location = {China, China},
series = {ICIIP '19}
}

@inproceedings{10.1145/3716895.3716897,
author = {Liu, Weidong and Zhou, Liping},
title = {Advanced Framework for Equilibrium Management in Intricate Supply Chains Using Collective Reinforcement Learning Techniques},
year = {2025},
isbn = {9798400718007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716895.3716897},
doi = {10.1145/3716895.3716897},
abstract = {In this study, we present a novel method for handling intricate transportation networks through the application of advanced multi-agent reinforcement learning strategies. Conventional techniques frequently mix supply and demand forecasting with combinatorial optimization. However, non-linear commercial restrictions, the complex structure of transportation networks, and the erratic fluctuations in supply and demand pose serious obstacles to traditional resource management. Our novel method incorporates a cooperative mechanism inspired by the interactions among resource agents, which aids in the design of state and reward systems. This results in transportation tactics that are more successful and efficient. Comprehensive tests on a simulated marine transportation service have shown that our approach outperforms traditional combinatorial optimization approaches in terms of enhancing agent collaboration and resulting in a notable boost in performance and stability.},
booktitle = {Proceedings of the 5th International Conference on Artificial Intelligence and Computer Engineering},
pages = {8–12},
numpages = {5},
keywords = {logistics network, multi-agen, reinforcement learning, resource balancing},
location = {
},
series = {ICAICE '24}
}

@inproceedings{10.5555/3709347.3743803,
author = {Ponse, Koen and Plaat, Aske and van Stein, Niki and Moerland, Thomas M.},
title = {EconoJax: A Fast &amp; Scalable Economic Simulation in JAX},
year = {2025},
isbn = {9798400714269},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Accurate economic simulations often require many experimental runs, particularly when combined with reinforcement learning. Unfortunately, training reinforcement learning agents in multi-agent economic environments can be slow. This paper introduces EconoJax, a fast simulated economy, based on the AI economist [38]. EconoJax, and its training pipeline, are completely written in JAX. This allows EconoJax to scale to large population sizes and perform large experiments, while keeping training times within minutes. Through experiments with populations of 100 agents, we show how real-world economic behavior emerges through training within 15 minutes, in contrast to previous work that required several days. We additionally perform experiments in varying sized action spaces to test if some multi-agent methods produce more diverse behavior compared to others. Here, our findings indicate no notable differences in produced behavior with different methods as is sometimes suggested in earlier works.},
booktitle = {Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems},
pages = {1679–1687},
numpages = {9},
keywords = {economics, jax, multi-agent, reinforcement learning},
location = {Detroit, MI, USA},
series = {AAMAS '25}
}

@inproceedings{10.1145/3627676.3627678,
author = {Yuan, Lei and Li, Lihe and Zhang, Ziqian and Chen, Feng and Zhang, Tianyi and Guan, Cong and Yu, Yang and Zhou, Zhi-Hua},
title = {Learning to Coordinate with Anyone},
year = {2023},
isbn = {9798400708480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627676.3627678},
doi = {10.1145/3627676.3627678},
abstract = {In open multi-agent environments, the agents may encounter unexpected teammates. Classical multi-agent learning approaches train agents that can only coordinate with seen teammates. Recent studies attempted to generate diverse teammates in order to enhance the generalizable coordination ability, but were restricted by pre-defined teammates. In this work, our aim is to train agents with strong coordination ability by generating teammates that fully cover the teammate policy space, so that agents can coordinate with any teammates. Since the teammate policy space is too huge to be enumerated, we find only dissimilar teammates that are incompatible with controllable agents, which highly reduces the number of teammates that needed to be trained with. However, it is hard to determine the number of such incompatible teammates beforehand. We therefore introduce a continual multi-agent learning process, in which the agent learns to coordinate with different teammates until no more incompatible teammates can be found. The above idea is implemented in the proposed Macop (Multi-agent compatible policy learning) algorithm. We conduct experiments in 8 scenarios from 4 environments that have distinct coordination patterns. Experiments show that Macop generates training teammates with much lower compatibility than previous methods. As a result, in all scenarios Macop achieves the best overall coordination ability while never significantly worse than the baselines, showing strong generalization ability.},
booktitle = {Proceedings of the Fifth International Conference on Distributed Artificial Intelligence},
articleno = {4},
numpages = {9},
keywords = {Continual Learning, Coordination and Cooperation, Multi-agent System, Reinforcement Learning},
location = {Singapore, Singapore},
series = {DAI '23}
}

@inproceedings{10.1145/3656766.3656806,
author = {Wang, Ershen and Wu, Xiaotong and Hong, Chen and Chen, Jihao},
title = {An Algorithm Combining Hidden States for Monotonic Value Function Factorisation},
year = {2024},
isbn = {9798400716478},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3656766.3656806},
doi = {10.1145/3656766.3656806},
abstract = {By using the communication and shared cooperation information between agents to infer the observation states of other agents, partially observable problems in multi-agent systems can be solved by centralized training decentralized execution (CTDE). However, even in CTDE, agents may still get stuck in local optimal solutions. In order to solve the problem, we design a novel multi-agent reinforcement learning (MARL) algorithm, hidden state based QMIX (HSQMIX), which uses the hidden state generated by recurrent neural networks (RNNs) to replace the true state of the agent as the input of the mixing network, and introduces transformer to process the hidden state. Besides, the priority experience replay and temporal difference learning are elaborately integrated into the algorithm. We also evaluate HSQMIX in the StarCraft multi-agent challenge (SMAC) map and compare it with other popular MARL algorithms. Experimental results show that HSQMIX outperforms other algorithms by a large margin. Our work will provide new insights into the multi-agent cooperative game.},
booktitle = {Proceedings of the 2023 3rd International Conference on Big Data, Artificial Intelligence and Risk Management},
pages = {231–235},
numpages = {5},
location = {Chengdu, China},
series = {ICBAR '23}
}

@article{10.1145/3711696,
author = {Wang, Xuchuang and Chen, Yu-Zhen Janice and Liu, Xutong and Yang, Lin and Hajiesmaili, Mohammad and Towsley, Don and Lui, John C.S.},
title = {Asynchronous Multi-Agent Bandits: Fully Distributed vs. Leader-Coordinated Algorithms},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
url = {https://doi.org/10.1145/3711696},
doi = {10.1145/3711696},
abstract = {We study the cooperative asynchronous multi-agent multi-armed bandits problem, where each agent's active (arm pulling) decision rounds are asynchronous. That is, in each round, only a subset of agents is active to pull arms, and this subset is unknown and time-varying. We consider two models of multi-agent cooperation, fully distributed and leader-coordinated, and propose algorithms for both models that attain near-optimal regret and communications bounds, both of which are almost as good as their synchronous counterparts. The fully distributed algorithm relies on a novel communication policy consisting of accuracy adaptive and on-demand components, and successive arm elimination for decision-making. For leader-coordinated algorithms, a single leader explores arms and recommends them to other agents (followers) to exploit. As agents' active rounds are unknown, a competent leader must be chosen dynamically. We propose a variant of the Tsallis-INF algorithm with low switches to choose such a leader sequence. Lastly, we report numerical simulations of our new asynchronous algorithms with other known baselines.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = mar,
articleno = {3},
numpages = {39},
keywords = {asynchronous communication, fully distributed algorithms, leader-coordinated algorithms, multi-agent bandits}
}

@article{10.1145/3749985,
author = {Datta, Aniruddha and Yaganti, Bhanu and Palocska, Mate and Dove, Andrew and Peltz, Arik and Chakrabarty, Krishnendu},
title = {Test-Fleet Scheduling in Complex Validation and Production Environments},
year = {2025},
issue_date = {September 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3749985},
doi = {10.1145/3749985},
abstract = {We present a solution to the complex design-automation problem of scheduling test operations in a validation laboratory or production facility. Our goal is to maximize the utilization of a fleet of test stations and minimize the overall test time for a set of products. We consider the realistic scenario where tests can have dependency graphs, implying that some tests must be completed and passed before others can proceed. We also consider a mix of product types that require different kinds of tests and a mix of testers, which implies that each product can only be tested only on a specific set of testers. To ensure scalability and flexibility, we have formulated this scheduling problem as a “partially observable stochastic game”, a multi-agent extension of a partially observable Markov decision process. We have implemented multi-agent reinforcement learning agents to maximize parallelization in a manner that speeds up both training and inferencing. We present scheduling results for synthetic test cases as well as real-life data from a production facility.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = sep,
articleno = {79},
numpages = {32},
keywords = {Test-fleet scheduling, validation, design-automation process}
}

@inproceedings{10.5555/3398761.3398884,
author = {Phan, Thomy and Gabor, Thomas and Sedlmeier, Andreas and Ritz, Fabian and Kempter, Bernhard and Klein, Cornel and Sauer, Horst and Schmid, Reiner and Wieghardt, Jan and Zeller, Marc and Linnhoff-Popien, Claudia},
title = {Learning and Testing Resilience in Cooperative Multi-Agent Systems},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {State-of-the-art multi-agent reinforcement learning has achieved remarkable success in recent years. The success has been mainly based on the assumption that all teammates perfectly cooperate to optimize a global objective in order to achieve a common goal. While this may be true in the ideal case, these approaches could fail in practice, since in multi-agent systems (MAS), all agents may be a potential source of failure. In this paper, we focus on resilience in cooperative MAS and propose an Antagonist-Ratio Training Scheme (ARTS) by reformulating the original target MAS as a mixed cooperative-competitive game between a group of protagonists which represent agents of the target MAS and a group of antagonists which represent failures in the MAS. While the protagonists can learn robust policies to ensure resilience against failures, the antagonists can learn malicious behavior to provide an adequate test suite for other MAS. We empirically evaluate ARTS in a cyber physical production domain and show the effectiveness of ARTS w.r.t. resilience and testing capabilities.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1055–1063},
numpages = {9},
keywords = {adversarial learning, learning and testing, multi-agent learning},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/3510513.3510527,
author = {Li, Shufeng and Shao, Wei and Peng, Yunfei},
title = {Cooperative Spectrum Handoff based on Counterfactual Multi-agent Method},
year = {2022},
isbn = {9781450385848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510513.3510527},
doi = {10.1145/3510513.3510527},
abstract = {Cognitive radio networks (CRNs) improve spectrum resource utilization rate by providing dynamic spectrum access for second users (SUs) when the spectrum is used by primary users (PUs). When multiple SUs exist in a CRN at the same time, spectrum handoff will witness more complicated questions, such as non-stationary of environment and credit assignment. In this paper, we propose a multi-agent cooperative spectrum handoff method based on counterfactual multi-agent (COMA) algorithm to assist multiple SUs in handoff decision. With centralized training and decentralized execution, the proposed method can effectively deal with the problem of multi-agent spectrum Handoff. Simulation experiments prove that our method can guarantee the efficiency of handoff and quality of experience (QoE) requirement of SUs, and significantly improve the throughput of the system.},
booktitle = {Proceedings of the 2021 10th International Conference on Networks, Communication and Computing},
pages = {84–90},
numpages = {7},
keywords = {Counterfactual multi-agent, Multi-agent system, Spectrum handoff},
location = {Beijing, China},
series = {ICNCC '21}
}

@inproceedings{10.1145/3404397.3404425,
author = {He, Bo and Wang, Jingyu and Qi, Qi and Sun, Haifeng and Zhuang, Zirui and Liu, Cong and Liao, Jianxin},
title = {DeepHop on Edge: Hop-by-hop Routing byDistributed Learning with Semantic Attention},
year = {2020},
isbn = {9781450388160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404397.3404425},
doi = {10.1145/3404397.3404425},
abstract = {Multi-access Edge Computing (MEC) and ubiquitous smart devices help serve end-users efficiently and optimally through providing emerging edge-deployed services. Meanwhile, heavy and time-varying traffic loads are produced in the edge network, so that an efficient traffic forwarding mechanism is required. In this paper, we propose a parallel and distributed learning approach, DeepHop, to adapt to the volatile environments and realize hop-by-hop routing. The Multi-Agent Deep Reinforcement Learning (MADRL) is used to alleviate the edge network congestion and maximize the utilization of network resources. DeepHop determines the routing among edge network nodes for heterogeneous types of traffic according to the current workload and capability. By joining with an attention mechanism, DeepHop obtains the semantics from the elements of the network state to help the agents learn the importance of each element on routing. Experiment results show that DeepHop achieves the increase of successfully transmitted packets by 15% compared with the state-of-the-art algorithms. Besides, DeepHop with an attention mechanism reduces convergence time by nearly half compared with the common-used structures of neural networks.},
booktitle = {Proceedings of the 49th International Conference on Parallel Processing},
articleno = {58},
numpages = {11},
keywords = {distributed routing, multi-agent reinforcement learning, self-attention, wireless edge networks},
location = {Edmonton, AB, Canada},
series = {ICPP '20}
}

@inproceedings{10.5555/3709347.3743565,
author = {Chopra, Ayush and Kumar, Shashank and Kuru, Nurullah Giray and Raskar, Ramesh and Quera-Bofarull, Arnau},
title = {On the Limits of Agency in Agent-based Models},
year = {2025},
isbn = {9798400714269},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Agent-based modeling (ABM) offers powerful insights into complex systems, but its practical utility has been limited by computational constraints and simplistic agent behaviors, especially when simulating large populations. Recent advancements in large language models (LLMs) could enhance ABMs with adaptive agents, but their integration into large-scale simulations remains challenging. This work introduces a novel methodology that bridges this gap by efficiently integrating LLMs into ABMs, enabling the simulation of millions of adaptive agents. We present LLM archetypes, a technique that balances behavioral complexity with computational efficiency, allowing for nuanced agent behavior in large-scale simulations. Our analysis explores the crucial trade-off between simulation scale and individual agent expressiveness, comparing different agent architectures ranging from simple heuristic-based agents to fully adaptive LLM-powered agents. We demonstrate the real-world applicability of our approach through a case study of the COVID-19 pandemic, simulating 8.4 million agents representing New York City and capturing the intricate interplay between health behaviors and economic outcomes. Our method significantly enhances ABM capabilities for predictive and counterfactual analyses, addressing limitations of historical data in policy design. By implementing these advances in an open-source framework, we facilitate the adoption of LLM archetypes across diverse ABM applications. Our results show that LLM archetypes can markedly improve the realism and utility of large-scale ABMs while maintaining computational feasibility, opening new avenues for modeling complex societal challenges and informing data-driven policy decisions.},
booktitle = {Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems},
pages = {500–509},
numpages = {10},
keywords = {agent-based simulations, differentiable abms, generative agents, llm as abm agents},
location = {Detroit, MI, USA},
series = {AAMAS '25}
}

@inproceedings{10.1145/3366622.3368146,
author = {Suh, Young-Ho and Woo, Sung-Pil and Kim, Hyunhak and Park, Dong-Hwan},
title = {A sim2real framework enabling decentralized agents to execute MADDPG tasks},
year = {2019},
isbn = {9781450370370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366622.3368146},
doi = {10.1145/3366622.3368146},
abstract = {Multi-agent RL is a process of training the agents to collaborate with others. We argue that an additional 'reality gap' in the system aspects occurs when applying sim2real to the multi-agent RL, especially when performing the 'transferred' collaborative task in the real-world environment. In this paper, we propose an ADO framework enabling decentralized agents to participate in performing collaborative tasks without suffering from the reality gap. Our contribution is threefold. First, we clearly identify and summarize the reality gaps in the context of the sim2real of multi-agent RL. Second, we propose a new system model to deal with system issues derived from when executing collaborative tasks. Third, we design and implement a software framework to support system issues required in developing and executing collaborative tasks in the real world.},
booktitle = {Proceedings of the Workshop on Distributed Infrastructures for Deep Learning},
pages = {1–6},
numpages = {6},
keywords = {Decentralized system, Deep Learning Framework, Multi-Agent Reinforcement Learning, Sim2Real},
location = {Davis, CA, USA},
series = {DIDL '19}
}

@inproceedings{10.5555/3545946.3599162,
author = {Nafi, Nasik Muhammad},
title = {Learning Representations and Robust Exploration for Improved Generalization in Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Deep Reinforcement Learning agents typically aim to learn a task through interacting in a particular environment. However, training on such singleton RL tasks, where the agent interacts with the same environment in every episode, implicitly leads to overfitting. Thus, the agent fails to generalize to minor changes in the environment, especially in image-based observation. Generalization is one of the main contemporary research challenges and recently proposed environments that enable diversified episode generation opens up the possibility to investigate generalization. My initial work towards this objective includes representation learning through the partial decoupling of policy and value networks and hyperbolic discounting in a single-agent setting. Efficient exploration is another crucial aspect of achieving generalization when learning from limited data. My dissertation would focus on proposing and evaluating methods that enable better representation learning and exploration for unseen scenarios. Another key objective is to extend my work to multi-agent generalization which is comparatively less studied.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {3032–3034},
numpages = {3},
keywords = {discounting, exploration, generalization, multi-agent systems, reinforcement learning, representation learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3426826.3426839,
author = {Arques Corrales, Pilar and Aznar Gregori, Fidel},
title = {Swarm AGV Optimization Using Deep Reinforcement Learning},
year = {2020},
isbn = {9781450388344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426826.3426839},
doi = {10.1145/3426826.3426839},
abstract = {Behavior design for Automated Guided Vehicles (AGV) systems is an active research area, fundamental for robotics, industrial systems automation. The rise of machine learning neural systems and deep learning make promising results in a multitude of areas including warehouse environments.In this paper, several different policies will be obtained by using reinforcement learning on a heterogeneous swarm robotic system, applied for solving logistical tasks in Automated Guided Vehicles. More specifically, two different types of agents will be used: the vehicles that collect, transport and deposit their package and the traffic lights that regulate the number of vehicles that circulate on the tracks. The main objective of our work is to learn simultaneously two different control policies, one for each kind of agent.The obtained policies have shown their ability to correctly learn the package transport behavior in addition to balance traffic flow to facilitate agent mobility and avoid collisions. Furthermore, the scalability of the system and the behavior performance for different number of vehicles has been shown.},
booktitle = {Proceedings of the 2020 3rd International Conference on Machine Learning and Machine Intelligence},
pages = {65–69},
numpages = {5},
keywords = {Automated Guided Vehicles, Deep Reinforcement Learning, Multi Agent Reinforcement Learning},
location = {Hangzhou, China},
series = {MLMI '20}
}

@inproceedings{10.1145/3677052.3698621,
author = {Brusatin, Simone and Padoan, Tommaso and Coletta, Andrea and Delli Gatti, Domenico and Glielmo, Aldo},
title = {Simulating the Economic Impact of Rationality through Reinforcement Learning and Agent-Based Modelling},
year = {2024},
isbn = {9798400710810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677052.3698621},
doi = {10.1145/3677052.3698621},
abstract = {Agent-based models (ABMs) are simulation models used in economics to overcome some of the limitations of traditional frameworks based on general equilibrium assumptions. However, agents within an ABM follow predetermined ‘bounded rational’ behavioural rules which can be cumbersome to design and difficult to justify. Here we leverage multi-agent reinforcement learning (RL) to expand the capabilities of ABMs with the introduction of ‘fully rational’ agents that learn their policy by interacting with the environment and maximising a reward function. Specifically, we propose a ‘Rational macro ABM’ (R-MABM) framework by extending a paradigmatic macro ABM from the economic literature. We show that gradually substituting ABM firms in the model with RL agents, trained to maximise profits, allows for studying the impact of rationality on the economy. We find that RL agents spontaneously learn three distinct strategies for maximising profits, with the optimal strategy depending on the level of market competition and rationality. We also find that RL agents with independent policies, and without the ability to communicate with each other, spontaneously learn to segregate into different strategic groups, thus increasing market power and overall profits. Finally, we find that a higher number of rational (RL) agents in the economy always improves the macroeconomic environment as measured by total output. Depending on the specific rational policy, this can come at the cost of higher instability. Our R-MABM framework allows for stable multi-agent learning, is available in open source, and represents a principled and robust direction to extend economic simulators.},
booktitle = {Proceedings of the 5th ACM International Conference on AI in Finance},
pages = {159–167},
numpages = {9},
keywords = {agent-based modelling, macroeconomics, reinforcement learning},
location = {Brooklyn, NY, USA},
series = {ICAIF '24}
}

@inproceedings{10.1145/3545008.3545025,
author = {Tahir, Anam and Cui, Kai and Koeppl, Heinz},
title = {Learning Mean-Field Control for Delayed Information Load Balancing in Large Queuing Systems},
year = {2023},
isbn = {9781450397339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545008.3545025},
doi = {10.1145/3545008.3545025},
abstract = {Recent years have seen a great increase in the capacity and parallel processing power of data centers and cloud services. To fully utilize the said distributed systems, optimal load balancing for parallel queuing architectures must be realized. Existing state-of-the-art solutions fail to consider the effect of communication delays on the behaviour of very large systems with many clients. In this work, we consider a multi-agent load balancing system, with delayed information, consisting of many clients (load balancers) and many parallel queues. In order to obtain a tractable solution, we model this system as a mean-field control problem with enlarged state-action space in discrete time through exact discretization. Subsequently, we apply policy gradient reinforcement learning algorithms to find an optimal load balancing solution. Here, the discrete-time system model incorporates a synchronization delay under which the queue state information is synchronously broadcasted and updated at all clients. We then provide theoretical performance guarantees for our methodology in large systems. Finally, using experiments, we prove that our approach is not only scalable but also shows good performance when compared to the state-of-the-art power-of-d variant of the Join-the-Shortest-Queue (JSQ) and other policies in the presence of synchronization delays.},
booktitle = {Proceedings of the 51st International Conference on Parallel Processing},
articleno = {42},
numpages = {11},
keywords = {load balancing, mean-field control, parallel systems, reinforcement learning},
location = {Bordeaux, France},
series = {ICPP '22}
}

@inproceedings{10.5555/3709347.3743617,
author = {Hosseini, Mehran and Lomuscio, Alessio and Paoletti, Nicola},
title = {LTL Verification of Memoryful Neural Agents},
year = {2025},
isbn = {9798400714269},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We present a framework for verifying Memoryful Neural Multi-Agent Systems (MN-MAS) against full Linear Temporal Logic (LTL) specifications. In MN-MAS, agents interact with a non-deterministic, partially observable environment. Examples of MN-MAS include multi-agent systems based on feed-forward and recurrent neural networks or state-space models. Different from previous approaches, we support the verification of both bounded and unbounded LTL specifications. We leverage well-established bounded model checking techniques, including lasso search and invariant synthesis, to reduce the verification problem to that of constraint solving. To solve these constraints, we develop efficient methods based on bound propagation, mixed-integer linear programming, and adaptive splitting. We evaluate the effectiveness of our algorithms in single and multi-agent environments from the Gymnasium and PettingZoo libraries, verifying unbounded specifications for the first time and improving the verification time for bounded specifications by an order of magnitude compared to the SoA.},
booktitle = {Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems},
pages = {978–987},
numpages = {10},
keywords = {formal verification, neural net verification, recurrent neural nets, safe control, safe reinforcement learning, verification of multi-agent systems},
location = {Detroit, MI, USA},
series = {AAMAS '25}
}

@article{10.1109/TNET.2024.3453067,
author = {Park, Soohyun and Baek, Hankyul and Kim, Joongheon},
title = {Spatio-Temporal Multi-Metaverse Dynamic Streaming for Hybrid Quantum-Classical Systems},
year = {2024},
issue_date = {Dec. 2024},
publisher = {IEEE Press},
volume = {32},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2024.3453067},
doi = {10.1109/TNET.2024.3453067},
abstract = {According to the challenges related to the limited availability of quantum bits (qubits) in the era of noisy intermediate-scale quantum (NISQ), the immediate replacement of all components in existing network architectures with quantum computing devices may not be practical. As a result, implementing a hybrid quantum-classical system is regarded as one of effective strategies. In hybrid quantum-classical systems, quantum computing devices can be used for computation-intensive applications, such as massive scheduling in dynamic environments. Furthermore, one of most popular network applications is advanced social media services such as metaverse. Accordingly, this paper proposes an advanced multi-metaverse dynamic streaming algorithm in hybrid quantum-classical systems. For this purpose, the proposed algorithm consists of three stages. For the first stage, three-dimensional (3D) point cloud data gathering should be conducted using spatially scheduled observing devices from physical-spaces for constructing virtual multiple meta-spaces in metaverse server. This is for massive scheduling over dynamic situations, i.e., quantum multi-agent reinforcement learning-based scheduling is utilized for scheduling dimension reduction into a logarithmic-scale. For the second stage, a temporal low-delay metaverse server’s processor scheduler is designed for region-popularity-aware multiple virtual meta-spaces rendering contents allocation via modified bin-packing with hard real-time constraints. Lastly, a novel dynamic dynamic streaming algorithm is proposed for high-quality, differentiated, and stabilized meta-spaces rendering contents delivery to individual users via Lyapunov optimization theory. Our performance evaluation results verify that the proposed spatio-temporal algorithm outperforms benchmarks in various aspects over hybrid quantum-classical systems.},
journal = {IEEE/ACM Trans. Netw.},
month = sep,
pages = {5279–5294},
numpages = {16}
}

@inproceedings{10.5555/3535850.3535967,
author = {Phan, Thomy and Sommer, Felix and Altmann, Philipp and Ritz, Fabian and Belzner, Lenz and Linnhoff-Popien, Claudia},
title = {Emergent Cooperation from Mutual Acknowledgment Exchange},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Peer incentivization (PI) is a recent approach, where all agents learn to reward or to penalize each other in a distributed fashion which often leads to emergent cooperation. Current PI mechanisms implicitly assume a flawless communication channel in order to exchange rewards. These rewards are directly integrated into the learning process without any chance to respond with feedback. Furthermore, most PI approaches rely on global information which limits scalability and applicability to real-world scenarios, where only local information is accessible. In this paper, we propose Mutual Acknowledgment Token Exchange (MATE), a PI approach defined by a two-phase communication protocol to mutually exchange acknowledgment tokens to shape individual rewards. Each agent evaluates the monotonic improvement of its individual situation in order to accept or reject acknowledgment requests from other agents. MATE is completely decentralized and only requires local communication and information. We evaluate MATE in three social dilemma domains. Our results show that MATE is able to achieve and maintain significantly higher levels of cooperation than previous PI approaches. In addition, we evaluate the robustness of MATE in more realistic scenarios, where agents can defect from the protocol and where communication failures can occur.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1047–1055},
numpages = {9},
keywords = {emergent cooperation, multi-agent learning, mutual acknowledgments, peer incentivization, reinforcement learning},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/3490354.3494372,
author = {Ardon, Leo and Vadori, Nelson and Spooner, Thomas and Xu, Mengda and Vann, Jared and Ganesh, Sumitra},
title = {Towards a fully rl-based market simulator},
year = {2022},
isbn = {9781450391481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490354.3494372},
doi = {10.1145/3490354.3494372},
abstract = {We present a new financial framework where two families of RL-based agents representing the Liquidity Providers and Liquidity Takers learn simultaneously to satisfy their objective. Thanks to a parametrized reward formulation and the use of Deep RL, each group learns a shared policy able to generalize and interpolate over a wide range of behaviors. This is a step towards a fully RL-based market simulator replicating complex market conditions particularly suited to study the dynamics of the financial market under various scenarios.},
booktitle = {Proceedings of the Second ACM International Conference on AI in Finance},
articleno = {7},
numpages = {9},
keywords = {market making, multi-agent, reinforcement learning},
location = {Virtual Event},
series = {ICAIF '21}
}

@inproceedings{10.5555/3535850.3535952,
author = {Merhej, Ramona and Santos, Fernando P. and Melo, Francisco S. and Chetouani, Mohamed and Santos, Francisco C.},
title = {Cooperation and Learning Dynamics under Risk Diversity and Financial Incentives},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In this paper, we investigate the role of risk diversity in groups of agents learning to play collective risk dilemmas (CRDs). We show that risk diversity poses new challenges to cooperation that are not observed in homogeneous groups. While increasing average risk contributes, in general, for agents to cooperate with higher probability, increasing risk diversity significantly reduces a population's ability to achieve a collective target. Risk diversity leads to asymmetrical changes in agents policies --- i.e. the increase in contributions from individuals at high risk is unable to compensate for the decrease in contributions from individuals at low risk --- which reduces the total contributions in a population and overall social welfare. At the same time, risk diversity offers novel opportunities to design financial incentives, which, as we show, can improve cooperation, target achievement and global welfare beyond the levels obtained in the absence of diversity. Our results highlight the need to align risk perceptions among agents and implement diversity-based incentive policies in order to improve collectives' abilities to avoid future catastrophic events.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {908–916},
numpages = {9},
keywords = {collective risk dilemmas, cooperation, financial incentives, heterogeneous agents, reinforcement learning, risk diversity, social dilemmas, social simulation},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.5555/3545946.3598696,
author = {Zhang, Libo and Chen, Yang and Takisaka, Toru and Khoussainov, Bakh and Witbrock, Michael and Liu, Jiamou},
title = {Learning Density-Based Correlated Equilibria for Markov Games},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Correlated Equilibrium (CE) is a well-established solution concept that captures coordination among agents and enjoys good algorithmic properties. In real-world multi-agent systems, in addition to being in an equilibrium, agents' policies are often expected to meet requirements with respect to safety, and fairness. Such additional requirements can often be expressed in terms of the em state density which measures the state-visitation frequencies during the course of a game. However, existing CE notions or CE-finding approaches cannot explicitly specify a CE with particular properties concerning state density; they do so implicitly by either modifying reward functions or using value functions as the selection criteria. The resulting CE may thus not fully fulfil the state-density requirements. In this paper, we propose em Density-Based Correlated Equilibria (DBCE), a new notion of CE that explicitly takes state density as selection criterion. Concretely, we instantiate DBCE by specifying different state-density requirements motivated by real-world applications. To compute DBCE, we put forward the em Density Based Correlated Policy Iteration algorithm for the underlying control problem. We perform experiments on various games where results demonstrate the advantage of our CE-finding approach over existing methods in scenarios with state-density concerns.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {652–660},
numpages = {9},
keywords = {Markov games, correlated equilibrium, state density},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3297280.3297368,
author = {Talamini, Jacopo and Medvet, Eric and Bartoli, Alberto},
title = {Communication-based cooperative tasks: how the language expressiveness affects reinforcement learning},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297368},
doi = {10.1145/3297280.3297368},
abstract = {We consider a cooperative multi-agent system in which cooperation may be enforced by communication between agents but in which agents must learn to communicate. The system consists of a game in which agents may move in a 2D world and are given the task of reaching specified targets. Each agent knows the target of another agent but not its own, thus the only way to solve the task is for the agents to guide one another using communication and, in particular, by learning how to communicate. We cast this game in terms of a partially observed Markov game and show that agents may learn policies for moving and communicating in the form of a neural network by means of reinforcement learning. We investigate in depth the impact on the learning quality of the expressiveness of the language, which is a function of vocabulary size, number of agents and number of targets.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {898–905},
numpages = {8},
keywords = {agent communication, language expressiveness, multi-agent systems, neural networks, reinforcement learning},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3580305.3599379,
author = {Zhang, Hancheng and Li, Guozheng and Liu, Chi Harold and Wang, Guoren and Tang, Jian},
title = {HiMacMic: Hierarchical Multi-Agent Deep Reinforcement Learning with Dynamic Asynchronous Macro Strategy},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599379},
doi = {10.1145/3580305.3599379},
abstract = {Multi-agent deep reinforcement learning (MADRL) has been widely used in many scenarios such as robotics and game AI. However, existing methods mainly focus on the optimization of agents' micro policies without considering the macro strategy. As a result, they cannot perform well in complex or sparse reward scenarios like the StarCraft Multi-Agent Challenge (SMAC) and Google Research Football (GRF). To this end, we propose a hierarchical MADRL framework called "HiMacMic" with dynamic asynchronous macro strategy. Spatially, HiMacMic determines a critical position by using a positional heat map. Temporally, the macro strategy dynamically decides its deadline and updates it asynchronously among agents. We validate HiMacMic in four widely used benchmarks, namely: Overcooked, GRF, SMAC and SMAC-v2 with nine chosen scenarios. Results show that HiMacMic not only converges faster and achieves higher results than ten existing approaches, but also shows its adaptability to different environment settings.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3239–3248},
numpages = {10},
keywords = {macro strategy, multi-agent deep reinforcement learning},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@article{10.1145/3589303,
author = {Wang, Yatong and Wu, Yuncheng and Chen, Xincheng and Feng, Gang and Ooi, Beng Chin},
title = {Incentive-Aware Decentralized Data Collaboration},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589303},
doi = {10.1145/3589303},
abstract = {Data collaboration enables multiple parties to pool data for deriving meaningful data insights. However, data misuse and unlawful data collection have led to precautionary measures being imposed by individual organizations to guide against data leakage and abuse. As a response, decentralized federated learning (DFL) has emerged as an attractive paradigm to facilitate data collaboration while being amenable to privacy-preserving data and knowledge sharing, cost reduction, and prediction accuracy improvement. Unfortunately, the participating parties in DFL tend to be heterogeneous with skew datasets and uneven capabilities. Inevitably, training and transmission costs, and the presence of free-riders pose challenges to the adoption and participation of DFL. The absence of centralized parameter servers further exacerbates the problem of evaluating the contribution of each individual party. Therefore, an effective incentive mechanism is essential to promote data collaboration.In this paper, we propose a novel Incentive-aware Decentralized fEderated leArning (IDEA) framework for facilitating data collaboration. Specifically, we first design a customizable reward scheme for heterogeneous parties to optimize their respective objectives such as higher model accuracy, communication efficiency, and computational efficiency. To reward fairly to deserving parties while offering flexibility, we propose a novel multi-agent reinforcement learning (MARL) incentive mechanism, which enables heterogeneous parties to learn their own optimal collaboration policy. We then design an efficient decentralized data collaboration algorithm that supports the customizable reward scheme based on individual objective-specific collaboration policy. We theoretically prove that the algorithm achieves a Nash equilibrium, which ensures the fairness of the corresponding rewards for parties. We conduct extensive experiments to evaluate the performance of our proposed framework against four baselines on five real-world datasets. The results show that IDEA outperforms state-of-the-art methods in terms of effectiveness, efficiency, and accumulated reward.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {158},
numpages = {27},
keywords = {data collaboration, decentralized learning, incentive mechanism}
}

@inproceedings{10.5555/3306127.3331757,
author = {Lowe, Ryan and Foerster, Jakob and Boureau, Y-Lan and Pineau, Joelle and Dauphin, Yann},
title = {On the Pitfalls of Measuring Emergent Communication},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {How do we know if communication is emerging in a multi-agent system? The vast majority of recent papers on emergent communication show that adding a communication channel leads to an increase in reward or task success. This is a useful indicator, but provides only a coarse measure of the agent's learned communication abilities. As we move towards more complex environments, it becomes imperative to have a set of finer tools that allow qualitative and quantitative insights into the emergence of communication. This may be especially useful to allow humans to monitor agents' behaviour, whether for fault detection, assessing performance, or even building trust. In this paper, we examine a few intuitive existing metrics for measuring communication, and show that they can be misleading. Specifically, by training deep reinforcement learning agents to play simple matrix games augmented with a communication channel, we find a scenario where agents appear to communicate (their messages provide information about their subsequent action), and yet the messages do not impact the environment or other agent in any way. We explain this phenomenon using ablation studies and by visualizing the representations of the learned policies. We also survey some commonly used metrics for measuring emergent communication, and provide recommendations as to when these metrics should be used.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {693–701},
numpages = {9},
keywords = {deep learning, learning agent capabilities, multi-agent learning},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.5555/3535850.3535981,
author = {Seraj, Esmaeil and Wang, Zheyuan and Paleja, Rohan and Martin, Daniel and Sklar, Matthew and Patel, Anirudh and Gombolay, Matthew},
title = {Learning Efficient Diverse Communication for Cooperative Heterogeneous Teaming},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {High-performing teams learn intelligent and efficient communication and coordination strategies to maximize their joint utility. These teams implicitly understand the different roles of heterogeneous team members and adapt their communication protocols accordingly. Multi-Agent Reinforcement Learning (MARL) seeks to develop computational methods for synthesizing such coordination strategies, but formulating models for heterogeneous teams with different state, action, and observation spaces has remained an open problem. Without properly modeling agent heterogeneity, as in prior MARL work that leverages homogeneous graph networks, communication becomes less helpful and can even deteriorate the cooperativity and team performance. We propose Heterogeneous Policy Networks (HetNet) to learn efficient and diverse communication models for coordinating cooperative heterogeneous teams. Building on heterogeneous graph-attention networks, we show that HetNet not only facilitates learning heterogeneous collaborative policies per existing agent-class but also enables end-to-end training for learning highly efficient binarized messaging. Our empirical evaluation shows that HetNet sets a new state of the art in learning coordination and communication strategies for heterogeneous multi-agent teams by achieving an 8.1% to 434.7% performance improvement over the next-best baseline across multiple domains while simultaneously achieving a 200X reduction in the required communication bandwidth.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1173–1182},
numpages = {10},
keywords = {cooperative marl, efficient communication, heterogeneous graph attention networks, heterogeneous teaming},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.5555/3398761.3398895,
author = {Rodriguez-Soto, Manel and Lopez-Sanchez, Maite and Rodriguez-Aguilar, Juan A.},
title = {A Structural Solution to Sequential Moral Dilemmas},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Social interactions are key in multi-agent systems. Social dilemmas have been widely studied to model specific problems in social interactions. However, state-of-the-art social dilemmas have disregarded specific ethical aspects affecting interactions. Here we propose a novel model for social dilemmas, the so-called Sequential Moral Dilemmas, that do capture the notion of moral value. First, we provide a formal definition of sequential moral dilemmas as Markov Games. Thereafter, we formally characterise the necessary and sufficient conditions for agents to learn to behave ethically, so that they are aligned with the moral value. Moreover, we exploit our theoretical characterisation to provide a structural solution to a sequential moral dilemma, namely how to configure the Markov game to solve the dilemma. Finally, we illustrate our proposal through the so-called public civility game, an example of a sequential moral dilemma considering the civility value. We show the social benefits obtained when the agents learn to adhere to the moral value.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1152–1160},
numpages = {9},
keywords = {game theory for practical applications, normative systems, reinforcement learning, values in multi-agent systems, including privacy, safety, security and transparency},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.5555/3712729.3712794,
author = {Murali, Pavithra Sripathanallur and Mohebbi, Shima},
title = {Hybrid Simulation and Reinforcement Learning-Based Scheduling for Resilient Infrastructure Networks},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {Infrastructure systems are interdependent at various levels, and their collective performance is influenced by factors such as topology, budgetary decisions, resource availability, and awareness of interdependency. Traditional resource allocation models for improving resilience often assume a single decision-maker overseeing all scheduling decisions. However, critical infrastructures, characterized by a network-of-networks structure, are managed by individual entities with distinct boundaries. Moreover, the dynamic and stochastic nature of decision-making processes cannot always be captured via mathematical programming. This study develops a hybrid simulation model that merges top-down and bottom-up approaches. It captures organizational-level budgetary decision-making dynamics through system dynamics, and maintenance activities alongside evolving network performance through an agent-based model. Optimal restoration strategies maximizing network resilience are identified via deep reinforcement learning, constrained by financial allocations. This approach is applied to water distribution and mobility networks in Tampa, FL demonstrating our method's efficacy for restoring interdependent infrastructures.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {786–797},
numpages = {12},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@inproceedings{10.1145/3357384.3357902,
author = {Wei, Hua and Xu, Nan and Zhang, Huichu and Zheng, Guanjie and Zang, Xinshi and Chen, Chacha and Zhang, Weinan and Zhu, Yanmin and Xu, Kai and Li, Zhenhui},
title = {CoLight: Learning Network-level Cooperation for Traffic Signal Control},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357902},
doi = {10.1145/3357384.3357902},
abstract = {Cooperation among the traffic signals enables vehicles to move through intersections more quickly. Conventional transportation approaches implement cooperation by pre-calculating the offsets between two intersections. Such pre-calculated offsets are not suitable for dynamic traffic environments. To enable cooperation of traffic signals, in this paper, we propose a model, CoLight, which uses graph attentional networks to facilitate communication. Specifically, for a target intersection in a network, CoLight can not only incorporate the temporal and spatial influences of neighboring intersections to the target intersection, but also build up index-free modeling of neighboring intersections. To the best of our knowledge, we are the first to use graph attentional networks in the setting of reinforcement learning for traffic signal control and to conduct experiments on the large-scale road network with hundreds of traffic signals. In experiments, we demonstrate that by learning the communication, the proposed model can achieve superior performance against the state-of-the-art methods.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {1913–1922},
numpages = {10},
keywords = {deep reinforcement learning, multi-agent system, traffic signal control},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.5555/3535850.3535946,
author = {Lucas, Keane and Allen, Ross E.},
title = {Any-Play: An Intrinsic Augmentation for Zero-Shot Coordination},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Cooperative artificial intelligence with human or superhuman proficiency in collaborative tasks stands at the frontier of machine learning research. Prior work has tended to evaluate cooperative AI performance under the restrictive paradigms of self-play (teams composed of agents trained together) and cross-play (teams of agents trained independently but using the same algorithm). Recent work has indicated that AI optimized for these narrow settings may make for undesirable collaborators in the real-world. We formalize an alternative criteria for evaluating cooperative AI, referred to as inter-algorithm cross-play, where agents are evaluated on teaming performance with all other agents within an experiment pool with no assumption of algorithmic similarities between agents. We show that existing state-of-the-art cooperative AI algorithms, such as Other-Play and Off-Belief Learning, under-perform in this paradigm. We propose the Any-Play learning augmentation---a multi-agent extension of diversity-based intrinsic rewards for zero-shot coordination (ZSC)---for generalizing self-play-based algorithms to the inter-algorithm cross-play setting. We apply the Any-Play learning augmentation to the Simplified Action Decoder (SAD) and demonstrate state-of-the-art performance in the collaborative card game Hanabi.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {853–861},
numpages = {9},
keywords = {Hanabi, cooperative AI, reinforcement learning, zero-shot coordination},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.5555/3398761.3398817,
author = {Ghosh, Ahana and Tschiatschek, Sebastian and Mahdavi, Hamed and Singla, Adish},
title = {Towards Deployment of Robust Cooperative AI Agents: An Algorithmic Framework for Learning Adaptive Policies},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We study the problem of designing an AI agent that can robustly cooperate with agents of unknown type (i.e., previously unobserved behavior) in multi-agent scenarios. Our work is inspired by real-world applications in which an AI agent, e.g., a virtual assistant, has to cooperate with new types of agents/users after its deployment. We model this problem via parametric Markov Decision Processes where the parameters correspond to a user's type and characterize her behavior. In the test phase, the AI agent has to interact with a user of an unknown type. We develop an algorithmic framework for learning adaptive policies: our approach relies on observing the user's actions to make inferences about the user's type and adapting the policy to facilitate efficient cooperation. We show that without being adaptive, an AI agent can end up performing arbitrarily bad in the test phase. Using our framework, we propose two concrete algorithms for computing policies that automatically adapt to the user in the test phase. We demonstrate the effectiveness of our algorithms in a cooperative gathering game environment for two agents.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {447–455},
numpages = {9},
keywords = {learning agent-to-agent interactions, machine learning, reinforcement learning},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@article{10.1145/3565810,
author = {Andelfinger, Philipp},
title = {Towards Differentiable Agent-Based Simulation},
year = {2023},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1049-3301},
url = {https://doi.org/10.1145/3565810},
doi = {10.1145/3565810},
abstract = {Simulation-based optimization using agent-based models is typically carried out under the assumption that the gradient describing the sensitivity of the simulation output to the input cannot be evaluated directly. To still apply gradient-based optimization methods, which efficiently steer the optimization towards a local optimum, gradient estimation methods can be employed. However, many simulation runs are needed to obtain accurate estimates if the input dimension is large. Automatic differentiation (AD) is a family of techniques to compute gradients of general programs directly. Here, we explore the use of AD in the context of time-driven agent-based simulations. By substituting common discrete model elements such as conditional branching with smooth approximations, we obtain gradient information across discontinuities in the model logic. On the examples of a synthetic grid-based model, an epidemics model, and a microscopic traffic model, we study the fidelity and overhead of the differentiable simulations as well as the convergence speed and solution quality achieved by gradient-based optimization compared with gradient-free methods. In traffic signal timing optimization problems with high input dimension, the gradient-based methods exhibit substantially superior performance. A further increase in optimization progress is achieved by combining gradient-free and gradient-based methods. We demonstrate that the approach enables gradient-based training of neural network-controlled simulation entities embedded in the model logic. Finally, we show that the performance overhead of differentiable agent-based simulations can be reduced substantially by exploiting sparsity in the model logic.},
journal = {ACM Trans. Model. Comput. Simul.},
month = jan,
articleno = {27},
numpages = {26},
keywords = {Agent-based simulation, optimization, Backpropagation}
}

@inproceedings{10.1145/3461702.3462515,
author = {Chaput, R\'{e}my and Duval, J\'{e}r\'{e}my and Boissier, Olivier and Guillermin, Mathieu and Hassas, Salima},
title = {A Multi-Agent Approach to Combine Reasoning and Learning for an Ethical Behavior},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462515},
doi = {10.1145/3461702.3462515},
abstract = {The recent field of Machine Ethics is experiencing rapid growth to answer the societal need for Artificial Intelligence (AI) algorithms imbued with ethical considerations, such as benevolence toward human users and actors. Several approaches already exist for this purpose, mostly either by reasoning over a set of predefined ethical principles (Top-Down), or by learning new principles (Bottom-Up). While both methods have their own advantages and drawbacks, only few works have explored hybrid approaches, such as using symbolic rules to guide the learning process for instance, combining the advantages of each. This paper draws upon existing works to propose a novel hybrid method using symbolic judging agents to evaluate the ethics of learning agents' behaviors, and accordingly improve their ability to ethically behave in dynamic multi-agent environments. Multiple benefits ensue from this separation between judging and learning agents: agents can evolve (or be updated by human designers) separately, benefiting from co-construction processes; judging agents can act as accessible proxies for non-expert human stakeholders or regulators; and finally, multiple points of view (one per judging agent) can be adopted to judge the behavior of the same agent, which produces a richer feedback. Our proposed approach is applied to an energy distribution problem, in the context of a Smart Grid simulator, with continuous and multi-dimensional states and actions. The experiments and results show the ability of learning agents to correctly adapt their behaviors to comply with the judging agents' rules, including when rules evolve over time.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {13–23},
numpages = {11},
keywords = {ethical judgment, ethics, hybrid neural-symbolic learning, machine ethics, multi-agent learning, reinforcement learning},
location = {Virtual Event, USA},
series = {AIES '21}
}

@inproceedings{10.1145/3449726.3463173,
author = {P\"{a}tzel, David and Heider, Michael and Wagner, Alexander R. M.},
title = {An overview of LCS research from 2020 to 2021},
year = {2021},
isbn = {9781450383516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449726.3463173},
doi = {10.1145/3449726.3463173},
abstract = {The International Workshop on Learning Classifier Systems (IWLCS) is an annual workshop at the GECCO conference where new concepts and results regarding learning classifier systems (LCSs) are presented and discussed. One recurring part of the workshop agenda is a presentation that reviews and summarizes the advances made in the field over the last year; this is intended to provide an easy entry point to the most recent progress and achievements. The 2020 presentation was accompanied by a survey workshop paper, a practice which we hereby continue. We give an overview of all the LCS-related publications from 11 March 2020 to 10 March 2021. The 46 publications we review are grouped into seven overall topics: Formal theoretic advances, contributions to LCS-based multi-agent reinforcement learning, approaches to setting and adapting LCS hyperparameters, new LCS architectures and adaptations, LCS implementations, improvements to existing LCSs and applications of LCSs.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1648–1656},
numpages = {9},
keywords = {learning classifier systems, survey},
location = {Lille, France},
series = {GECCO '21}
}

@inproceedings{10.5555/3635637.3663003,
author = {Nakagawa, Haruyuki and Miyatani, Yoshitaka and Kanezaki, Asako},
title = {Linking Vision and Multi-Agent Communication through Visible Light Communication using Event Cameras},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Various robots, rovers, drones, and other agents of mass-produced products are expected to encounter scenes where they intersect and collaborate in the near future. In such multi-agent systems, individual identification and communication play crucial roles. In this paper, we explore camera-based visible light communication using event cameras to tackle this problem. An event camera captures the events occurring in regions with changes in brightness and can be utilized as a receiver for visible light communication, leveraging its high temporal resolution. Generally, agents with identical appearances in mass-produced products are visually indistinguishable when using conventional CMOS cameras. Therefore, linking visual information with information acquired through conventional radio communication is challenging. We empirically demonstrate the advantages of a visible light communication system employing event cameras and LEDs for visual individual identification over conventional CMOS cameras with ArUco marker recognition. In the simulation, we also verified scenarios where our event camera-based visible light communication outperforms conventional radio communication in situations with visually indistinguishable multi-agents. Finally, our newly implemented multi-agent system verifies its functionality through physical robot experiments.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {1436–1444},
numpages = {9},
keywords = {cooperation, event-based vision sensor, multiagent reinforcement learning, optical wireless communication},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@inproceedings{10.5555/3306127.3331971,
author = {Everett, Richard and Cobb, Adam and Markham, Andrew and Roberts, Stephen},
title = {Optimising Worlds to Evaluate and Influence Reinforcement Learning Agents},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Training reinforcement learning agents on a distribution of procedurally generated environments has become an increasingly common method for obtaining more generalisable agents. However, this makes evaluation challenging, as the space of possible environment settings is large; simply looking at the average performance is insufficient for understanding how well - or how poorly - the agents perform. To address this, we introduce a method for strategically evaluating and influencing the behaviour of reinforcement learning agents. Using deep generative modelling to encode the environment, we propose a World Agent which efficiently generates and optimises worlds (i.e. environment settings) relative to the performance of the agents. Through the use of our method on two distinct environments, we demonstrate the existence of worlds which minimise and maximise agent reward beyond the typically reported average reward. Additionally, we show how our method can also be used to modify the distribution of worlds that agents train on, influencing their emergent behaviour to be more desirable.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1943–1945},
numpages = {3},
keywords = {agent simulation, evaluation, procedurally generated environments, reinforcement learning, training},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.5555/3398761.3398953,
author = {Zhu, Changxi and Cai, Yi and Leung, Ho-fung and Hu, Shuyue},
title = {Learning by Reusing Previous Advice in Teacher-Student Paradigm},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Reinforcement Learning (RL) has been widely used to solve sequential decision-making problems. However, RL algorithms suffer from poor sample efficiency and require a long time to learn a suitable policy, especially when multiple agents are learning without prior knowledge. This problem can be alleviated through reusing knowledge from other agents during the learning process. One notable approach is advising actions based on a teacher-student relationship, where the decision of a student agent during learning is aided by an experienced teacher agent. A critical assumption in teacher-student paradigm is that the communication may be limited, so that a student may wait for a while and learn by itself before receiving the next advice. More importantly, in some noisy or stochastic environments, the student may not be able to master the advised actions when they are only performed once. We propose three methods for agents choosing between learning by exploration, asking for advice and reusing previous advice. The results show that our approaches significantly outperform existing advising methods without reusing advice.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1674–1682},
numpages = {9},
keywords = {action advising, multi-agent, reinforcement learning, teacher-student},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/3167486.3167574,
author = {El Fouki, Mohammed and Aknin, Noura and El. Kadiri, K. Ed},
title = {Intelligent Adapted e-Learning System based on Deep Reinforcement Learning},
year = {2017},
isbn = {9781450353069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167486.3167574},
doi = {10.1145/3167486.3167574},
abstract = {Today, E-Learning platforms are getting more popularity and relevance between educational institutions such as open and distance universities and research institutes. But unfortunately, these platforms present yet unsolved problems, due to the absence of face-to-face interaction. Distance formation instructors have real difficulties recognizing who their students are, understanding their student's behaviors in the virtual course, what's possibilities they have to achieve the subject, what complications they find. Basically, they require having a reply which assists them to increase the learning/teaching process. So, it is necessary to develop an intelligent and adapted learning strategy based on the recommendation system that helps professors to do their work efficiently.},
booktitle = {Proceedings of the 2nd International Conference on Computing and Wireless Communication Systems},
articleno = {85},
numpages = {6},
keywords = {Decision Support System, Deep Neural network, E-learning, Learning Management System (LMS), Personalized learning, Reinforcement Learning},
location = {Larache, Morocco},
series = {ICCWCS'17}
}

@article{10.1145/3700420,
author = {Li, Pengfei and Yang, Jianyi and Wierman, Adam and Ren, Shaolei},
title = {Learning-Augmented Decentralized Online Convex Optimization in Networks},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
url = {https://doi.org/10.1145/3700420},
doi = {10.1145/3700420},
abstract = {This paper studies learning-augmented decentralized online convex optimization in a networked multi-agent system, a challenging setting that has remained under-explored. We first consider a linear learning-augmented decentralized online algorithm (LADO-Lin) that combines a machine learning (ML) policy with a baseline expert policy in a linear manner. We show that, while LADO-Lin can exploit the potential of ML predictions to improve the average cost performance, it cannot have guaranteed worst-case performance. To address this limitation, we propose a novel online algorithm (LADO) that adaptively combines the ML policy and expert policy to safeguard the ML predictions to achieve strong competitiveness guarantees. We also prove the average cost bound for LADO, revealing the tradeoff between average performance and worst-case robustness and demonstrating the advantage of training the ML policy by explicitly considering the robustness requirement. Finally, we run an experiment on decentralized battery management. Our results highlight the potential of ML augmentation to improve the average performance as well as the guaranteed worst-case performance of LADO.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = dec,
articleno = {38},
numpages = {42},
keywords = {decentralized optimization, learning to optimize, online algorithm}
}

@inproceedings{10.1145/3696474.3698033,
author = {Wang, Wei and Gao, Jian and Wang, Miao and Zhao, Dawei},
title = {Research on Cooperative Hunting Strategy of Unmanned Surface Vehicle Based on Deep Reinforcement Learning},
year = {2025},
isbn = {9798400710100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696474.3698033},
doi = {10.1145/3696474.3698033},
abstract = {Unmanned Surface Vehicle (USV) swarm will play a key role in the future maritime operations, and the cooperative encirclement of USV is one of the important areas, and the key lies in the cooperative action to form the surrounding situation of the target. However, the traditional cluster encirclement algorithm is limited by the uncertainty of the environment, and its generalization ability is low. Especially, the encirclement of escaping targets involves the fields of game theory, optimal control and so on. These traditional algorithms highly rely on the accurate mathematical modeling of the environmen. The construction of encirclement strategy is not only complex, but also very expensive. In addition, with the increase of the number of research objects and the transformation from single agent system to multi-agent system, the mathematical model of the system will become more and more complex, and it is very difficult to analyze and solve the optimal strategy. As long as the environment and task are changed, the rules of the encirclement strategy for the original task will not be able to adapt to the encirclement task under the new conditions, which are the limitations of the traditional algorithm. In order to improve this problem and achieve better performance in the cooperative scene, this paper uses the deep reinforcement learning algorithm to study the cooperative hunting problem of USV group in the obstacle constrained environment, especially the compound reward function with the guidance and obstacle avoidance characteristics, which helps to improve the tracking performance of USV.},
booktitle = {Proceedings of the 2024 4th International Joint Conference on Robotics and Artificial Intelligence},
pages = {168–173},
numpages = {6},
keywords = {Multi-Agent Deep Reinforcement Learning, PPO Algorithm, Unmanned Surface Vehicle Swarm},
location = {
},
series = {JCRAI '24}
}

@inproceedings{10.1145/3108421.3108434,
author = {Dong, Liu and xuefei, Yan and Xinming, Li and Shoubiao, Wang},
title = {Research of Multi-Agent Cognition and Decision},
year = {2017},
isbn = {9781450352482},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3108421.3108434},
doi = {10.1145/3108421.3108434},
abstract = {Reinforcement Learning(RL) is the main cognition and decision method of Agent since its adaptation and exploration ability to the unknown environment, but it cannot ensure the convergence for the multi-Agent situation since the transition of the state space is not only influenced by the Agent itself but also influenced by the other Agent's action, so the other Agent's action has to be taken into account. In consideration of the deep theory foundation and advantages to deal with the other player's policy, this paper think that combination of the games theory and multi-Agent is a breakthrough for the cognition and decision in the status of the multi-Agent Learning. Based on the introduction of the traditional decision technology, the learning technology and the typical algorithm framework of the multi-Agent reinforcement learning with the games theory, some related algorithms such as MinMax-Q, nash-Q, FF-Q, CE-Q and Nego-Q were presented and analyzed.},
booktitle = {Proceedings of the 1st International Conference on E-Commerce, E-Business and E-Government},
pages = {24–29},
numpages = {6},
keywords = {Games theory, Multi-Agent, Reinforcement Learning, cognition},
location = {Turku, Finland},
series = {ICEEG '17}
}

@inproceedings{10.5555/3306127.3331712,
author = {Yu, Chao and Wang, Xin and Hao, Jianye and Feng, Zhanbo},
title = {Reinforcement Learning for Cooperative Overtaking},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This paper solves the cooperative overtaking problem in autonomous driving using reinforcement learning techniques. Learning in such a situation is challenging due to vehicular mobility, which renders a continuously changing environment for each learning vehicle. Without no explicit coordination mechanisms, inefficient behaviors among vehicles might cause fatal uncoordinated outcomes. To solve this issue, we propose two basic coordination models to enable distributed learning of cooperative overtaking maneuvers in a group of vehicles. Extension mechanisms are then presented to make these models workable in more complex and realistic settings with any number of vehicles. Experiments verify that, by capturing the underlying consistency of identities or positions during vehicles' movement, efficient coordinated behaviors can be achieved simply through vehicles' local learning interactions.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {341–349},
numpages = {9},
keywords = {autonomous driving, cooperative overtaking, coordination graph, multiagent learning, reinforcement learning},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.5555/3237383.3237907,
author = {Hong, Zhang-Wei and Su, Shih-Yang and Shann, Tzu-Yun and Chang, Yi-Hsiang and Lee, Chun-Yi},
title = {A Deep Policy Inference Q-Network for Multi-Agent Systems},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We present DPIQN, a deep policy inference Q-network that targets multi-agent systems composed of controllable agents, collaborators, and opponents that interact with each other. We focus on one challenging issue in such systems---modeling agents with varying strategies---and propose to employ "policy features'' learned from raw observations (e.g., raw images) of collaborators and opponents by inferring their policies. DPIQN incorporates the learned policy features as a hidden vector into its own deep Q-network (DQN), such that it is able to predict better Q values for the controllable agents than the state-of-the-art deep reinforcement learning models. We further propose an enhanced version of DPIQN, called deep recurrent policy inference Q-network (DRPIQN), for handling partial observability. Both DPIQN and DRPIQN are trained by an adaptive training procedure, which adjusts the network's attention to learn the policy features and its own Q-values at different phases of the training process. We present a comprehensive analysis of DPIQN and DRPIQN, and highlight their effectiveness and generalizability in various multi-agent settings. Our models are evaluated in a classic soccer game involving both competitive and collaborative scenarios. Experimental results performed on 1 vs. 1 and 2 vs. 2 games show that DPIQN and DRPIQN demonstrate superior performance to the baseline DQN and deep recurrent Q-network (DRQN) models. We also explore scenarios in which collaborators or opponents dynamically change their policies, and show that DPIQN and DRPIQN do lead to better overall performance in terms of stability and mean scores.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1388–1396},
numpages = {9},
keywords = {deep reinforcement learning, multi-agent learning, opponent modeling},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.5555/3306127.3331997,
author = {Hu, Shuyue and Leung, Chin-wing and Leung, Ho-fung and Liu, Jiamou},
title = {To be Big Picture Thinker or Detail-Oriented? Utilizing Perceived Gist Information to Achieve Efficient Convention Emergence with Bilateralism and Multilateralism},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Recently, the study of social conventions (or norms) has attracted much attention. In this paper, we study the emergence of conventions from agents' repeated coordination games via bilateralism and multilateralism. We assume that agents can perceive the gist information, i.e., a big picture of how popular each action is in their neighbourhood. A novel reinforcement learning approach which utilizes the gist information is proposed. Experiment verifies that the proposed approach significantly outperforms the baseline and the state-of-the-art approaches, in terms of the speed of convention emergence.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2021–2023},
numpages = {3},
keywords = {convention emergence, fuzzy trace theory, norm},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@article{10.1145/3654439,
author = {Gavidia-Calderon, Carlos and Kordoni, Anastasia and Bennaceur, Amel and Levine, Mark and Nuseibeh, Bashar},
title = {The IDEA of Us: An Identity-Aware Architecture for Autonomous Systems},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3654439},
doi = {10.1145/3654439},
abstract = {Autonomous systems, such as drones and rescue robots, are increasingly used during emergencies. They deliver services and provide situational awareness that facilitate emergency management and response. To do so, they need to interact and cooperate with humans in their environment. Human behaviour is uncertain and complex, so it can be difficult to reason about it formally. In this article, we propose IDEA: an adaptive software architecture that enables cooperation between humans and autonomous systems, by leveraging the social identity approach. This approach establishes that group membership drives human behaviour. Identity and group membership are crucial during emergencies, as they influence cooperation among survivors. IDEA systems infer the social identity of surrounding humans, thereby establishing their group membership. By reasoning about groups, we limit the number of cooperation strategies the system needs to explore. IDEA systems select a strategy from the equilibrium analysis of game-theoretic models that represent interactions between group members and the IDEA system. We demonstrate our approach using a search-and-rescue scenario, in which an IDEA rescue robot optimises evacuation by collaborating with survivors. Using an empirically validated agent-based model, we show that the deployment of the IDEA system can reduce median evacuation time by 13.6%.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {164},
numpages = {38},
keywords = {Autonomous systems, game theory, social identity, agent-based modelling}
}

@inproceedings{10.1145/3308532.3329411,
author = {Collins Jackson, Aryana and Bevacqua, Elisabetta and De Loor, Pierre and Querrec, Ronan},
title = {Modelling an Embodied Conversational Agent for Remote and Isolated Caregivers on Leadership Styles},
year = {2019},
isbn = {9781450366724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308532.3329411},
doi = {10.1145/3308532.3329411},
abstract = {In a medical environment, coordination between medical staff is imperative. In cases in which a human doctor or medical coordinator is not present, patient care, particularly from non-experts, becomes more difficult. The difficulty increases when care is completed at a remote site, for example, on a manned mission to Mars. Communication capability from medical experts on Mars is limited. To address this problem, a medical assistant remote system is proposed to act as a coordinator between the humans present and the remote medical experts. A virtual agent assuming such a role will accept feedback from both, running the situation without errors and additional stress. Leadership styles will be employed by the agent to develop trust and perception of competence among its followers. Additionally, prediction of behaviour and situational changes by both medical professionals and by the agent are necessary in order to combat a 10-minute latency affecting communication between Earth and Mars.},
booktitle = {Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents},
pages = {256–259},
numpages = {4},
keywords = {eca, human-computer interaction, leadership behaviour, team coordination},
location = {Paris, France},
series = {IVA '19}
}

@inproceedings{10.1145/3452296.3472927,
author = {Yan, Siyu and Wang, Xiaoliang and Zheng, Xiaolong and Xia, Yinben and Liu, Derui and Deng, Weishan},
title = {ACC: automatic ECN tuning for high-speed datacenter networks},
year = {2021},
isbn = {9781450383837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452296.3472927},
doi = {10.1145/3452296.3472927},
abstract = {For the widely deployed ECN-based congestion control schemes, the marking threshold is the key to deliver high bandwidth and low latency. However, due to traffic dynamics in the high-speed production networks, it is difficult to maintain persistent performance by using the static ECN setting. To meet the operational challenge, in this paper we report the design and implementation of an automatic run-time optimization scheme, ACC, which leverages the multi-agent reinforcement learning technique to dynamically adjust the marking threshold at each switch. The proposed approach works in a distributed fashion and combines offline and online training to adapt to dynamic traffic patterns. It can be easily deployed based on the common features supported by major commodity switching chips. Both testbed experiments and large-scale simulations have shown that ACC achieves low flow completion time (FCT) for both mice flows and elephant flows at line-rate. Under heterogeneous production environments with 300 machines, compared with the well-tuned static ECN settings, ACC achieves up to 20% improvement on IOPS and 30% lower FCT for storage service. ACC has been applied in high-speed datacenter networks and significantly simplifies the network operations.},
booktitle = {Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
pages = {384–397},
numpages = {14},
keywords = {AQM, ECN, congestion control, datacenter network},
location = {Virtual Event, USA},
series = {SIGCOMM '21}
}

@article{10.1145/3716872,
author = {Wang, Yiming and Zhang, Weizhe and Hao, Meng and Kong, Weizhi and Wen, Yuan},
title = {Dynamic Power Management Through Multi-agent Deep Reinforcement Learning for Heterogeneous Systems},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/3716872},
doi = {10.1145/3716872},
abstract = {Power management and optimization play a significant role in modern computer systems, from battery-powered devices to servers running in data centers. Existing approaches for power capping fail to meet the requirements presented by dynamic workloads, and the situation becomes even more severe, given the divergent energy efficiency of workloads on heterogeneous hardware platforms. Adaptively optimizing energy consumption for dynamic workloads presents a great challenge to heterogeneous systems. To tackle this challenge, we present a machine learning based method to improve system-level power efficiency. We employ multi-agent deep reinforcement learning (MADRL) to automatically explore the relationship between long-term performance and the power budget for workloads of different types on classic CPU-GPU heterogeneous platforms. Our framework equips each device with an agent, enabling decentralized control over its power budget while maintaining centralized coordination to maximize the running time of applications within a power cap. We evaluate our approach against state-of-the-art methods on CPU-GPU platforms. Experimental results show that our method improves performance by an average of 8.5%. Additionally, our method is significantly more stable compared to the state-of-the-art heuristic approach.},
journal = {ACM Trans. Archit. Code Optim.},
month = jun,
articleno = {46},
numpages = {23},
keywords = {Energy and performance optimization, heterogeneous system, multiple agent deep reinforcement learning}
}

@inproceedings{10.5555/3237383.3238051,
author = {Lupu, Andrei and Durand, Audrey and Precup, Doina},
title = {Leveraging Observational Learning for Exploration in Bandits},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Learning from a target has been tackled in the reinforcement learning (RL) setting [1, 7] as imitation learning, either through behaviour cloning or inverse RL. In the former, the agent regresses directly onto the policy of a target [5], while in the latter, the agent infers a reward function from the behaviour of other agents and optimizes this function [6]. Extending upon these notions, observational learning was recently introduced in RL as the ability for an agent to modify its behavior or to acquire information as an effect of observing another agent sharing its environment [3]. In this work, we study the observational learning problem under the bandit setting. More specifically, we consider a learner (agent) that has access to actions performed by a target policy in the same environment. The agent only observes the target's actions, but not their associated rewards. Note that the target actions can in fact be performed by several other agents. This should not be confused with cooperative bandits [4], where several agents share knowledge with each other regarding the actions and obtained rewards.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2001–2003},
numpages = {3},
keywords = {bandits, imitation learning, observational learning},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.5555/3635637.3663268,
author = {Cui, Jiaxun},
title = {Communication and Generalization in Multi-Agent Learning},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Significant challenges exist in robustly interacting and communicating with a diverse array of agents, especially in intricate settings like autonomous driving where AI agents and humans coexist. This work approaches these challenges from three perspectives: generalization of agent policies, development of communication-supporting representations, and interactions between humans and AI agents using natural language. We provide an overview of preliminary achievements in each area and outline proposed research focusing on enhancing cooperative driving through natural language communication, aiming to comprehensively address these complex multi-agent interaction challenges.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {2728–2730},
numpages = {3},
keywords = {communication, generalization, human-ai interaction, llms},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@inproceedings{10.1145/3767052.3767085,
author = {Wang, Zhixin and Wang, Yong and Li, Xiaofeng and Feng, Shuai},
title = {Intelligent Dispatch and Optimization Technology in Market-Based Electricity Trading},
year = {2025},
isbn = {9798400716010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3767052.3767085},
doi = {10.1145/3767052.3767085},
abstract = {This paper presents a deep reinforcement learning (DRL)-based approach for intelligent dispatch optimization in electricity market environments. By modeling generation units, storage systems, and market signals within a multi-agent framework, the proposed system enables adaptive, real-time decision-making under uncertainty. Key components include state representation design, policy learning through actor-critic networks, and dynamic constraint handling. Simulation results on a modified IEEE 39-bus system show that the DRL model reduces dispatch cost by up to 12.6% during high-renewable scenarios, lowers curtailment, and improves system resilience under faults and forecast errors. Compared to traditional methods, the DRL agent demonstrates faster response, better generalization, and lower dependence on point predictions, indicating strong potential for future grid applications.},
booktitle = {Proceedings of the 2025 International Conference on Big Data, Artificial Intelligence and Digital Economy},
pages = {214–218},
numpages = {5},
keywords = {Deep Reinforcement Learning, Electricity Market, Intelligent Dispatch, Multi-Agent Systems},
location = {
},
series = {BDAIE '25}
}

@inproceedings{10.1145/3652628.3652655,
author = {Huang, Heng and Guo, Jianfeng and Zhu, Kun and Peng, Biqi},
title = {Flexible Mobility MADDPG Algorithm for UAV Swarm Unknown Region Searching},
year = {2024},
isbn = {9798400708831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652628.3652655},
doi = {10.1145/3652628.3652655},
abstract = {The application of UAV swarm has been very wide, including rescue, search, confrontation, assistance communication, path search, etc. Usually, when UAV swarm performs a task, it needs to solve many problems, including but not limited to the requirement of high efficiency in the execution of the task, the interference of the execution environment to the UAV, and the degree of cooperation between each individual UAV. However, some tasks are unknown and need an exploration process, and the exploration process can be repeated to mine the cooperation between UAV clusters, so as to improve the efficiency of the task execution. In order to improve the efficiency of UAV clusters searching unknown areas, this paper proposes an efficient UAV cluster search algorithm based on FM-MADDPG. The FM-MADDPG proposed in this paper optimizes the detection efficiency of UAV clusters in unknown areas, and the flexible assistance between multiple UAVs improves the efficiency of UAV clusters in performing tasks. We establish the UAV kinematics model and task model, and conduct simulation experiments. FM-MADDPG has better performance than MADDPG and DDPG algorithms in convergence speed and reward value.},
booktitle = {Proceedings of the 4th International Conference on Artificial Intelligence and Computer Engineering},
pages = {155–160},
numpages = {6},
location = {Dalian, China},
series = {ICAICE '23}
}

@inproceedings{10.5555/3306127.3331788,
author = {Song, Xinliang and Wang, Tonghan and Zhang, Chongjie},
title = {Convergence of Multi-Agent Learning with a Finite Step Size in General-Sum Games},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Learning in a multi-agent system is challenging because agents are simultaneously learning and the environment is not stationary, undermining convergence guarantees. To address this challenge, this paper presents a new gradient-based learning algorithm, called Gradient Ascent with Shrinking Policy Prediction (GA-SPP), which augments the basic gradient ascent approach with the concept of shrinking policy prediction. The key idea behind this algorithm is that an agent adjusts its strategy in response to the forecasted strategy of the other agent, instead of its current one. GA-SPP is shown formally to have Nash convergence in larger settings than existing gradient-based multi-agent learning methods. Furthermore, unlike existing gradient-based methods, GA-SPP's theoretical guarantees do not assume the learning rate to be infinitesimal.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {935–943},
numpages = {9},
keywords = {convergence, finite step size, multi-agent learning, nash equilibrium},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@article{10.1177/26339137231162025,
author = {Vinitsky, Eugene and K\"{o}ster, Raphael and Agapiou, John P and Du\'{e}\~{n}ez-Guzm\'{a}n, Edgar A and Vezhnevets, Alexander S and Leibo, Joel Z},
title = {A learning agent that acquires social norms from public sanctions in decentralized multi-agent settings},
year = {2023},
issue_date = {April-June 2023},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1177/26339137231162025},
doi = {10.1177/26339137231162025},
abstract = {Society is characterized by the presence of a variety of social norms: collective patterns of sanctioning that can prevent miscoordination and free-riding. Inspired by this, we aim to construct learning dynamics where potentially beneficial social norms can emerge. Since social norms are underpinned by sanctioning, we introduce a training regime where agents can access all sanctioning events but learning is otherwise decentralized. This setting is technologically interesting because sanctioning events may be the only available public signal in decentralized multi-agent systems where reward or policy-sharing is infeasible or undesirable. To achieve collective action in this setting, we construct an agent architecture containing a classifier module that categorizes observed behaviors as approved or disapproved, and a motivation to punish in accord with the group. We show that social norms emerge in multi-agent systems containing this agent and investigate the conditions under which this helps them achieve socially beneficial outcomes.},
journal = {Collective Intelligence},
month = apr,
numpages = {14},
keywords = {Multi-agent systems, social norms, reinforcement learning}
}

@inproceedings{10.5555/3635637.3663303,
author = {Dinh, Le Cong and Mguni, David Henry and Tran-Thanh, Long and Wang, Jun and Yang, Yaodong},
title = {A Summary of Online Markov Decision Processes with Non-oblivious Strategic Adversary},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We study a novel setting in Online Markov Decision Processes (OMDPs) where the loss function is chosen by a non-oblivious strategic adversary who follows a no-external regret algorithm. In this setting, we first demonstrate that MDP-Expert, an existing algorithm that works well with oblivious adversaries can still apply and achieve a policy regret bound of O (√Tlog(L) + τ2√ T log(|A|)) where L is the size of adversary's pure strategy set and |A| denotes the size of agent's action space. Considering real-world games where the support size of a NE is small, we further propose a new algorithm: MDP-Online Oracle Expert (MDP-OOE), that achieves a policy regret bound of O (√Tlog(L)&lt; + τ 2 √ Tk log(k)) where k depends only on the support size of the NE. MDP-OOE leverages the key benefit of Double Oracle in game theory and thus can solve games with prohibitively large action space. Finally, to better understand the learning dynamics of no-regret methods, under the same setting of no-external regret adversary in OMDPs, we introduce an algorithm that achieves last-round convergence result to a NE. To our best knowledge, this is first work leading to the last iteration result in OMDPs.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {2830–2832},
numpages = {3},
keywords = {non-oblivious adversary, online markov decision processes},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@inproceedings{10.5555/3635637.3663007,
author = {Niu, Tong and Zhang, Weihao and Zhao, Rong},
title = {Solution-oriented Agent-based Models Generation with Verifier-assisted Iterative In-context Learning},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Agent-based models (ABMs) stand as an essential paradigm for proposing and validating hypothetical solutions or policies aimed at addressing challenges posed by complex systems and achieving various objectives. This process demands labor-intensive endeavors and multidisciplinary expertise. Large language models (LLMs) encapsulating cross-domain knowledge and programming proficiency could potentially alleviate the difficulty of this process. However, LLMs excel in handling sequential information, making it challenging for analyzing the intricate interactions and nonlinear dynamics inherent in ABMs. Additionally, due to the lack of self-evaluation capability of LLMs, relying solely on LLMs is insufficient to effectively accomplish this process. In this paper, we present SAGE, a general solution-oriented ABM generation framework designed for automatic modeling and generating solutions for targeted problems. Unlike approaches reliant on expert handcrafting or resource-intensive neural network training, SAGE establishes a verifier-assisted iterative in-context learning process employing large language models (LLMs) to leverages their inherent cross-domain knowledge for tackling intricate demands from diverse domain scenarios. In SAGE, we introduce an semi-structured conceptual representation expliciting the intricate structures of ABMs and an objective representation to guide LLMs in modeling scenarios and proposing hypothetical solutions through in-context learning. To ensure the model executability and solution feasibility, SAGE devises a two-level verifier with chain-of-thought prompting tailored to the complex interactions and non-linear dynamics of ABMs, driving the iterative generation optimization. Moreover, we construct an evaluation dataset of solution-oriented ABMs from open sources. It contains practical models across various domains, completed with scenario descriptions and executable agent-based solutions. Evaluations by various LLMs demonstrate that SAGE leads to an average improvement of 18.7% in modeling quality and 38.1% in solution generation effectiveness. This work advances our understanding and ability in tackling complex real-world challenges across diverse domains through the application of ABM methodologies.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {1473–1481},
numpages = {9},
keywords = {automatic verification and generation, chain-of-thought prompting, iterative in-context learning, large language models, solution-oriented agent-based modeling},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@article{10.1145/3771094,
author = {Li, Chunlin and Zhang, Zihao and Wang, Jiaqi and Yuan, Shaochong and Wang, Zonghe and Chai, Long and Li, Aoyong and Wan, Shaohua},
title = {Improved Multi-Agent Proximal Policy Optimization Algorithm for Resource Allocation with Radar-Perception in UAV-Assisted VEC},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1550-4859},
url = {https://doi.org/10.1145/3771094},
doi = {10.1145/3771094},
abstract = {In congested road environments, the spectrum resources available for Roadside Units (RSUs) are often insufficient to meet the communication needs of a large number of users simultaneously. To address this, Unmanned Aerial Vehicles (UAVs) can be deployed to supplement spectrum resources temporarily. This paper proposes a UAV-assisted Vehicle Edge Computing (VEC) system, integrating UAVs to enhance RSU capabilities in congested scenarios. Traditional spectrum sensing techniques, however, struggle to autonomously monitor vehicular movements and maintain stable spectrum performance. To overcome this, we introduce radar sensing devices into the RSUs to improve perception accuracy and consistency. The integration of radar sensors, while beneficial, creates additional competition for limited system resources. We, therefore, formulate the resource allocation problem considering computation delay, communication rate, and perception data, constrained by spectrum resources, offloading decisions, and time-slot allocations. The problem is modeled as a Markov Decision Process (MDP), and we propose an Improved Multi-Agent Proximal Policy Optimization (IMAPPO) algorithm to optimize resource allocation under these constraints. The experimental results show that compared to baseline algorithms such as A3C, our proposed algorithm reduces the average task processing delay by 15.53%, increases the radar estimation mutual information (MI) by 9.52%, and improves the task completion rate by 4.1%.},
note = {Just Accepted},
journal = {ACM Trans. Sen. Netw.},
month = oct,
keywords = {UAV-assisted VEC, Resource allocation, Markov Decision Process, Improved multi-agent proximal policy optimization algorithm.}
}

@inproceedings{10.5555/3545946.3598823,
author = {Kortvelesy, Ryan and Morad, Steven and Prorok, Amanda},
title = {Permutation-Invariant Set Autoencoders with Fixed-Size Embeddings for Multi-Agent Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {The problem of permutation-invariant learning over set representations is particularly relevant in the field of multi-agent systems---a few potential applications include unsupervised training of aggregation functions in graph neural networks (GNNs), neural cellular automata on graphs, and prediction of scenes with multiple objects. Yet existing approaches to set encoding and decoding tasks present a host of issues, including non-permutation-invariance, fixed-length outputs, reliance on iterative methods, non-deterministic outputs, computationally expensive loss functions, and poor reconstruction accuracy. In this paper we introduce a Permutation-Invariant Set Autoencoder (PISA), which tackles these problems and produces encodings with significantly lower reconstruction error than existing baselines. PISA also provides other desirable properties, including a similarity-preserving latent space, and the ability to insert or remove elements from the encoding. After evaluating PISA against baseline methods, we demonstrate its usefulness in a multi-agent application. Using PISA as a subcomponent, we introduce a novel GNN architecture which serves as a generalised communication scheme, allowing agents to use communication to gain full observability of a system.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1661–1669},
numpages = {9},
keywords = {autoencoder, graph neural network, multi-agent systems, set},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3677052.3698643,
author = {Balcau, Andrei-Bogdan and S\'{a}nchez-Betancourt, Leandro and Sarkadi, Stefan and Ventre, Carmine},
title = {Detecting Collective Liquidity Taking Distributions},
year = {2024},
isbn = {9798400710810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677052.3698643},
doi = {10.1145/3677052.3698643},
abstract = {Tools to identify and characterise the various types of agents in financial markets are essential for both regulators and practitioners. We introduce a methodology that combines agent-based modelling and machine learning to detect collective trading behaviour. Our detection method employs observable market variables to estimate the hidden composition of market participants. More precisely, we use the paths followed by the trend and the volatility of the midprice, and the traded volumes to infer the proportions in which different types of liquidity takers are active in the market (i.e., the market composition). We focus on a market with strategic continuous liquidity provision, populated by three common types of liquidity takers: informed traders, noise traders, and trend followers. We find that the paths of the trend and the volatility carry insufficient information about market composition when employed separately as estimators. However, when these two are non-linearly combined with the volume path, the detector performance increases substantially. Our study contributes to the financial behaviour recognition literature by offering insights into which market factors best describe the collective trading behaviour of liquidity takers.},
booktitle = {Proceedings of the 5th ACM International Conference on AI in Finance},
pages = {504–512},
numpages = {9},
keywords = {agent-based modelling, behaviour detection;, trading},
location = {Brooklyn, NY, USA},
series = {ICAIF '24}
}

@inproceedings{10.1145/3456415.3456462,
author = {Sui, Changhao and Tang, Huaiyu and Gao, Jianyin and Liu, Liang and Wang, Rui and Xu, Hao},
title = {Research on Neighbor Discovery Algorithms Based on Reinforcement Learning with Directional Antennas for Ad Hoc Networks},
year = {2021},
isbn = {9781450389174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3456415.3456462},
doi = {10.1145/3456415.3456462},
abstract = {The use of directional antennas in Ad Hoc networks can improve the performance of networks, but it will make neighbor discovery challenging. The neighbor discovery algorithms based on reinforcement learning improve the efficiency of neighbor discovery by learning the experience of the process according to the reward obtained by the nodes without the prior location information of neighbors. Four reinforcement learning algorithms, including Q-Learning, SARSA, Q(λ), and SARSA(λ), are used to simulate the time to discover all neighbors and the neighbor discovery ratio in different conditions. According to the simulation results, compared with the completely random algorithm, the neighbor discovery algorithm based on Q-Learning has the largest increase in efficiency with the 1-way handshake, especially in the network with high node density and low speed movement of nodes.},
booktitle = {Proceedings of the 2021 9th International Conference on Communications and Broadband Networking},
pages = {285–290},
numpages = {6},
keywords = {Ad Hoc network, Directional antenna, Neighbor discovery, Reinforcement learning},
location = {Shanghai, China},
series = {ICCBN '21}
}

@inproceedings{10.1145/3606305.3606324,
author = {Minarolli, Dorian},
title = {Distributed Task Allocation in Network of Agents Based on Ant Colony Foraging Behavior},
year = {2023},
isbn = {9798400700477},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3606305.3606324},
doi = {10.1145/3606305.3606324},
abstract = {Task allocation is an important problem that is encountered in different distributed computing environments such as grid and cloud computing. In this paper we take a multi-agent learning approach for task allocation in distributed systems composed of network of nodes where each node is connected to a fixed number of neighbor nodes. We developed an approach for distributed task allocation based on principles of ant colony foraging behavior. Ant colony systems as part of more general swarm intelligence systems are notorious for their self-organization, scalability, adaptability and robustness. Because of these properties these systems are very suitable for our distributed task allocation problem. We compared our approach with other heuristic based algorithms. Simulation results on different tasks lengths, load levels, node types and random network graphs show improved performance of our algorithm compared to other approaches. Especially, combining the ant colony based algorithm with an heuristic approach gives the best performance.},
booktitle = {Proceedings of the 24th International Conference on Computer Systems and Technologies},
pages = {59–64},
numpages = {6},
keywords = {ant colony optimization, cloud computing, distributed task allocation, multi-agent learning, task scheduling},
location = {Ruse, Bulgaria},
series = {CompSysTech '23}
}

@inproceedings{10.5555/3545946.3598851,
author = {Chopra, Ayush and Rodr\'{\i}guez, Alexander and Subramanian, Jayakumar and Quera-Bofarull, Arnau and Krishnamurthy, Balaji and Prakash, B. Aditya and Raskar, Ramesh},
title = {Differentiable Agent-based Epidemiology},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Mechanistic simulators are an indispensable tool for epidemiology to explore the behavior of complex, dynamic infections under varying conditions and navigate uncertain environments. Agent-based models (ABMs) are an increasingly popular simulation paradigm that can represent the heterogeneity of contact interactions with granular detail and agency of individual behavior. However, conventional ABM frameworks not differentiable and present challenges in scalability; due to which it is non-trivial to connect them to auxiliary data sources. In this paper, we introduce GradABM: a scalable, differentiable design for agent-based modeling that is amenable to gradient-based learning with automatic differentiation. GradABM can quickly simulate million-size populations in few seconds on commodity hardware, integrate with deep neural networks and ingest heterogeneous data sources. This provides an array of practical benefits for calibration, forecasting, and evaluating policy interventions. We demonstrate the efficacy of GradABM via extensive experiments with real COVID-19 and influenza datasets.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1848–1857},
numpages = {10},
keywords = {automatic differentiation, computational epidemiology, deep neural networks, differentiable agent-based modeling},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@article{10.5555/3648699.3648748,
author = {Agarwal, Mridul and Aggarwal, Vaneet},
title = {Reinforcement learning for joint optimization of multiple rewards},
year = {2023},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {Finding optimal policies which maximize long term rewards of Markov Decision Processes requires the use of dynamic programming and backward induction to solve the Bellman optimality equation. However, many real-world problems require optimization of an objective that is non-linear in cumulative rewards for which dynamic programming cannot be applied directly. For example, in a resource allocation problem, one of the objectives is to maximize long-term fairness among the users. We notice that when an agent aim to optimize some function of the sum of rewards is considered, the problem loses its Markov nature. This paper addresses and formalizes the problem of optimizing a non-linear function of the long term average of rewards. We propose model-based and model-free algorithms to learn the policy, where the model-based policy is shown to achieve a regret of \~{O}(LKDS√A/T) for K objectives combined with a concave L-Lipschitz function. Further, using the fairness in cellular base-station scheduling, and queueing system scheduling as examples, the proposed algorithm is shown to significantly outperform the conventional RL approaches.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {49},
numpages = {41}
}

@article{10.1145/3507906,
author = {Chavhan, Suresh and Gupta, Deepak and Gochhayat, Sarada Prasad and N., Chandana B. and Khanna, Ashish and Shankar, K. and Rodrigues, Joel J. P. C.},
title = {Edge Computing AI-IoT Integrated Energy-efficient Intelligent Transportation System for Smart Cities},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3507906},
doi = {10.1145/3507906},
abstract = {With the advancement of information and communication technologies (ICTs), there has been high-scale utilization of IoT and adoption of AI in the transportation system to improve the utilization of energy, reduce greenhouse gas (GHG) emissions, increase quality of services, and provide many extensive benefits to the commuters and transportation authorities. In this article, we propose a novel edge-based AI-IoT integrated energy-efficient intelligent transport system for smart cities by using a distributed multi-agent system. An urban area is divided into multiple regions, and each region is sub-divided into a finite number of zones. At each zone an optimal number of RSUs are installed along with the edge computing devices. The MAS deployed at each RSU collects a huge volume of data from the various sensors, devices, and infrastructures. The edge computing device uses the collected raw data from the MAS to process, analyze, and predict. The predicted information will be shared with the neighborhood RSUs, vehicles, and cloud by using MAS with the help of IoT. The predicted information can be used by freight vehicles to maintain smooth and steady movement, which results in reduction in GHG emissions and energy consumption, and finally improves the freight vehicles’ mileage by reducing traffic congestion in the urban areas. We have exhaustively carried out the simulation results and demonstrated the effectiveness of the proposed system.},
journal = {ACM Trans. Internet Technol.},
month = nov,
articleno = {106},
numpages = {18},
keywords = {Artificial intelligence, Internet of Things, ITS, MAS, edge computing, cloud computing, cyber physical systems}
}

@article{10.1145/3697834,
author = {Esmaeili, Ahmad and Rayz, Julia and Matson, Eric},
title = {Hybrid Algorithm Selection and Hyperparameter Tuning on Distribute Machine Learning Resources: Hierarchical Agent-based Approach},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3697834},
doi = {10.1145/3697834},
abstract = {Algorithm&nbsp;selection and hyperparameter tuning are critical steps in both academic and applied machine learning (ML). These steps are becoming increasingly delicate due to the extensive rise in the number, diversity, and distributed nature of ML resources. Multi-agent systems, when applied to the design of ML platforms, bring about several distinctive characteristics, such as scalability, flexibility, and robustness, just to name a few. This article proposes a fully automatic and collaborative agent-based mechanism for selecting distributed ML algorithms and simultaneously tuning their hyperparameters. Our method builds upon an existing agent-based hierarchical ML platform and augments its query structure to support the aforementioned functionalities without being limited to specific learning, selection, and tuning mechanisms. We have conducted theoretical assessments, formal verification, and analytical study to demonstrate the correctness, resource utilization, and computational efficiency of our technique. According to the results, our solution is algorithmically correct and exhibits linear time and space complexity in relation to the size of available resources. To further verify its correctness and demonstrate its effectiveness and flexibility across a range of algorithmic options and datasets, the article also presents a series of empirical results on a system composed of 24 algorithms and 9 datasets. The findings not only highlight the efficiency and scalability of the proposed approach, but also show its flexibility and openness to responding to the dynamic and distributed ML ecosystem.},
journal = {ACM Trans. Internet Technol.},
month = nov,
articleno = {31},
numpages = {30},
keywords = {Distributed machine learning, hyperparameter tuning, algorithm selection}
}

@inproceedings{10.5555/3635637.3662928,
author = {Guo, Hao and Wang, Zhen and Xing, Junliang and Tao, Pin and Shi, Yuanchun},
title = {Cooperation and Coordination in Heterogeneous Populations with Interaction Diversity},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Cooperation, a prosocial behavior enhancing collective rewards in multi-agent games, intricately intertwines with coordination. This study explores how interaction diversity and zero-sum gifting influence cooperation and coordination in heterogeneous populations, where agents engage in threshold public goods games with multiple equilibria. Our model accommodates two sources of inequality: variations in agents' capabilities to provide public goods and differences in the rewards they receive upon successful public good provision. In the absence of gifting, we demonstrate the inevitability of intermediate interaction intensity in fostering global cooperation, elucidating conditions for co-dominance, coexistence, and the polarized state of cooperation. While gifting introduces reciprocity opportunities, our findings highlight the importance of maintaining moderate levels of gifting, as excessive gifting can paradoxically undermine global cooperation. This research contributes valuable insights into the emergence of cooperation and coordination dynamics.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {752–760},
numpages = {9},
keywords = {cooperation, coordination, evolutionary game theory, heterogeneous populations, public goods game},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@article{10.1145/3590154,
author = {Tong, Junbo and Shi, Daming and Liu, Yi and Fan, Wenhui},
title = {GLDAP: Global Dynamic Action Persistence Adaptation for Deep Reinforcement Learning},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/3590154},
doi = {10.1145/3590154},
abstract = {In the implementation of deep reinforcement learning (DRL), action persistence strategies are often adopted so agents maintain their actions for a fixed or variable number of steps. The choice of the persistent duration for agent actions usually has notable effects on the performance of reinforcement learning algorithms. Aiming at the research gap of global dynamic optimal action persistence and its application in multi-agent systems, we propose a novel framework: global dynamic action persistence (GLDAP), which achieves global action persistence adaptation for deep reinforcement learning. We introduce a closed-loop method that is used to learn the estimated value and the corresponding policy of each candidate action persistence. Our experiment shows that GLDAP achieves an average of 2.5%~90.7% performance improvement and 3~20 times higher sampling efficiency over several baselines across various single-agent and multi-agent domains. We also validate the ability of GLDAP to determine the optimal action persistence through multiple experiments.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = may,
articleno = {7},
numpages = {18},
keywords = {Deep reinforcement learning, action persistence, temporal abstraction}
}

@inproceedings{10.5555/3635637.3662897,
author = {Do Carmo Alves, Matheus Aparecido and Varma, Amokh and Elkhatib, Yehia and Soriano Marcolino, Leandro},
title = {It Is Among Us: Identifying Adversaries in Ad-hoc Domains using Q-valued Bayesian Estimations},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Ad-hoc teamwork models are crucial for solving distributed tasks in environments with unknown teammates. In order to improve performance, agents may collaborate in the same environment, trusting each other and exchanging information. However, what happens if there is an impostor among the team? In this paper, we present BAE, a novel and efficient framework for online planning and estimation within ad-hoc teamwork domains where there is an adversarial agent disguised as a teammate. Our approach considers the identification of the impostor through a process we term "Q-valued Bayesian Estimation''. BAE can identify the adversary at the same time the agent performs ad-hoc estimation in order to improve coordination. Our results show that BAE has superior accuracy and faster reasoning capabilities in comparison to the state-of-the-art.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {472–480},
numpages = {9},
keywords = {ad-hoc teamwork, adversarial detection, online planning},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@inproceedings{10.1145/3677052.3698599,
author = {Dwarakanath, Kshama and Dong, Jialin and Vyetrenko, Svitlana},
title = {Tax Credits and Household Behavior: The Roles of Myopic Decision-Making and Liquidity in a Simulated Economy},
year = {2024},
isbn = {9798400710810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677052.3698599},
doi = {10.1145/3677052.3698599},
abstract = {There has been a growing interest in multi-agent simulators in the domain of economic modeling. However, contemporary research often involves developing reinforcement learning (RL) based models that focus solely on a single type of agents, such as households, firms, or the government. Such an approach overlooks the adaptation of interacting agents thereby failing to capture the complexity of real-world economic systems. In this work, we consider a multi-agent simulator comprised of RL agents of numerous types, including heterogeneous households, firm, central bank and government. In particular, we focus on the crucial role of the government in distributing tax credits to households. We conduct two broad categories of comprehensive experiments dealing with the impact of tax credits on 1) households with varied degrees of myopia (short-sightedness in spending and saving decisions), and 2) households with diverse liquidity profiles. The first category of experiments examines the impact of the frequency of tax credits (e.g. annual vs quarterly) on consumption patterns of myopic households. The second category of experiments focuses on the impact of varying tax credit distribution strategies on households with differing liquidities. We validate our simulation model by reproducing trends observed in real households upon receipt of unforeseen, uniform tax credits, as documented in a JPMorgan Chase report. Based on the results of the latter, we propose an innovative tax credit distribution strategy for the government to reduce inequality among households. We demonstrate the efficacy of this strategy in improving social welfare in our simulation results.},
booktitle = {Proceedings of the 5th ACM International Conference on AI in Finance},
pages = {168–176},
numpages = {9},
location = {Brooklyn, NY, USA},
series = {ICAIF '24}
}

@inproceedings{10.1145/3503823.3503905,
author = {Sidiropoulos, George and Kiourt, Chairi and Sevetlidis, Vasileios and Pavlidis, George},
title = {Shaping the Behavior of Reinforcement Learning Agents},
year = {2022},
isbn = {9781450395557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503823.3503905},
doi = {10.1145/3503823.3503905},
abstract = {With the advent of machine learning and agent-based approaches, behavior-shaping in environments composed of several autonomous entities has become a popular and active research field for the development of unique realistic behaviors. Realistic simulations have been particularly studied in the fields of crowd management, swarm behavior analysis and civilization simulation. In this study, we present a new dynamic rewarding approach for shaping the behavior of reinforcement learning agents in mixed (cooperative and competitive) multi-agent environments. The evaluation of the proposed rewarding approach is tested in a developed 3D environment of two groups of ancient Greek warriors fighting inside an octagonal arena, testing different agent behaviors in various scenarios. Interestingly, the results reveal that the trained agents’ behaviors vary based on the situations and the constraints of the environment, resembling realistic behavior variations.},
booktitle = {Proceedings of the 25th Pan-Hellenic Conference on Informatics},
pages = {448–453},
numpages = {6},
keywords = {behavior-shaping, multi-agent environments, reinforcement learning, self-playing agents},
location = {Volos, Greece},
series = {PCI '21}
}

@inproceedings{10.1145/3651890.3672231,
author = {Gui, Fei and Wang, Songtao and Li, Dan and Chen, Li and Gao, Kaihui and Min, Congcong and Wang, Yi},
title = {RedTE: Mitigating Subsecond Traffic Bursts with Real-time and Distributed Traffic Engineering},
year = {2024},
isbn = {9798400706141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3651890.3672231},
doi = {10.1145/3651890.3672231},
abstract = {Internet traffic bursts usually happen within a second, thus conventional burst mitigation methods ignore the potential of Traffic Engineering (TE). However, our experiments indicate that a TE system, with a sub-second control loop latency, can effectively alleviate burst-induced congestion. TE-based methods can leverage network-wide tunnel-level information to make globally informed decisions (e.g., balancing traffic bursts among multiple paths). Our insight in reducing control loop latency is to let each router make local TE decisions, but this introduces the key challenge of minimizing performance loss compared to centralized TE systems.In this paper, we present RedTE, a novel distributed TE system with a control loop latency of &lt; 100ms, while achieving performance comparable to centralized TE systems. RedTE's innovation is the modeling of TE as a distributed cooperative multi-agent problem, and we design a novel multi-agent deep reinforcement learning algorithm to solve it, which enables each agent to make globally informed decisions solely based on local information. We implement real RedTE routers and deploy them on a WAN spanning six city datacenters. Evaluation reveals notable improvements compared to existing solutions: &lt; 100ms of control loop latency, a 37.4% reduction in maximum link utilization, and a 78.9% reduction in average queue length.},
booktitle = {Proceedings of the ACM SIGCOMM 2024 Conference},
pages = {71–85},
numpages = {15},
keywords = {traffic engineering, network optimization, machine learning},
location = {Sydney, NSW, Australia},
series = {ACM SIGCOMM '24}
}

@article{10.1145/3583886,
author = {Rudrauf, D. and Sergeant-Perhtuis, G. and Tisserand, Y. and Monnor, T. and De Gevigney, V. and Belli, O.},
title = {Combining the Projective Consciousness Model and Virtual Humans for Immersive Psychological Research: A Proof-of-concept Simulating a ToM Assessment},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/3583886},
doi = {10.1145/3583886},
abstract = {Relating explicit psychological mechanisms and observable behaviours is a central aim of psychological and behavioural science. One of the challenges is to understand and model the role of consciousness and, in particular, its subjective perspective as an internal level of representation (including for social cognition) in the governance of behaviour. Toward this aim, we implemented the principles of the Projective Consciousness Model (PCM) into artificial agents embodied as virtual humans, extending a previous implementation of the model. Our goal was to offer a proof-of-concept, based purely on simulations, as a basis for a future methodological framework. Its overarching aim is to be able to assess hidden psychological parameters in human participants, based on a model relevant to consciousness research, in the context of experiments in virtual reality. As an illustration of the approach, we focused on simulating the role of Theory of Mind (ToM) in the choice of strategic behaviours of approach and avoidance to optimise the satisfaction of agents’ preferences. We designed a main experiment in a virtual environment that could be used with real humans, allowing us to classify behaviours as a function of order of ToM, up to the second order. We show that agents using the PCM demonstrated expected behaviours with consistent parameters of ToM in this experiment. We also show that the agents could be used to estimate correctly each other’s order of ToM. Furthermore, in a supplementary experiment, we demonstrated how the agents could simultaneously estimate order of ToM and preferences attributed to others to optimize behavioural outcomes. Future studies will empirically assess and fine tune the framework with real humans in virtual reality experiments.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = may,
articleno = {8},
numpages = {31},
keywords = {Consciousness, causal role, projective geometry, active inference, behavioural science, virtual humans, navigation, emotion}
}

@inproceedings{10.1145/3511808.3557112,
author = {Wang, Yiheng and Jin, Hexi and Zheng, Guanjie},
title = {CTRL: Cooperative Traffic Tolling via Reinforcement Learning},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557112},
doi = {10.1145/3511808.3557112},
abstract = {People have been working long to tackle the traffic congestion problem. Among the different measures, traffic tolling has been recognized as an effective way to mitigate citywide congestion. However, traditional tolling methods can not deal with the dynamic traffic flow in cities. Meanwhile, thanks to the development of traffic sensing technology, how to set appropriate dynamic tolling according to real time traffic observations has attracted research attention in recent years.In this paper, we put the dynamic tolling problem in a reinforcement learning setting and try to tackle the three key challenges of complex state representation, pricing action credit assignment, and route price relative competition. We propose a soft actor-critic method with (1) a route-level state attention, (2) an interpretable and provable reward design, and (3) a competition-aware Q attention. Extensive experiments on real datasets have shown the superior performance of our proposed method. In addition, interesting analysis on pricing actions and vehicle routes have demonstrated why the proposed method can outperform baselines.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {3545–3554},
numpages = {10},
keywords = {dynamic tolling, reinforcement learning},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.5555/3545946.3598626,
author = {Gautier, Anna and Rigter, Marc and Lacerda, Bruno and Hawes, Nick and Wooldridge, Michael},
title = {Risk-Constrained Planning for Multi-Agent Systems with Shared Resources},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Planning under uncertainty requires complex reasoning about future events, and this complexity increases with the addition of multiple agents. One problem faced when considering multi-agent systems under uncertainty is the handling of shared resources. Adding a resource constraint limits the actions that agents can take, forcing collaborative decision making on who gets to use what resources. Prior work has considered different formulations, such as satisfying a resource constraint in expectation or ensuring that a resource constraint is met some percent of the time. However, these formulations of constrained planning ignore important distributional information about resource usage. Namely, they do not consider how bad the worst cases can get. In this paper, we formulate a risk-constrained shared resource problem and aim to limit the risk of excessive use of such resources. We focus on optimising for reward while constraining the Conditional Value-at-Risk (CVaR) of the shared resource. While CVaR is well studied in the single-agent setting, we consider the challenges that arise from the state and action space explosion in the multi-agent setting. In particular, we exploit risk contributions, a measure introduced in finance research which quantifies how much individual agents affect the joint risk. We present an algorithm that uses risk contributions to iteratively update single-agent policies until the joint risk constraint is satisfied. We evaluate our algorithm on two synthetic domains.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {113–121},
numpages = {9},
keywords = {conditional value-at-risk, multi-agent systems, planning under uncertainty, risk contribution, risk-aware planning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3331453.3360975,
author = {Zhou, Wenhong and Chen, Yiting and Li, Jie},
title = {Competitive Evolution Multi-Agent Deep Reinforcement Learning},
year = {2019},
isbn = {9781450362948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331453.3360975},
doi = {10.1145/3331453.3360975},
abstract = {As an effective method to solve the optimal policy in multi-agent systems, multi-agent deep reinforcement learning (MADRL) has achieved impressive results in many applications. However, the training of a deep neural network used in MADRL is time-consuming and laborious. In order to improve the training efficiency and accelerate the training speed of the neural network, this paper proposed a sharing learning framework to support the effectively training of multi-agent state-action value function neural network through making full use of sharing diverse experience from different agents to train a single shared network. Besides, a novel learning algorithm combined with competition and evolution idea was developed to accelerate the training process and improve the performance of the trained neural network. Finally, the experiment results prove the effectiveness of the proposed framework and the fact that the proposed algorithm with an appropriate update rate can indeed accelerate the training process and improve the performance of the trained network by providing a well-matched opponent.},
booktitle = {Proceedings of the 3rd International Conference on Computer Science and Application Engineering},
articleno = {24},
numpages = {6},
keywords = {Competitive evolution, Cooperation, Multi-agent deep reinforcement learning, Sharing learning framework},
location = {Sanya, China},
series = {CSAE '19}
}

@inproceedings{10.5555/3535850.3535987,
author = {Street, Charlie and Lacerda, Bruno and Staniaszek, Michal and M\"{u}hlig, Manuel and Hawes, Nick},
title = {Context-Aware Modelling for Multi-Robot Systems Under Uncertainty},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Formal models of multi-robot behaviour are fundamental to planning, simulation, and model checking techniques. However, existing models are invalidated by strong assumptions that fail to capture execution-time multi-robot behaviour, such as simplistic duration models or synchronisation constraints. In this paper we propose a novel multi-robot Markov automaton formulation which models asynchronous multi-robot execution in continuous time. Robot dynamics are captured using phase-type distributions over action durations. Moreover, we explicitly model the effects of robot interactions, as they are a key factor for the duration of action execution. We also present a scalable discrete-event simulator which yields realistic statistics over execution-time robot behaviour by sampling through the Markov automaton. We validate our model and simulator against a Gazebo simulation in a range of multi-robot navigation scenarios, demonstrating that our model accurately captures high-level multi-robot behaviour.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1228–1236},
numpages = {9},
keywords = {Markov automata, discrete-event simulation, multi-robot systems},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@article{10.1145/3406678.3406684,
author = {Kelley, Dean},
title = {Technical Report Column},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0163-5700},
url = {https://doi.org/10.1145/3406678.3406684},
doi = {10.1145/3406678.3406684},
abstract = {Welcome to the Technical Reports Column. If your institution publishes technical reports that you'd like to have included here, please contact me at the email address above.},
journal = {SIGACT News},
month = jun,
pages = {15–26},
numpages = {12}
}

@inproceedings{10.1145/3408308.3427611,
author = {Feng, Yuzhou and Li, Qi and Chen, Dong and Rangaswami, Raju},
title = {SolarTrader: Enabling Distributed Solar Energy Trading in Residential Virtual Power Plants},
year = {2020},
isbn = {9781450380614},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408308.3427611},
doi = {10.1145/3408308.3427611},
abstract = {Distributed solar energy resources (DSERs) in smart grid systems are rapidly increasing due to the steep decline in solar module prices. This DSER penetration has prompted utilities to balance the real-time supply and demand of electricity proactively. A direct consequence of this is virtual power plants (VPPs) that enable solar generated energy trading to mitigate the impact of the intermittent DSERs while also benefiting from distributed generation for more reliable and profitable grid management. However, existing energy trading approaches in residential VPPs do not actually allow DSER users to trade their surplus solar energy independently and concurrently to maximize benefit potential; they typically require a trusted third-party to play the role of an online middleman. Furthermore, due to a lack of fair trading algorithms, these approaches do not necessarily result in "fair" solar energy saving among all the VPP users in the long term.We propose Sola/Trader, a new solar energy trading system that enables unsupervised, distributed, and long term fair solar energy trading in residential VPPs. In essence, SolarTrader leverages a new multi-agent deep reinforcement learning approach that enables peer-to-peer solar energy trading among different DSERs to ensure that both the DSER users and the VPPs maximize benefit. We implement SolarTrader and evaluate it using both synthetic and real smart meter data from 4 U.S. residential VPP communities that are comprised of ~229 residential DSERs in total. Our results show that SolarTrader can reduce the aggregated VPP energy consumption by 83.8% when compared against a non-trading approach. Furthermore, SolarTrader achieves a ~105% average saving in VPP residents' monthly electricity cost. We also find that SolarTrader-enabled VPPs can achieve a fairness of 0.05, as measured by the Gini Coefficient, a level equivalent to that achieved by the fairness-maximizing Round-Robin approach.},
booktitle = {Proceedings of the 7th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {60–69},
numpages = {10},
keywords = {Deep Learning, ML, Reinforcement Learning, Solar Energy Trading},
location = {Virtual Event, Japan},
series = {BuildSys '20}
}

@inproceedings{10.1145/3128128.3128143,
author = {Lakhal, Said and Guennoun, Zouhair},
title = {Using Markov decision process in cognitive radio networks towards the optimal reward},
year = {2017},
isbn = {9781450352819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3128128.3128143},
doi = {10.1145/3128128.3128143},
abstract = {The Learning is an indispensable phase in the cognition cycle of cognitive radio network. It corresponds between the executed actions and the estimated rewards. Based on this phase, the agent learns from past experiences to improve his actions in the next interventions. In the literature, there are several methods that treat the artificial learning. Among them, we cite the reinforcement learning that look for the optimal policy, for ensuring the maximum reward.The present work exposes an approach, based on a model of reinforcement learning, namely Markov decision process, to maximize the sum of transfer rates of all secondary users. Such conception defines all notions relative to an environment with finite set of states, including: the agent, all states, the allowed actions with a given state, the obtained reward after the execution of an action and the optimal policy. After the implementation, we remark a correlation between the started policy and the optimal policy, and we improve the performances by referring to a previous work.},
booktitle = {Proceedings of the 2017 International Conference on Smart Digital Environment},
pages = {93–100},
numpages = {8},
keywords = {Markov decision process, agent, cognitive radio networks, optimal policy, reinforcement learning},
location = {Rabat, Morocco},
series = {ICSDE '17}
}

@inproceedings{10.1145/3630138.3630419,
author = {Liu, Shengfeng and Dai, Feipeng and Cui, Haixia},
title = {Joint Resource Allocation and Interference Management for Vehicular Networks},
year = {2024},
isbn = {9781450399951},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630138.3630419},
doi = {10.1145/3630138.3630419},
abstract = {Vehicular communication is a promising technology for the intelligent transportation system. With the growth of data requirement, the rapid evolvement of vehicular communication has been hindered by the constraints imposed by limited network resources and the resource sharing is benefit for this issue. Vehicle-to-vehicle (V2V) pairs, by sharing the spectrum of vehicle-to-infrastructure (V2I) links, can improve the network resource utilization significantly. However, they also bring out lots of interference to the V2I links, with the aim of enhancing the user experience quality and driving safety of the vehicular networks and then there needs suitable methods to solve it. In our study, we put forward an algorithm that combines one-to-many resource allocation and interference management to improve the spectrum efficiency for vehicular networks. Considering complex network environment with high mobility of vehicular networks, it is difficult to accurately model such requirements mathematically. We resolve the joint optimization problem with the assistance of deep reinforcement learning (DRL). Simulation results show that our proposed method exhibits significant advantages over the baseline algorithm in terms of network capacity and system interference. These findings indicate our proposed method can effectively reduce the cumulative interference between vehicular users and increase the total capacity of vehicular networks.},
booktitle = {Proceedings of the 2023 International Conference on Power, Communication, Computing and Networking Technologies},
articleno = {7},
numpages = {5},
keywords = {Vehicular networks, interference management, reinforcement learning, resource allocation},
location = {Wuhan, China},
series = {PCCNT '23}
}

@article{10.1145/3625236,
author = {Qu, Ao and Tang, Yihong and Ma, Wei},
title = {Adversarial Attacks on Deep Reinforcement Learning-based Traffic Signal Control Systems with Colluding Vehicles},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3625236},
doi = {10.1145/3625236},
abstract = {The rapid advancements of Internet of Things (IoT) and Artificial Intelligence (AI) have catalyzed the development of adaptive traffic control systems (ATCS) for smart cities. In particular, deep reinforcement learning (DRL) models produce state-of-the-art performance and have great potential for practical applications. In the existing DRL-based ATCS, the controlled signals collect traffic state information from nearby vehicles, and then optimal actions (e.g., switching phases) can be determined based on the collected information. The DRL models fully “trust” that vehicles are sending the true information to the traffic signals, making the ATCS vulnerable to adversarial attacks with falsified information. In view of this, this article first time formulates a novel task in which a group of vehicles can cooperatively send falsified information to “cheat” DRL-based ATCS in order to save their total travel time. To solve the proposed task, we develop CollusionVeh, a generic and effective vehicle-colluding framework composed of a road situation encoder, a vehicle interpreter, and a communication mechanism. We employ our framework to attack established DRL-based ATCS and demonstrate that the total travel time for the colluding vehicles can be significantly reduced with a reasonable number of learning episodes, and the colluding effect will decrease if the number of colluding vehicles increases. Additionally, insights and suggestions for the real-world deployment of DRL-based ATCS are provided. The research outcomes could help improve the reliability and robustness of the ATCS and better protect the smart mobility systems.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {113},
numpages = {22},
keywords = {Adversarial attacks, adaptive traffic control system, reinforcement learning, connected vehicles}
}

