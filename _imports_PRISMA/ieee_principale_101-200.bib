@ARTICLE{10942368,
  author={Choi, Youngrok and Di Marco, Piergiuseppe and Park, Pangun},
  journal={IEEE Access}, 
  title={Communication-Aware Graph Neural Network for Multi-Agent Reinforcement Learning}, 
  year={2025},
  volume={13},
  number={},
  pages={55832-55840},
  abstract={Multi-agent reinforcement learning (MARL) requires effective communication strategies to solve complex control tasks over uncertain communication channels. This paper explores a communication-aware graph neural network (GNN) approach for MARL, where the interactions between agents are modeled as a dynamic directed graph that explicitly considers time-varying lossy links. We integrate communication aspects into MARL by combining the self-attention-based coordination graph and a graph convolution with zero-input compensation to migrate the information losses over multi-hop networks. We evaluate our approach on two challenging tasks: the predator-prey and the coverage problems. We show 1) the operational benefits of communication-aware GNN with sensing range, node density, and task complexity, 2) the robust performance of the proposed scheme to support the graph convolution over various ranges of packet loss probabilities of links, and 3) the effectiveness of the residual connection of the GNN model on the overall performance and the communication architecture.},
  keywords={Convolution;Graph neural networks;Reinforcement learning;Multi-agent systems;Message passing;Vectors;Packet loss;Nickel;Hands;Aggregates;Coordination;graph attention;graph neural network;lossy link;multi-agent system},
  doi={10.1109/ACCESS.2025.3554736},
  ISSN={2169-3536},
  month={},}@ARTICLE{10197196,
  author={Zou, Yifei and Jin, Zongjing and Zheng, Yanwei and Yu, Dongxiao and Lan, Tian},
  journal={Tsinghua Science and Technology}, 
  title={Optimized Consensus for Blockchain in Internet of Things Networks via Reinforcement Learning}, 
  year={2023},
  volume={28},
  number={6},
  pages={1009-1022},
  abstract={Most blockchain systems currently adopt resource-consuming protocols to achieve consensus between miners; for example, the Proof-of-Work (PoW) and Practical Byzantine Fault Tolerant (PBFT) schemes, which have a high consumption of computing/communication resources and usually require reliable communications with bounded delay. However, these protocols may be unsuitable for Internet of Things (IoT) networks because the IoT devices are usually lightweight, battery-operated, and deployed in an unreliable wireless environment. Therefore, this paper studies an efficient consensus protocol for blockchain in IoT networks via reinforcement learning. Specifically, the consensus protocol in this work is designed on the basis of the Proof-of-Communication (PoC) scheme directly in a single-hop wireless network with unreliable communications. A distributed MultiAgent Reinforcement Learning (MARL) algorithm is proposed to improve the efficiency and fairness of consensus for miners in the blockchain system. In this algorithm, each agent uses a matrix to depict the efficiency and fairness of the recent consensus and tunes its actions and rewards carefully in an actor-critic framework to seek effective performance. Empirical results from the simulation show that the fairness of consensus in the proposed algorithm is guaranteed, and the efficiency nearly reaches a centralized optimal solution.},
  keywords={Fault tolerance;Wireless networks;Fault tolerant systems;Reinforcement learning;Proof of Work;Consensus protocol;Delays;consensus in blockchain;Proof-of-Communication (PoC);MultiAgent Reinforcement Learning (MARL);Internet of Things (IoT) networks},
  doi={10.26599/TST.2022.9010045},
  ISSN={1007-0214},
  month={December},}@INPROCEEDINGS{8761441,
  author={Li, Xinge and Hu, Xiaoya and Li, Wei and Hu, Hui},
  booktitle={ICC 2019 - 2019 IEEE International Conference on Communications (ICC)}, 
  title={A Multi-Agent Reinforcement Learning Routing Protocol for Underwater Optical Sensor Networks}, 
  year={2019},
  volume={},
  number={},
  pages={1-7},
  abstract={Much attention has been paid to underwater optical wireless sensor networks with the characteristics of high transmission rate and low delay for high-bandwidth underwater applications. However, several issues may take place and hinder the routing of underwater optical communication nodes due to the highly dynamic topology caused by the ocean current movement. On the purpose of addressing the problem and enhancing the robustness of dynamic network, in this paper, we propose a novel routing protocol, based on multi-agent reinforcement learning (MARL) for underwater optical sensor networks. The network is firstly modeled as a multi-agent system and the protocol based on reinforcement learning algorithm is designed to realize dynamic route selection by information interacting between adjacent nodes and maximize the network lifetime. The simulation results demonstrate that MARL has lower energy consumption and higher delivery ratio (about 95&#x0025;) in a dynamic topology than the existing Q-learning, QDTR and AODV routing protocols.},
  keywords={Routing;Routing protocols;Wireless sensor networks;Delays;Topology;Reinforcement learning;Optical sensors},
  doi={10.1109/ICC.2019.8761441},
  ISSN={1938-1883},
  month={May},}@INPROCEEDINGS{8616604,
  author={Aotani, Takumi and Kobayashi, Taisuke and Sugimoto, Kenji},
  booktitle={2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={Bottom-up Multi-agent Reinforcement Learning for Selective Cooperation}, 
  year={2018},
  volume={},
  number={},
  pages={3590-3595},
  abstract={Applications of multi-agent system like cooperative transport are found in various domains of real world. Due to the complexity inherent in multi-agent system, however, handling with preprogramming is difficult. Multi-agent reinforcement learning (MARL), which is a framework to make multiple agents in the same environment learn their policies simultaneously using reinforcement learning, is receiving attention. In the conventional MARL, although decentralization is essential for feasible learning, rewards for the agents have been allocated from a centralized system in the environment. Instead of such "top-down" MARL, to achieve the completely distributed autonomous systems, we tackle a new paradigm named "bottom-up" MARL, where the agents get their own rewards. The bottom-up MARL requires to share the respective rewards for emerging orderly group behaviors, which cannot be acquired merely by maximizing the mean of them. We therefore propose the architecture that has three components: estimating rewards of other agents; selecting rewards to reinforce from the correlation, and; promoting the exploration to find unknown correlation. The proposed architecture is verified that every element is essential by numerical simulation performed in stages. A similar task is also accomplished in dynamical simulation under the same conditions as the actual robots.},
  keywords={Task analysis;Reinforcement learning;Correlation;Uncertainty;Multi-agent systems;Autonomous systems;Robots;Distributed autonomous system;Reinforcement leaning;Stochastic neural network},
  doi={10.1109/SMC.2018.00607},
  ISSN={2577-1655},
  month={Oct},}@ARTICLE{9946429,
  author={Li, Li and Li, Wei and Chen, Xiaonan and Wang, Jun and Peng, Qihang},
  journal={IEEE Transactions on Vehicular Technology}, 
  title={Learning Based Multi-Channel Access in Vehicular Networks With Illegal Operators and Users}, 
  year={2023},
  volume={72},
  number={3},
  pages={4097-4102},
  abstract={This paper investigates the multi-channel access (MCA) problem for vehicular networks in presence of illegal operators and users. Due to the limited spectrum resource, vehicle-to-vehicle (V2V) links need to reuse the frequency spectrum allocated for vehicle-to-infrastructure (V2I) links in networks. For high-mobility environments of vehicular networks, the fast-varying channel conditions will inevitably lead to the significant uncertainty of the acquired channel state information (CSI). Hence, the traditional centralized MCA methods, which rely on the CSI, could not be performed in a timely manner. We formulate the MCA of the vehicle users as a distributed optimization problem, by maximizing the sum capacity of V2I links while guaranteeing the reliability of V2V links as well as reducing the cost due to frequency hopping. A Multi-Agent Deep Deterministic Policy Gradient (MADDPG) framework is introduced to tackle this problem, where each vehicle user, connecting with either a V2I or V2V link, acts as a learning agent to make spectrum access decisions rapidly and locally to meet the latency requirement. In order to adapt to the dynamic environment, we further propose a delayed interaction aided MADDPG algorithm, which enables online training for the distributed MCA problem. Moreover, to guarantee the proposed algorithm to converge stably, we optimize the selecting weights of experience traces and propose a Batch Prioritized Experience Replay (BPER) strategy to ensure that the high priority traces can be learned timely. Compared with baselines, simulation results showcase remarkable performance gain of our proposed algorithm.},
  keywords={Jamming;Manganese;Vehicle-to-infrastructure;Vehicular ad hoc networks;Resource management;Vehicle dynamics;Safety;Vehicular networks;illegal operator;distributed spectrum access;multi-agent reinforcement learning},
  doi={10.1109/TVT.2022.3221485},
  ISSN={1939-9359},
  month={March},}@INPROCEEDINGS{10650899,
  author={Chen, Hao and Zhang, Bin and Fan, Guoliang},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={SGCD: Subgroup Contribution Decomposition for Multi-Agent Reinforcement Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Cooperative multi-agent reinforcement learning (MARL) tasks rely on the efficient coordination among agents, working collectively as a team to address diverse challenges. However, considering the team as a cohesive entity introduces a flat structure to cooperation. In contrast, grouping serves as a method to tackle the issue by decomposing the team, thereby providing a more compact representation of the team’s structure. While grouping has been proven effective, numerous grouping methods are limited to specific composition structures and struggle to introduce diverse group patterns into the framework. In this paper, we propose SGCD, a subgroup contribution decomposition method that incorporates the idea of subgroups and inner subgroups, leveraging the Shapley Value to distribute contributions. This approach facilitates the decomposition of contributions from subgroups to the collective onto individual agents, enabling the high-level network to maintain consistency across various grouping patterns, thereby fostering continued cooperation among agents. Notably, our decomposition method is not confined to a specific team decomposition, making it adaptable to different grouping structures. The effectiveness of SGCD is demonstrated through experiments conducted in the Google Research Football (GRF) and StarCraft Multi-Agent Challenge (SMAC) environments.},
  keywords={Training;Neural networks;Reinforcement learning;Internet;Sports;Multi-agent system;Deep reinforcement learning},
  doi={10.1109/IJCNN60899.2024.10650899},
  ISSN={2161-4407},
  month={June},}@ARTICLE{9601214,
  author={Lee, Joash and Niyato, Dusit and Guan, Yong Liang and Kim, Dong In},
  journal={IEEE Transactions on Vehicular Technology}, 
  title={Learning to Schedule Joint Radar-Communication With Deep Multi-Agent Reinforcement Learning}, 
  year={2022},
  volume={71},
  number={1},
  pages={406-422},
  abstract={Radar detection and communication are two essential sub-tasks for the operation of next-generation autonomous vehicles (AVs). The forthcoming proliferation of faster 5G networks utilizing mmWave has raised concerns on interference with automotive radar sensors, which has led to a body of research on Joint Radar-Communication (JRC). This paper considers the problem of time-sharing for JRC, with the additional simultaneous objective of minimizing the average age of information (AoI) transmitted by a JRC-equipped AV. We first formulate the problem as a Markov Decision Process (MDP). We then propose a more general multi-agent system, with an appropriate medium access control (MAC) protocol, which is formulated as a partially observed Markov game (POMG). To solve the POMG, we propose a multi-agent extension of the Proximal Policy Optimization (PPO) algorithm, along with algorithmic features to enhance learning from raw observations. Simulations are run with a range of environmental parameters to mimic variations in real-world operation. The results show that the chosen deep reinforcement learning methods allow the agents to obtain strong performance with minimal a priori knowledge about the environment.},
  keywords={Sensors;Radar;Automotive engineering;Accidents;Sensor systems;Reinforcement learning;Cameras;Reinforcement learning;deep learning;task scheduling;vehicle safety;communication},
  doi={10.1109/TVT.2021.3124810},
  ISSN={1939-9359},
  month={Jan},}@INPROCEEDINGS{11149745,
  author={Wang, Jingyi and Yang, Yi and Wang, Keping},
  booktitle={2025 40th Youth Academic Annual Conference of Chinese Association of Automation (YAC)}, 
  title={MATD3-based Multi-Agent Hydraulic Support Straightening Control with Cooperative Game Theory}, 
  year={2025},
  volume={},
  number={},
  pages={565-569},
  abstract={Hydraulic Support Straightening control in intelligent coal mining systems faces significant challenges due to complex geological conditions and system uncertainties. This paper proposes a novel cooperative multi-agent reinforcement learning framework that integrates the MATD3 algorithm with cooperative game theory principles. The framework introduces two key innovations: (1) a Shapley value-based credit assignment mechanism within MATD3 to enhance coordination fairness, (2) a neural network-based coalition formation strategy for efficient group behavior. Experimental results demonstrate superior performance compared to baseline MATD3, achieving a stable average reward of 7.5 (vs. baseline’s 6.5) and improved linearity values around-1 (vs. baseline’s-3) The framework converges within 1,200 episodes and maintains low variance in performance metrics, indicating robust stability. Combining the subsequent algorithms advances hydraulic support control techniques and cooperative multi-agent system approaches in complex industrial environments.},
  keywords={Training;Technological innovation;Uncertainty;Robust stability;Geology;Scalability;Linearity;Hydraulic systems;Reinforcement learning;Game theory;Hydraulic Support Straightening;Multi-Agent Reinforcement Learning;Cooperative Game Theory;MATD3;Shapley Values;Dec-POMDP},
  doi={10.1109/YAC66630.2025.11149745},
  ISSN={2837-8601},
  month={May},}@INPROCEEDINGS{10533525,
  author={V, Nijana. and Rajendran, P. Selvi},
  booktitle={2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS)}, 
  title={A Comparative Study on Reinforcement Learning in Disease Prediction on Medical Data}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Medical imaging has been used extensively in healthcare in recent years for a variety of purposes, including disease diagnosis, treatment planning, and tracking the course of an illness. These applications entail taking pictures of the afflicted organ using a variety of modalities. Image segmentation and classification is two important process performed in disease diagnostic application. Deep reinforcement learning approach applied in many gaming application. DRL provides the accurate result in many real world applications, so researchers pay attention in DRL in medical data that helps the physicians in treating the patient. This study focus on surveying the various applications such as Image registration, lesion localization, image segmentation, image classification and Landmark detection using deep reinforcement learning in healthcare. In addition, this study investigates on the various DRL algorithms (Qlearning, deep deterministic policy gradient (DDPG), Twin Delayed Deep Deterministic Policy Gradient (TDDDPG), Deep Q Network (DQN) and Soft Actor-Critic (SAC) in Alzheimer's disease prediction. Among the DRL methods the DDPG achieved the highest accuracy in detecting the Alzheimer's disease with 97%.},
  keywords={Location awareness;Surveys;Image segmentation;Image registration;Deep reinforcement learning;Prediction algorithms;Lesions;Reinforcement Learning;Alzheimer's Disease Prediction;Medical Data;MRI Images},
  doi={10.1109/ADICS58448.2024.10533525},
  ISSN={},
  month={April},}@INPROCEEDINGS{9619002,
  author={Harris, Anthony and Liu, Siming},
  booktitle={2021 IEEE Conference on Games (CoG)}, 
  title={MAIDRL: Semi-centralized Multi-Agent Reinforcement Learning using Agent Influence}, 
  year={2021},
  volume={},
  number={},
  pages={01-08},
  abstract={In recent years, reinforcement learning algorithms have been used in the field of multi-agent systems to help the agents with interactions and cooperation on a variety of tasks. Controlling multiple agents simultaneously is extremely challenging as the complexity increases drastically with the number of agents in the system. In this study, we propose a novel semi-centralized deep reinforcement learning algorithm, MAIDRL, for mixed cooperative and competitive multi-agent environments. Specifically, we design a robust DenseNet-style actor-critic structured deep neural network for controlling multiple agents based on the combination of local observation and abstracted global information to compete with opponent agents. We extract common knowledge through influence maps considering both enemy and friendly agents for unit positioning and decision-making in combat. Compared to the centralized method, our design promotes a thorough understanding of the potential influence that a unit has without the need for a complete view of the global state. In addition, this design enables multiagent understanding of common goals, unlike fully decentralized methods. The proposed method has been evaluated on StarCraft Multi-Agent Challenge scenarios in the real-time strategy game, StarCraft II, and the results show that, statistically, the agents controlled by MAIDRL perform better than or as well as those controlled by centralized and decentralized methods.},
  keywords={Deep learning;Design methodology;Conferences;Decision making;Reinforcement learning;Games;Real-time systems;Deep reinforcement learning;multi-agent system;influence map;StarCraft II;SMAC;MAIRL;MAIDRL},
  doi={10.1109/CoG52621.2021.9619002},
  ISSN={2325-4289},
  month={Aug},}@INPROCEEDINGS{9626024,
  author={Liu, Qi and Hayashida, Tomohiro and Nishizaki, Ichiro and Sekizaki, Shinya},
  booktitle={2021 IEEE 12th International Workshop on Computational Intelligence and Applications (IWCIA)}, 
  title={Improvement of learning method of multi-agent system by sharing learning data}, 
  year={2021},
  volume={},
  number={},
  pages={1-5},
  abstract={Deep reinforcement learning algorithms have been employed to solve the problems of multi-agent learning, especially the non-stationarity problem that reduces the learning efficiency. In this study, an architecture of improving the learning efficiency by using an adaptation of Actor-Critic method is proposed. It was clear that the convergence of learning became faster when the agents shared their experience data with the other agents at some rate. In two kinds of maze benchmarks, some numerical experiments was conducted to show the performance of the learning architecture. And the result of this numerical experiments showed that the agents achieved their purpose in the mazes by the proposed method. Even though the agents did not share all their data, they could be trained to take the optimal policies.},
  keywords={Learning systems;Conferences;Reinforcement learning;Computer architecture;Benchmark testing;Computational intelligence;Multi-agent systems;Deep reinforcement learning;Multi-agent systems;Sharing data;Actor-Critic},
  doi={10.1109/IWCIA52852.2021.9626024},
  ISSN={1883-3977},
  month={Nov},}@ARTICLE{10812765,
  author={Rizvi, Danish and Boyle, David},
  journal={IEEE Transactions on Machine Learning in Communications and Networking}, 
  title={Multi-Agent Reinforcement Learning With Action Masking for UAV-Enabled Mobile Communications}, 
  year={2025},
  volume={3},
  number={},
  pages={117-132},
  abstract={Unmanned Aerial Vehicles (UAVs) are increasingly used as aerial base stations to provide ad hoc communications infrastructure. Building upon prior research efforts which consider either static nodes, 2D trajectories or single UAV systems, this paper focuses on the use of multiple UAVs for providing wireless communication to mobile users in the absence of terrestrial communications infrastructure. In particular, we jointly optimize UAV 3D trajectory and NOMA power allocation to maximize system throughput. Firstly, a weighted K-means-based clustering algorithm establishes UAV-user associations at regular intervals. Then the efficacy of training a novel Shared Deep Q-Network (SDQN) with action masking is explored. Unlike training each UAV separately using DQN, the SDQN reduces training time by using the experiences of multiple UAVs instead of a single agent. We also show that SDQN can be used to train a multi-agent system with differing action spaces. Simulation results confirm that: 1) training a shared DQN outperforms a conventional DQN in terms of maximum system throughput (+20%) and training time (-10%); 2) it can converge for agents with different action spaces, yielding a 9% increase in throughput compared to Mutual DQN algorithm; and 3) combining NOMA with an SDQN architecture enables the network to achieve a better sum rate compared with existing baseline schemes.},
  keywords={Autonomous aerial vehicles;NOMA;Trajectory;Throughput;Three-dimensional displays;Resource management;Ad hoc networks;Training;Clustering algorithms;Base stations;Aerial networks;non-orthogonal multiple access;multi-agent reinforcement learning;invalid action masking},
  doi={10.1109/TMLCN.2024.3521876},
  ISSN={2831-316X},
  month={},}@ARTICLE{10013773,
  author={Bernárdez, Guillermo and Suárez-Varela, José and López, Albert and Shi, Xiang and Xiao, Shihan and Cheng, Xiangle and Barlet-Ros, Pere and Cabellos-Aparicio, Albert},
  journal={IEEE Transactions on Cognitive Communications and Networking}, 
  title={MAGNNETO: A Graph Neural Network-Based Multi-Agent System for Traffic Engineering}, 
  year={2023},
  volume={9},
  number={2},
  pages={494-506},
  abstract={Current trends in networking propose the use of Machine Learning (ML) for a wide variety of network optimization tasks. As such, many efforts have been made to produce ML-based solutions for Traffic Engineering (TE), which is a fundamental problem in Internet Service Provider (ISP) networks. Nowadays, state-of-the-art TE optimizers rely on traditional optimization techniques, such as Local search, Constraint Programming, or Linear programming. In this paper, we present MAGNNETO, a distributed ML-based framework that leverages Multi-Agent Reinforcement Learning and Graph Neural Networks for distributed TE optimization. MAGNNETO deploys a set of agents across the network that learn and communicate in a distributed fashion via message exchanges between neighboring agents. Particularly, we apply this framework to optimize link weights in Open Shortest Path First (OSPF), with the goal of minimizing network congestion. In our evaluation, we compare MAGNNETO against several state-of-the-art TE optimizers in more than 75 topologies (up to 153 nodes and 354 links), including realistic traffic loads. Our experimental results show that, thanks to its distributed nature, MAGNNETO achieves comparable performance to state-of-the-art TE optimizers with significantly lower execution times. Moreover, our ML-based solution demonstrates a strong generalization capability to successfully operate in new networks unseen during training.},
  keywords={Optimization;Routing;Topology;Proposals;Network topology;Training;Standards;Traffic engineering;routing optimization;multi-agent reinforcement learning;graph neural networks},
  doi={10.1109/TCCN.2023.3235719},
  ISSN={2332-7731},
  month={April},}@INPROCEEDINGS{8847943,
  author={Xu, Linjie and Chen, Yihong},
  booktitle={2019 IEEE Conference on Games (CoG)}, 
  title={A Hierarchical Approach for MARLÖ Challenge}, 
  year={2019},
  volume={},
  number={},
  pages={1-4},
  abstract={Recently reinforcement learning has been showing remarkable performance in playing games. However, the majority of conventional approaches merely solve games with a single task. It is not yet well studied whether reinforcement learning is effective in games like Minecraft, where players are required to finish multiple different tasks while cooperating with other collaborators. In such games, AIs are confronted with dual challenges - finishing multiple tasks and building a multi-agent system. We propose a hierarchical approach with reinforcement learning policies to address the challenges. Experiments show that our approach performs well when dealing with multiple tasks and multiple agents simultaneously. Our approach got the second runner-up in MARLO Challenge, demonstrating its potential in tackling the challenges.},
  keywords={Task analysis;Games;Reinforcement learning;Training;Animals;Multi-agent systems;Game AI;Multi-Task;Multi-Agent;Reinforcement Learning;Minecraft},
  doi={10.1109/CIG.2019.8847943},
  ISSN={2325-4289},
  month={Aug},}@INPROCEEDINGS{10183145,
  author={He, Wenji and Yao, Haipeng and Wang, Fu and Wang, Zunliang and Xiong, Zehui},
  booktitle={2023 International Wireless Communications and Mobile Computing (IWCMC)}, 
  title={Enhancing the Efficiency of UAV Swarms Communication in 5G Networks through a Hybrid Split and Federated Learning Approach}, 
  year={2023},
  volume={},
  number={},
  pages={1371-1376},
  abstract={The integration of unmanned aerial vehicles (UAVs) with 5G networks presents a promising opportunity to revolutionize wireless communication and provide high-speed internet access to remote areas. Nevertheless, the vast quantity of data generated by UAVs requires the implementation of efficient distributed learning techniques. In this study, we present a novel hybrid approach that merges Federated Learning (FL) and Split Learning (SL) to optimize the performance of UAV swarms in 5G networks. While FL is capable of reducing communication overhead and preserving privacy, SL can enhance the accuracy of the model through the utilization of the local computational resources of each device. To realize the hybrid approach, we first locally train the model on each UAV using split learning. Subsequently, the encrypted model parameters are transmitted to a central server for federated averaging. Finally, the updated model is dispatched back to each UAV for local fine-tuning, and this cycle is repeated until convergence is achieved. The hybrid approach capitalizes on the strengths of both FL and SL to minimize communication overhead and increase accuracy. To tackle the challenge of selecting the most suitable UAVs for participation in the learning process, we propose a multiagent algorithm that considers factors such as communication latency and training time. Our experimental results indicate that the proposed approach leads to substantial improvements in communication overhead and accuracy compared to conventional methods.},
  keywords={Wireless communication;Privacy;Computer aided instruction;5G mobile communication;Federated learning;Distance learning;Computational modeling;UAV swarms;split learning;federated learning;user selection;multi-agent reinforcement learning},
  doi={10.1109/IWCMC58020.2023.10183145},
  ISSN={2376-6506},
  month={June},}@ARTICLE{10550936,
  author={Liu, Dingbang and Ren, Fenghui and Yan, Jun and Su, Guoxin and Gu, Wen and Kato, Shohei},
  journal={IEEE Access}, 
  title={Scaling Up Multi-Agent Reinforcement Learning: An Extensive Survey on Scalability Issues}, 
  year={2024},
  volume={12},
  number={},
  pages={94610-94631},
  abstract={Multi-agent learning has made significant strides in recent years. Benefiting from deep learning, multi-agent deep reinforcement learning (MADRL) has transcended traditional limitations seen in tabular tasks, arousing tremendous research interest. However, compared to other challenges in MADRL, scalability remains underemphasized, impeding the application of MADRL in complex scenarios. Scalability stands as a foundational attribute of the multi-agent system (MAS), offering a potent approach to understand and improve collective learning among agents. It encompasses the capacity to handle the increasing state-action space which arises not only from a large number of agents but also from other factors related to agents and environment. In contrast to prior surveys, this work provides a comprehensive exposition of scalability concerns in MADRL. We first introduce foundational knowledge about deep reinforcement learning and MADRL to underscore the distinctiveness of scalability issues in this domain. Subsequently, we delve into the problems posed by scalability, examining agent complexity, environment complexity, and robustness against perturbation. We elaborate on the methods that demonstrate the evolution of scalable algorithms. To conclude this survey, we discuss challenges, identify trends, and outline possible directions for future work on scalability issues. It is our aspiration that this survey enhances the understanding of researchers in this field, providing a valuable resource for in-depth exploration.},
  keywords={Scalability;Complexity theory;Robustness;Surveys;Deep reinforcement learning;Task analysis;Games;Multi-agent systems;Collective intelligence;Multi-agent learning;reinforcement learning;scalability;collective learning},
  doi={10.1109/ACCESS.2024.3410318},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11014655,
  author={Silveria, Dimitria and Cabral, Kleber and Givigi, Sidney},
  booktitle={2025 IEEE International systems Conference (SysCon)}, 
  title={Scalable Swarm Control Using Deep Reinforcement Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={Autonomous swarm navigation has been extensively studied due to its wide range of applications, from agriculture to surveillance and defense. Among the techniques used for swarm coordination, multi-agent reinforcement learning (MARL) has shown promise but is hindered by two main challenges. The first is the stochasticity of the environment, which is caused by the dynamic interaction among agents. The second is scalability, which becomes an issue as larger swarms require more complex neural networks and computational resources. To address these challenges, we propose a pipeline in which agents are trained using deep reinforcement learning in a single-agent, static environment. The resulting policy is then applied to multi-agent scenarios. We present a framework detailing this approach and its components. Our results show that a policy trained in a single-agent, static setting can be generalized effectively to multi-agent environments, mitigating the stochasticity issue. Furthermore, our model achieved collective behavior relying only on local information (agents in its neighborhood and a shared common goal), enabling scalability to large swarms. We compared our approach with a classical swarm control algorithm (flocking control) and a MARL approach (MADDPG), highlighting its efficiency and scalability. Our framework's performance is comparable with these two baselines, even performing better in some cases.},
  keywords={Training;Target tracking;Navigation;Scalability;Surveillance;Pipelines;Neural networks;Deep reinforcement learning;Control systems;Multi-agent systems;Swarm control;multi-agent system;reinforcement learning},
  doi={10.1109/SysCon64521.2025.11014655},
  ISSN={2472-9647},
  month={April},}@INPROCEEDINGS{10191777,
  author={Hu, Xunhan and Zhao, Jian and Zhao, Youpeng and Zhou, Wengang and Li, Houqiang},
  booktitle={2023 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Q-SAT: Value Factorization with Self-Attention for Deep Multi-Agent Reinforcement Learning}, 
  year={2023},
  volume={},
  number={},
  pages={01-08},
  abstract={In many real-world tasks, a team of agents learn to cooperate with each other under the setting of partial observability and communication constraints, where value factorization has been demonstrated as an effective solution. In a multi-agent system, it's important to capture the inter-connection between agents and push agents to consider more of the relevant teammates. Motivated by the success of self-attention in natural language processing and computer vision, we propose a novel value factorization mechanism, called Q-function Self ATtention (Q-SAT). It models the pairwise action-value functions and connection coefficient between agent pairs explicitly, and pays more attention to the interrelated agents when making decisions. Satisfying the IGM principle, Q-SAT introduces the self-attention into value factorization network. This attention mechanism enables more effective and efficient learning in complex multi-agent environments. Q-SAT can be viewed as a basic building block and is ready to be applied to existing value factorization methods. The experimental results show that Q-SAT captures the connection relationship between agents and significantly improves the learning performance on the challenging StarCraft II micromanagement task and Google Research Football task.},
  keywords={Weight measurement;Technological innovation;Neural networks;Reinforcement learning;Natural language processing;Internet;Task analysis;Multi-agent System;Reinforcement Learning},
  doi={10.1109/IJCNN54540.2023.10191777},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{11050286,
  author={Zhang, Haoyang and Zhan, Sen and Kok, Koen and Paterakis, Nikolaos G.},
  booktitle={2025 21st International Conference on the European Energy Market (EEM)}, 
  title={Review of Market Power Detection in Distribution Networks: Case Study on Local Flexibility Markets}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Market power in electricity markets refers to the ability of a participant or group of participants to earn additional profits by manipulating market prices. In distribution networks (DNs), market power has become a significant concern with the growth of local electricity markets (LEMs) and local flexibility markets (LFMs) as more distributed energy resources (DERs) are integrated into the grid. Detecting and averting market power is essential to ensure efficient and fair local market operations. This paper provides a comprehensive review of existing methodologies for market power detection in electricity markets, highlighting their applicability and limitations within local markets in the DNs. A simulation environment using multi-agent reinforcement learning (MARL) is developed through a case study on an LFM to model the strategic interactions and market power of participants. By utilizing this simulation environment and the known profit gap between truthful bidding prices based on marginal costs and strategic bidding prices using the MARL-based method, various market power detection indices are evaluated for their accuracy and robustness. The simulation results highlight the need to consider the market design and the presence of prosumers when applying conventional market power detection indices to LFMs, particularly with regard to pricing mechanisms.},
  keywords={Costs;Reviews;Simulation;Europe;Reinforcement learning;Distribution networks;Pricing;Electricity supply industry;Robustness;Distributed power generation;Distribution grid;Local flexibility market;Market power detection indices;Reinforcement learning;Strategic bidding},
  doi={10.1109/EEM64765.2025.11050286},
  ISSN={2165-4093},
  month={May},}@INPROCEEDINGS{10865198,
  author={Ting, Wang and Mengya, Wang and Sipan, Chen and Lianjie, Jing and Wenxuan, He},
  booktitle={2024 China Automation Congress (CAC)}, 
  title={Deceptive Pollination Process - Modeling Based on Individual Agents}, 
  year={2024},
  volume={},
  number={},
  pages={3050-3054},
  abstract={This study explores the interactions between deceptive plants and pollinators using Agent-Based Modeling (ABM) to overcome the limitations of traditional mathematical models. By treating plants and pollinators as independent agents with defined behaviors and incorporating deep reinforcement learning, agents adapt to environmental changes and optimize strategies. Simulated experiments reveal the dynamic evolution of these interactions, highlighting competition and cooperation patterns. The findings show that ABM effectively simulates the interactions between deceptive plants and pollinators, providing valuable insights and analytical tools for understanding complex ecological interactions and evolution.},
  keywords={Fluctuations;Automation;Evolution (biology);System dynamics;Chaotic communication;Simulation;Games;Deep reinforcement learning;Mathematical models;Ecology;pollinator;ecological dynamics;intraspecific communication},
  doi={10.1109/CAC63892.2024.10865198},
  ISSN={2688-0938},
  month={Nov},}@ARTICLE{10947715,
  author={Baleghi, Behzad and Mohammad-Bagher Malaek, Seyed},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Real-Time Intelligent Landing-Management Under Urban Unpredictable Operations}, 
  year={2025},
  volume={26},
  number={6},
  pages={8247-8256},
  abstract={The integration of electric Vertical Takeoff and Landing (eVTOL) vehicles into urban transportation presents challenges in scalability, real-time adaptability, and operational efficiency. This study introduces an Agent-Based Modeling (ABM) framework for traffic management, dynamically assigning eVTOLs to landing pads based on real-time data to enhance efficiency in congested urban environments. A comparative analysis is conducted across various scheduling and sequencing approaches, including Mixed-Integer Linear Programming (MILP), Time-Advance (TA), heuristic methods, receding horizon scheduling, reinforcement learning (RL)-based frameworks, and decentralized agent-based strategies. While MILP and TA offer structured scheduling, they struggle with scalability. Heuristic and receding horizon methods improve adaptability but require frequent recomputation, and RL-based approaches show promise but demand extensive training. Current Decentralized models support distributed decision-making but face efficiency constraints at scale. The proposed ABM framework effectively manages 200 eVTOLs with near-linear computational scaling, facilitating real-time negotiations and reducing computational bottlenecks seen in centralized models. Simulation results indicate improved assignment efficiency, landing pad utilization, and reduced negotiation times under high-density conditions. As UAM systems expand, ABM may contribute to operational resilience. Future work will explore integrating environmental factors to further enhance robustness.},
  keywords={Real-time systems;Atmospheric modeling;Vehicle dynamics;Air traffic control;Adaptation models;Scalability;Agent-based modeling;Resilience;Decision making;Navigation;Urban air mobility (UAM);eVTOL vehicles;agent-based modeling (ABM);real-time traffic management;dynamic scheduling;autonomous landing allocation;scalability;real-time adaptability;urban airspace management;fleet operations optimization},
  doi={10.1109/TITS.2025.3554453},
  ISSN={1558-0016},
  month={June},}@ARTICLE{10417835,
  author={Karimi, Sara and Asadi, Sahar and Payberah, Amir H.},
  journal={IEEE Transactions on Games}, 
  title={BaziGooshi: A Hybrid Model of Reinforcement Learning for Generalization in Gameplay}, 
  year={2024},
  volume={16},
  number={3},
  pages={722-734},
  abstract={While reinforcement learning (RL) is gaining popularity in gameplay, creating a generalized RL model is still challenging. This study presents BaziGooshi, a generalized RL solution for games, focusing on two different types of games: 1) a puzzle game Candy Crush Friends Saga and 2) a platform game Sonic the Hedgehog Genesis. BaziGooshi rewards RL agents for mastering a set of intrinsic basic skills as well as achieving the game objectives. The solution includes a hybrid model that takes advantage of a combination of several agents pretrained using intrinsic or extrinsic rewards to determine the actions. We propose an RL-based method for assigning weights to the pretrained agents. Through experiments, we show that the RL-based approach improves generalization to unseen levels, and BaziGooshi surpasses the performance of most of the defined baselines in both games. Also, we perform additional experiments to investigate further the impacts of using intrinsic rewards and the effects of using different combinations in the proposed hybrid models.},
  keywords={Games;Color;Training;Green products;Encoding;Shape;Reinforcement learning;Deep reinforcement learning},
  doi={10.1109/TG.2024.3355172},
  ISSN={2475-1510},
  month={Sep.},}@INPROCEEDINGS{10919340,
  author={Ding, Ao and Zhang, Huaqing and Ma, Hongbin},
  booktitle={2024 International Annual Conference on Complex Systems and Intelligent Science (CSIS-IAC)}, 
  title={An Efficient Policy Gradient Algorithm with Historical Behaviors Reusing in Multi Agent System}, 
  year={2024},
  volume={},
  number={},
  pages={585-592},
  abstract={In multi-agent reinforcement learning, the algorithm's sampling efficiency of historical experience trajectory is regarded as the key to improving the working effect of the agent. In order to make full use of interactive data to improve the sampling ability of agents, an efficient multi-agent reinforcement learning algorithm is proposed in this paper. For multi-agent systems, the policy gradient algorithm with historical behavior reusing (MAPG-HBR) proposed in this paper can take into account the influence of historical behavior on the policy in the policy promotion stage, so that the multi-agents can learn the approximately optimal joint policy. To obtain the advantage functions used in MAPG-HBR with only one critic network, a theoretically interpretable twin universal critic network is proposed in this paper, which is capable of simultaneously estimating the action-value function as well as the state-value function and the corresponding objective value function for Clipped Double Q Learning. We compare the effectiveness of this algorithm with several baselines in Waterworld and Multi-Agent Mujoco, which are currently very popular multi-agent test environments. The results show that MAPG-HBR algorithm has better performance than other algorithms in the environments.},
  keywords={Q-learning;Approximation algorithms;Entropy;Trajectory;Complex systems;Multi-agent systems;Historical behaviors reusing;Policy gradient;Sampling efficiency;Multi-agent system},
  doi={10.1109/CSIS-IAC63491.2024.10919340},
  ISSN={},
  month={Sep.},}@ARTICLE{10965637,
  author={Kazim, Raza Muhammad and Wang, Guoxin and Ming, Zhenjun and Cao, Jinhui and Allen, Janet K. and Mistree, Farrokh},
  journal={IEEE Access}, 
  title={A Design Framework for Scalable and Adaptive Multi-Agent Coordination in Dynamic Environments: Addressing Concurrent Agent and Environment Interactions}, 
  year={2025},
  volume={13},
  number={},
  pages={67029-67055},
  abstract={In dynamic environments, such as box-pushing tasks, multi-agent systems (MAS) face significant challenges in coordinating agents within high-density settings while managing uncertainties arising from fluctuations in agent configurations and environmental dynamics. In this study, we explore the integration of surrogate response surface modeling (SRSM) with optimization algorithms, comparing Stochastic Gradient Descent (SGD) with a fixed learning rate, and adaptive learning rate (ALR) optimizers—including Adaptive Moment Estimation (ADAM) and Adaptive Approximate Direction Method Algorithm (AADMA)—to enhance MAS performance metrics, such as Agent Collision Rate (ACR), Agent Movement Frequency (AMF), and Task Completion Time (TCT). Through systematic experimentation across five scenarios, SRSM is employed to uncover key trends in MAS performance and identify configurations that improve scalability and adaptability. From the analysis of simulation data, it has been observed that SGD struggles significantly in dynamic environments, while ADAM demonstrates moderate improvements. However, AADMA consistently outperforms both by reducing loss, lowering collision rates, increasing movement efficiency, and achieving shorter task completion times. Performance comparison charts and loss function graphs emphasize AADMA’s superiority in addressing the complexities of real-time coordination and adaptability. Through this study, we highlight the critical role of combining SRSM with ARL to design an MAS that is capable of thriving in complex, dynamic, and high-density environments. By addressing key scalability and adaptability challenges, the proposed framework significantly advances MAS design, paving the way for improved multi-agent coordination in real-world applications.},
  keywords={Multi-agent systems;Scalability;Heuristic algorithms;Vehicle dynamics;Adaptive learning;Reinforcement learning;Adaptation models;Response surface methodology;Machine learning algorithms;Robot sensing systems;Multi-agent reinforcement learning;adaptive learning rate;box-pushing task;multi-agent system;dynamic environments;scalability;adaptability},
  doi={10.1109/ACCESS.2025.3560988},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9356182,
  author={Osoba, Osonde A. and Vardavas, Raffaele and Grana, Justin and Zutshi, Rushil and Jaycocks, Amber},
  booktitle={2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA)}, 
  title={Modeling Agent Behaviors for Policy Analysis via Reinforcement Learning}, 
  year={2020},
  volume={},
  number={},
  pages={213-219},
  abstract={Agent-based Models (ABMs) are valuable tools for policy analysis. ABMs help analysts explore the emergent consequences of regulatory and policy interventions in multi-agent decision-making settings. But the validity of inferences drawn from ABM explorations depends on the quality of the ABM agents' behavioral models. Prior approaches for specifying behaviors have limitations. This paper examines the value of reinforcement learning (RL) models as adaptive, high-performing, and behaviorally-valid models of agent decision-making in ABMs. We discuss the value of RL for modeling agents' utility-maximizing behaviors in policy-relevant ABMs. We address the problem of adapting RL algorithms to handle multi-agency in games by adapting and extending methods from recent literature. We evaluate examples of such RL-based ABM agents via experiments on two policy-relevant ABMs: a Minority Game ABM, and an ABM of Influenza Transmission. The RL behavioral models can outperform the default adaptive behavioral models. We also run analytic experiments on our RL-equipped ABMs: explorations of the effects of dynamic behavioral heterogeneity in a population, the impact of social network factors on adaptability, and the emergence of synchronization in a community. Our results suggest that the RL formalism can be an efficient abstraction for behavioral models in ABMs.},
  keywords={Adaptation models;Analytical models;Machine learning algorithms;Decision making;Reinforcement learning;Tools;Synchronization;Multi-agent learning;reinforcement learning;policy optimization;actor-critic models;agent-based models},
  doi={10.1109/ICMLA51294.2020.00043},
  ISSN={},
  month={Dec},}@ARTICLE{11023596,
  author={Zhou, Sihan and Hou, Yaqing and Wu, Yaoxin and Yu, Xiangchao and Zhou, Liran and Piao, Haiyin and Zhang, Qiang},
  journal={IEEE Transactions on Cognitive and Developmental Systems}, 
  title={Cooperative Multi-Agent Advice Exchange via Topological Graph Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1-13},
  abstract={Advice exchange is a commonly used approach to enhance the performance of multi-agent reinforcement learning (MARL). It refers (requesting) agents to beneficial behaviors of (target) agents and thus facilitates efficient policy learning in a multi-agent system (MAS). However, traditional advice exchange approaches often depend on polling all agents, causing substantial communication costs and computational effort. Moreover, they adopt manually designed rules to select teacher agents, which ignore the natural topology in MAS and limit policy learning. In this paper, we propose a Cooperative Multi-Agent Advice Exchange via Topological Graph Learning (ToGAE), which entails the similarity of knowledge domains among agents in cooperative MAS. ToGAE enables agents to select their corresponding target agents with the largest knowledge domain similarity for advice exchange. The knowledge domain similarity is extracted by a two-stage graph attention network with Jensen–Shannon divergence-based training loss, and favorably reflects the functional relationship between two agents. In addition, we design a clarity-based advice acceptance scheme to avoid the blind execution of any advice, and thus further boost the efficiency of MARL. Extensive experiments show that ToGAE significantly outperforms the baseline methods in terms of the efficiency of policy learning and performance on different MARL tasks.},
  keywords={Training;Correlation;Graph neural networks;Games;Collaboration;Attention mechanisms;Decision making;Topology;Technological innovation;Q-learning;Multi-agent reinforcement learning (MARL);advice exchange;knowledge domain similarity},
  doi={10.1109/TCDS.2025.3576377},
  ISSN={2379-8939},
  month={},}@INPROCEEDINGS{10023568,
  author={Liu, Yuxiao and Wang, Qingling},
  booktitle={2022 37th Youth Academic Annual Conference of Chinese Association of Automation (YAC)}, 
  title={Game Confrontation of 5v5 Multi-Agent Based on MAPPO Reinforcement Learning Algorithm}, 
  year={2022},
  volume={},
  number={},
  pages={1395-1398},
  abstract={In the 5v5 air intelligence game, this is an environment in which the data of the agent platform can be observed. We will first consider using rules to build agents, but rules can not deal with all kinds of combat situations, which makes us think of the method of Multi-Agent Reinforcement Learning. This paper combines rules and reinforcement learning methods to construct an agent to try to solve the problem of air intelligent game.},
  keywords={Automation;Reinforcement learning;Games;Multi-agent systems;Multi agent system;reinforcement learning;rule agent},
  doi={10.1109/YAC57282.2022.10023568},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{8569346,
  author={Kravaris, Theocharis and Spatharis, Christos and Blekas, Konstantions and Vouros, George A. and Cordero Garcia, Jose Manuel},
  booktitle={2018 IEEE/AIAA 37th Digital Avionics Systems Conference (DASC)}, 
  title={Multiagent Reinforcement Learning Methods for Resolving Demand - Capacity Imbalances}, 
  year={2018},
  volume={},
  number={},
  pages={1-10},
  abstract={In this article, we explore the computation of joint policies for autonomous agents, representing flights, to resolve congestions problems in the Air Traffic Management (ATM) domain in the context of Demand-Capacity Balance (DCB) process. We formalize the problem as a multi-agent Markov Decision Process (MDP) towards deciding flight ground delays to resolve imbalances, during the pre-tactical phase. To this end, we present and evaluate multi-agent reinforcement learning methods. An experimental study on real-world cases confirms the effectiveness of our approach.},
  keywords={Trajectory;Delays;Aircraft;Air traffic control;Schedules;Demand-Capacity Problem;Multi-agent system;Reinforcement Learning},
  doi={10.1109/DASC.2018.8569346},
  ISSN={2155-7209},
  month={Sep.},}@INPROCEEDINGS{10847364,
  author={Wang, Lei},
  booktitle={2024 IEEE 16th International Conference on Computational Intelligence and Communication Networks (CICN)}, 
  title={Dynamic Pricing Algorithm Based on Deep Reinforcement Learning}, 
  year={2024},
  volume={},
  number={},
  pages={303-308},
  abstract={Deep reinforcement learning has received widespread attention in recent years and has achieved significant success in various fields Due to the fact that real-world environments typically involve multiple agents interacting with the environment, multi-agent deep reinforcement learning has flourished and achieved excellent performance in complex sequential decision-making tasks in various fields. In order to solve the problem of dynamic pricing under unknown demand functions, this paper constructs a deep reinforcement learning framework with the goal of maximizing expected returns to find the optimal dynamic pricing strategy. Firstly, taking the service industry as a case study, a model and method for the decision-making process of revenue management were constructed, and the Markov property of revenue management problems was defined; Furthermore, a program was developed to conduct experiments, comparing and analyzing the experimental results of revenue management methods based on deep reinforcement learning with traditional methods The results indicate that the dynamic pricing strategy of intelligent agents can flexibly adjust prices at different levels of demand, and its performance is significantly better than traditional strategies. This article provides a theoretical basis for enterprises to make precise pricing and marketing decisions in the context of big data.},
  keywords={Training;Analytical models;Heuristic algorithms;Soft sensors;Decision making;Supervised learning;Pricing;Deep reinforcement learning;Mathematical models;Performance analysis;deep reinforcement learning;Dynamic pricing;Revenue management},
  doi={10.1109/CICN63059.2024.10847364},
  ISSN={2472-7555},
  month={Dec},}@ARTICLE{11170313,
  author={Hillali, Youness and Zegrari, Mourad and Chafik, Samir and Alfathi, Najlae},
  journal={IEEE Access}, 
  title={An Intelligent Framework for Multi-Agent System Based on Dynamic Balancing of Production 4.0}, 
  year={2025},
  volume={13},
  number={},
  pages={165695-165717},
  abstract={Balancing assembly lines is one of the biggest problems facing industrial processes. Conventionally, this balancing consists of optimally allocating production tasks between different workstations or stations, taking into account precedence, capacity and cycle time constraints. However, in the face of today’s dynamic environment (fluctuations in demand, technical hazards, labor shortages, etc.), static balancing is no longer sufficient. Companies need dynamic rebalancing solutions, designed to adapt task distribution in real time as soon as a perturbation appears. Our study based on the use of multi-agent systems (MAS) offers a potentially attractive approach. MAS enable decision-making to be decentralized, with each agent representative of a workstation, a set of tasks, an operator or a piece of equipment. Each agent assesses its situation locally (workload, availability, etc.) and negotiates with the others to achieve a more flexible, proactive assignment of tasks, in line with the principles of Industry 4.0 (connectivity, distributed intelligence and adaptability). The approach proposed in our study is a dynamic framework based on the use of three agents, each of which executes and cooperates with the other agents in order to determine parameters with high variability, propose adjustments and implement them. The results found show that the use of the dynamic balancing environment facilitates monitoring and decision-making through a dynamic dashboard including a set of KPIs specific to balancing. The operational impact of implementing this environment shows a significant increase in terms of productivity and availability, implicitly the synthetic rate of return (‘OEE’) of the production line. As a result, the financial aspect of reconfiguring the line has been reduced thanks to the recommendation agent, which proposes future adjustments without impacting the production line. The line balancing no longer depends on a single a priori measurement, but is instead adapted on an ongoing process, giving greater strength in the face of the uncertainties of a VUCA world.},
  keywords={Assembly;Production;Multi-agent systems;Workstations;Real-time systems;Indexes;Decision making;Resource management;Heuristic algorithms;Faces;Dynamic balancing;multi-agent system;VUCA world;variability},
  doi={10.1109/ACCESS.2025.3611631},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10191833,
  author={Yu, Yang and Yin, Qiyue and Zhang, Junge and Chen, Hao and Huang, Kaiqi},
  booktitle={2023 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Underexplored Subspace Mining for Sparse-Reward Cooperative Multi-Agent Reinforcement Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Learning cooperation in sparse-reward multi-agent reinforcement learning is challenging, since agents need to explore in the large joint-state space with sparse feedback. However, in cooperative games, the cooperative target is often related to partial attributes, hence there is no need to treat the whole state space equally. Therefore, we propose Underexplored Subspace Mining (USM), a novel type of intrinsic reward that encourages agents to selectively explore partial attributes instead of wasting time on the whole state space to accelerate learning. Specially, considering that the target-related attributes are varying in different games and hard to predefine, we choose to focus on the underexplored subspace as an alternative, which is an automatic aggregation of the underexplored bottom-level dimensions without any human design or learning parameters. We evaluate our method in cooperative games with discrete and continuous state space separately. Results demonstrate that USM consistently outperforms existing state-of-the-art methods, and becomes the only method that has succeeded in sparse-reward games evaluated with larger state space or more complicated cooperation dynamics.},
  keywords={Accelerated aging;Neural networks;Games;Reinforcement learning;Task analysis;sparse-reward cooperation;multi-agent system;reinforcement learning;selective exploration},
  doi={10.1109/IJCNN54540.2023.10191833},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{9898089,
  author={Yang, Ning and Ding, Bo and Shi, PeiChang and Feng, Dawei},
  booktitle={2022 IEEE International Conference on Joint Cloud Computing (JCC)}, 
  title={Improving scalability of multi-agent reinforcement learning with parameters sharing}, 
  year={2022},
  volume={},
  number={},
  pages={37-42},
  abstract={Improving the scalability of a multi-agent system is one of the key challenges for applying reinforcement learning to learn an effective policy. Parameter sharing is a common approach used to improve the efficiency of learning by reducing the volume of policy network parameters that need to be updated. However, sharing parameters also reduces the variance between agents’ policies, which further restricts the diversity of their behaviors. In this paper, we introduce a policy parameter sharing approach, it maintains a policy network for each agent, and only updates one of them. The differentiated behavior of agents is maintained by the policy, while sharing parameters are updated through a soft way. Experiments in foraging scenarios demonstrate that our method can effectively improve the performance and also the scalability of the multi-agent systems.},
  keywords={Training;Cloud computing;Scalability;Reinforcement learning;Behavioral sciences;Multi-agent systems;Deep reinforcement learning;Multi-agent sys-tem;Scalability},
  doi={10.1109/JCC56315.2022.00013},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10520504,
  author={Duong, Quang Huy and Janulewicz, Emil and Jaumard, Brigitte and Bentaleb, Abdelhak and Slobodrian, Sergio},
  booktitle={2023 IEEE Future Networks World Forum (FNWF)}, 
  title={Fully Distributed Multi-Agent RL Framework for QoS Routing}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Classical routing heuristics, e.g., Open Shortest Path First, have several significant issues, such as they are not able to generalize or adapt to heterogeneous environments including dynamics of topology, traffic patterns, and Quality of Service (QoS) requirements. To generalize solutions, network operators recently utilized machine learning algorithms at centralized controllers. However, centralized machine learning solutions are not scalable due to many reasons, such as slow data transfer to the central controller in a large network. Distributed multi-agent systems do not require a tedious and complex central controller while reducing data storage and computation burden as tasks are divided and handled at local servers/computers. In this paper, we present a fully distributed multi-agent system named MADQN, addressing the request provisioning problem, i.e., provisioning a request on a dedicated path satisfying latency and bandwidth requirements. MADQN applies a Deep Q-Network reinforcement learning algorithm to train the agents. Although each agent has its data and policy locally, they can still cooperate with other agents to finish a common routing task that maximizes the total reward. We evaluate the effectiveness of the MADQN with a benchmark network that consists of 100 nodes and 432 directed links, and a dynamic set of thousands of requests. The results show that the agents in MADQN can learn to cooperate and provision 99% of the requests, which is about a 9% improvement against the centralized single agent scheme.},
  keywords={Service function chaining;Network topology;Quality of service;Bandwidth;Reinforcement learning;Traffic control;Routing;5G;Multi-Agent Reinforcement Learning;distributed routing;multi-agent artificial intelligent},
  doi={10.1109/FNWF58287.2023.10520504},
  ISSN={2770-7679},
  month={Nov},}@ARTICLE{9801608,
  author={Johnson, Dazzle and Chen, Gang and Lu, Yuqian},
  journal={IEEE Robotics and Automation Letters}, 
  title={Multi-Agent Reinforcement Learning for Real-Time Dynamic Production Scheduling in a Robot Assembly Cell}, 
  year={2022},
  volume={7},
  number={3},
  pages={7684-7691},
  abstract={As industry rapidly shifts towards mass personalisation, the need for a decentralised multi-agent system capable of dynamic flexible job shop scheduling (FJSP) is evident. Traditional heuristic and meta-heuristic scheduling methods cannot achieve satisfactory results and have limited application to static environments. Recent Reinforcement Learning (RL) approaches that consider dynamic FJSP, lack flexibility and autonomy as they use a single-agent centralised model, assuming global observability. As such, we propose a Multi-Agent Reinforcement Learning (MARL) system for scheduling dynamically arriving assembly jobs in a robot assembly cell. We applied a Double DQN-based algorithm and proposed a generalised observation, action and reward design for the dynamic FJSP setting. Using a centralised training phase, each agent (i.e., robot) in the assembly cell executes decentralised scheduling decisions based on local observations. Our solution demonstrated improved performance against rule-based heuristic methods, for optimising makespan. We also reported the impact of different observation sizes of each agent on optimisation performance.},
  keywords={Robots;Job shop scheduling;Dynamic scheduling;Robot kinematics;Computer architecture;Microprocessors;Heuristic algorithms;Reinforcement learning;intelligent and flexible manufacturing;double DQN;flexible job shop scheduling problem (FJSP);multi-robot systems;mass personalisation},
  doi={10.1109/LRA.2022.3184795},
  ISSN={2377-3766},
  month={July},}@INPROCEEDINGS{10515998,
  author={Arshad, Mohd Anuar and Yao, Wenyan},
  booktitle={2024 International Conference on Distributed Computing and Optimization Techniques (ICDCOT)}, 
  title={Human Resource Management Decision Support System Based on Multi Agent}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Due to the rapid development of the economy and the increasing level of management, human resource management is increasingly valued by enterprise managers and has become a very important and essential part of enterprise management. However, many traditional human resource management methods are still based on experience and subjective judgment, lacking statistical or data analysis support, which can easily lead to inaccurate decision-making and waste of resources. This article aims to design a human resource management decision support system that can alleviate secondary problems. The second paragraph of this article introduces the current research on human resource management, the third paragraph introduces the structure and design methods of this system, and the fourth paragraph tests the system designed in this article and achieves good results. Further research is needed to address the issue of human resources.},
  keywords={Decision support systems;Data analysis;Design methodology;Reinforcement learning;Intelligent agents;Human resource management;Distributed computing;multi agent technology;decision support system;human resource management;system effectiveness},
  doi={10.1109/ICDCOT61034.2024.10515998},
  ISSN={},
  month={March},}@INPROCEEDINGS{11065776,
  author={Tang, Mingyu and Wang, Fuyong and Sun, Mingwei},
  booktitle={2025 IEEE 14th Data Driven Control and Learning Systems (DDCLS)}, 
  title={A Dynamic Model of Multi-Agent Reinforcement Learning Based on the Framework of Game Theory}, 
  year={2025},
  volume={},
  number={},
  pages={1563-1568},
  abstract={Modeling multi-agent learning dynamics has long been a significant research topic. A multi-agent Q-Learning model based on game theory is proposed. In large systems, agents interact using symmetric normal-form games and they employ Boltzmann exploration to learn optimal strategies, dynamic equations capture this learning process. Simulations show the model accurately describes behavior evolution and agents can learn to converge to Nash equilibrium strategies, validating model stability.},
  keywords={Q-learning;Heuristic algorithms;Games;Nash equilibrium;Solids;Mathematical models;Stability analysis;Data models;Particle swarm optimization;Multi-agent systems;Multi-agent system;Reinforcement learning;Game theory;Q-Learning algorithm;Dynamic model},
  doi={10.1109/DDCLS66240.2025.11065776},
  ISSN={2767-9861},
  month={May},}@INPROCEEDINGS{9400275,
  author={Mishra, Rajesh K and Vasal, Deepanshu and Vishwanath, Sriram},
  booktitle={2021 55th Annual Conference on Information Sciences and Systems (CISS)}, 
  title={Decentralized Multi-agent Reinforcement Learning with Shared Actions}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={In this paper, we consider a multi-agent system with N cooperative agents where each agent privately observes its own private type and publicly observes each others' actions. We propose a novel model-free reinforcement learning algorithm to compute the optimal policies for the agents that maximizes their collective reward. This setting belongs to the broad class of decentralized control problems with partial information. We use the common agent approach [1], wherein some fictitious common agent chooses the best policy based on a belief on the current states of the agents. These beliefs are updated individually for each agent from their current belief and action histories without the knowledge of the system dynamics. In this paper, we employ particle filters, called the bootstrap filters, to update the belief of all agents in a distributed manner. We illustrate our results with the help of a smart-grid application, where the users strive to reduce collective cost of power for all the agents in the grid. Finally, we compare the performances for model and model-free implementation of the reinforcement learning (RL) algorithm establishing the effectiveness of particle filter (PF) method.},
  keywords={System dynamics;Computational modeling;Decentralized control;Reinforcement learning;Particle filters;Smart grids;Multi-agent systems},
  doi={10.1109/CISS50987.2021.9400275},
  ISSN={},
  month={March},}@ARTICLE{11215785,
  author={Ahmed, Saleha and Uzair, Muhammad and Ullah, Syed Asad and Dev, Kapal and Mahmood, Aamir and Gidlund, Mikael and Hassan, Syed Ali},
  journal={IEEE Internet of Things Journal}, 
  title={Multi-Agent Reinforcement Learning for Joint Spectrum and Energy Optimization in CR-NOMA Enabled Internet of Unmanned Agents}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={With the rapid growth of Internet-of-Things (IoT) devices and unmanned agents (UAs), there is a rising need for energy- and spectrum-efficient wireless networks that can support large-scale, resource-constrained deployments. To meet this demand, integration of deep reinforcement learning (DRL), non-orthogonal multiple access (NOMA), and energy harvesting (EH) offers a promising approach to enhance energy efficiency (EE) and spectrum utilization in future sixth-generation (6G) networks, particularly for sustainable Internet of UA (IUA) communications. In this paper, we investigate an IUA network where multiple low-power secondary users (SUs), equipped with radio frequency energy harvesting (RF-EH) antennas, use a cognitive radio NOMA (CR-NOMA) scheme to share uplink channels with nearby primary users (PUs). We formulate a joint transmit power control and EH scheduling problem to maximize the long-term EE of the SUs and spectrum utilization of the network, subject to quality-of-service (QoS) constraints. To address the decentralized nature of the problem, we model the environment as a multi-agent system where each SU independently optimizes its transmission and EH strategies. A range of DRL and non-DRL algorithms is then applied to solve this optimization problem. We also explore different RF-EH diversity combining techniques to further boost system performance. Simulation results highlight the impact of these techniques on EE of SU, offering insights for optimizing performance under dynamic EH conditions.},
  keywords={Diversity reception;NOMA;Internet of Things;Resource management;Wireless communication;Quality of service;Optimization;Wireless sensor networks;Wireless networks;Power control;Internet of unmanned agents (IUA);deep reinforcement learning (DRL);non-orthogonal multiple access (NOMA);energy harvesting (EH);energy efficiency (EE)},
  doi={10.1109/JIOT.2025.3624882},
  ISSN={2327-4662},
  month={},}@INPROCEEDINGS{10817188,
  author={Iturria-Rivera, Pedro Enrique and Gaigalas, Raimundas and Elsayed, Medhat and Bavand, Majid and Ozcan, Yigit and Erol-Kantarci, Melike},
  booktitle={2024 IEEE 35th International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC)}, 
  title={Extended Reality (XR) Codec Adaptation in 5G using Multi-Agent Reinforcement Learning with Attention Action Selection}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Extended Reality (XR) services will revolutionize applications over $5^{\text {th }}$ and $\mathbf{6}^{\text {th }}$ generation wireless networks by providing seamless virtual and augmented reality experiences. These applications impose significant challenges on network infrastructure, which can be addressed by machine learning algorithms due to their adaptability. This paper presents a Multi-Agent Reinforcement Learning (MARL) solution for optimizing codec parameters of XR traffic, comparing it to the Adjust Packet Size (APS) algorithm. Our cooperative multi-agent system uses an Optimistic Mixture of Q-Values ($\mathbf{O Q M I X}$) approach for handling Cloud Gaming (CG), Augmented Reality (AR), and Virtual Reality (VR) traffic. Enhancements include an attention mechanism and slate-Markov Decision Process (MDP) for improved action selection. Simulations show our solution outperforms APS with average gains of $30.1 \%, 15.6 \%, 16.5 \% 50.3 \%$ in XR index, jitter, delay, and Packet Loss Ratio (PLR), respectively. APS tends to increase throughput but also packet losses, whereas oQMIX reduces PLR, delay, and jitter while maintaining goodput.},
  keywords={Codecs;Machine learning algorithms;Extended reality;Packet loss;Reinforcement learning;Jitter;Throughput;Delays;Indexes;Augmented reality;Extended Reality;Quality of Experience;Value Function Factorization},
  doi={10.1109/PIMRC59610.2024.10817188},
  ISSN={2166-9589},
  month={Sep.},}@INPROCEEDINGS{11178736,
  author={Li, Junying and Chen, Min and Wang, Yuhan and Sun, Haoyang and Pu, Zhiqiang},
  booktitle={2025 44th Chinese Control Conference (CCC)}, 
  title={Low Complexity MARL via Fluid Approximation and Successive Convex Approximation}, 
  year={2025},
  volume={},
  number={},
  pages={6040-6045},
  abstract={Scalability remains a critical challenge in Multi-Agent Reinforcement Learning (MARL), as individual agents often need to account for the joint state and action space, whose dimensionality grows exponentially with the number of agents. To address this challenge, we propose a novel low-complexity optimization framework, Fluid Approximation and Successive Convex Approximation (FA-SCA). Our approach employs fluid approximation to linearize the Bellman equation, thereby rendering the original problem into an approximate Bellman equation, which is more manageable for optimization purposes. Successive Convex Approximation (SCA) is then used to iteratively solve this problem by replacing the non-convex objective with convex surrogate functions, enabling the efficient solution of a series of convex sub-problems. This framework avoids the high computational cost of directly solving the Bellman equation and the extensive training required by deep learning models in high-dimensional spaces. Experimental results in the Multi-Agent Particle Environment (MPE) demonstrate that FA-SCA significantly enhances computational efficiency and scalability without sacrificing convergence performance. By effectively mitigating the joint action space explosion and ensuring stable, efficient solutions, FA-SCA provides a robust and practical framework for extending MARL to large-scale systems.},
  keywords={Degradation;Deep learning;Fluids;Scalability;Reinforcement learning;Approximation algorithms;Mathematical models;Complexity theory;Computational efficiency;Optimization;Multi-Agent System;Reinforcement Learning},
  doi={10.23919/CCC64809.2025.11178736},
  ISSN={1934-1768},
  month={July},}@ARTICLE{9606939,
  author={Wang, Jidong and Wu, Jiahui and Kong, Xiangyu},
  journal={CSEE Journal of Power and Energy Systems}, 
  title={Multi-agent Simulation for Strategic Bidding in Electricity Markets Using Reinforcement Learning}, 
  year={2023},
  volume={9},
  number={3},
  pages={1051-1065},
  abstract={In this paper, a theoretical framework of Multi-agent Simulation (MAS) is proposed for strategic bidding in electricity markets using reinforcement learning, which consists of two parts: one is a MAS system used to simulate the competitive bidding of the actual electricity market; the other is an adaptive learning strategy bidding system used to provide agents with more intelligent bidding strategies. An Experience-Weighted Attraction (EWA) reinforcement learning algorithm (RLA) is applied to the MAS model and a new MAS method is presented for strategic bidding in electricity markets using a new Improved EWA (IEWA). From both qualitative and quantitative perspectives, it is compared with three other MAS methods using the Roth-Erev (RE), Q-learning and EWA. The results show that the performance of the MAS method using IEWA is proved to be better than the others. The four MAS models using four RLAs are built for strategic bidding in electricity markets. Through running the four MAS models, the rationality and correctness of the four MAS methods are verified for strategic bidding in electricity markets using reinforcement learning.},
  keywords={Electricity supply industry;Reinforcement learning;Games;Economics;Heuristic algorithms;Prediction algorithms;Mathematical models;Electricity market;multi-agent simulation;reinforcement learning;strategic bidding},
  doi={10.17775/CSEEJPES.2020.02820},
  ISSN={2096-0042},
  month={May},}@INPROCEEDINGS{9537063,
  author={Muzahid, Abu Jafar Md and Kamarulzaman, Syafiq Fauzi and Rahman, Md Arafatur},
  booktitle={2021 International Conference on Software Engineering & Computer Systems and 4th International Conference on Computational Science and Information Management (ICSECS-ICOCSIM)}, 
  title={Comparison of PPO and SAC Algorithms Towards Decision Making Strategies for Collision Avoidance Among Multiple Autonomous Vehicles}, 
  year={2021},
  volume={},
  number={},
  pages={200-205},
  abstract={Multiple vehicle collision avoidance strategies with safe lane changing strategy for vehicle control using learning base technique are the most crucial concern in autonomous driving system. Statistics shows that the latest autonomous driving systems are usually prone to rear-end collision. Rear-end collisions often result in severe injuries as well as traffic jam and the consequences are much worse for multiple-vehicle collision. Many previous autonomous driving research focused solely on collision avoidance strategies for two consecutive vehicles. This study proposes a centralised control strategy for multiple vehicles using reinforcement learning focused on partner consideration and goal attainment. The system depicted as a group of vehicles are communicate and coordinate each others by a set of rays and maintain a short following move away. In order to address this challenge, a simulation was implemented in the Unity3D game engine and two state-of-the-art RL algorithms PPO (Proximal Policy Optimization) and SAC (Soft Actor-Critic) were trained by an agent using Unity ML-Agents Toolkit. In terms of success rate, performance, training speed and stability two algorithms are comparable. The potency of algorithms has been assessed by the traffic flow (1) change in vehicle speed, (2) differ in the vehicle beginning positions, and (3) switch to next lane. The agent performed similarly at a 91% success rate in PPO or SAC applications},
  keywords={Training;Scientific computing;Computational modeling;Switches;Reinforcement learning;Stability analysis;Collision avoidance;Autonomous driving;Multiple vehicle collision;Robotics;Reinforcement Learning},
  doi={10.1109/ICSECS52883.2021.00043},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10732085,
  author={Tupayachi, Jose and Ferguson, Madelaine Martinez and Li, Xueping},
  booktitle={2024 Annual Modeling and Simulation Conference (ANNSIM)}, 
  title={A Simulation-Based Real-Time Deep Reinforcement Learning Approach for Fighting Wildfires}, 
  year={2024},
  volume={},
  number={},
  pages={1-12},
  abstract={Wildfire incidents are catastrophic events with a high level of uncertainty. Therefore, resource allocation towards its suppression is a challenging task. Our research presents a method for wildfire management that combines digital twins with a real-time deep reinforcement learning (DRL) technique. The detection of wildfires is supported by the use of real-time satellite imagery. The developed digital twin operates under two stages. Initially, a discrete simulation approach is employed to model the behavior of fires, their immediate impact on the population, and the response times of firefighting resources. Then, an Advantage Actor-Critic (A2C) DRL policy optimizes the response under different scenarios accounting for a time-based environment to adjust agent actions. The holistic approach presented in this study strives to synergize and minimize the effect of wildfires over population to create an adaptive and effective wildfire response system.},
  keywords={Wildfires;Adaptive systems;Uncertainty;Scalability;Decision making;Deep reinforcement learning;Real-time systems;Digital twins;Resource management;Time factors;wildfires;deep reinforcement learning;resource allocation;digital twins;real-time decision-making},
  doi={10.23919/ANNSIM61499.2024.10732085},
  ISSN={},
  month={May},}@INPROCEEDINGS{10975741,
  author={Chen, Ruxin and Zhang, Zeqiang},
  booktitle={2025 IEEE Symposium on Computational Intelligence for Financial Engineering and Economics (CiFer)}, 
  title={Deep Reinforcement Learning in Labor Market Simulations}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={This paper proposes a novel framework for applying reinforcement learning (RL) within agent-based models (ABMs) to study labor market dynamics in labor economics. ABMs provide a flexible platform for simulating economic systems, particularly by modelling heterogeneous agents and their complex interactions, which effectively capture non-linear and emergent phenomena in labor markets. We extend an existing labor market model by integrating it into an RL framework, using the Deep Deterministic Policy Gradient (DDPG) algorithm to train agents to maximize profits, and comparing their performance with bounded-rational agents governed by predefined policies. Our findings show that RL agents, depending on the level of competition and rationality in the market, spontaneously learn distinct strategies, which significantly impact outcomes, such as unemployment and wage distribution. This work underscores the importance of ABMs in analyzing labor market dynamics and illustrates how RL-equipped agents can learn optimal strategies in evolving economic environments, offering a robust tool for policy analysis and exploring the complexities of labor market behavior.},
  keywords={Economics;Biological system modeling;Computational modeling;Deep reinforcement learning;Complexity theory;Unemployment;Computational intelligence;Emergent phenomena;reinforcement learning;labor economics;agentbased model},
  doi={10.1109/CiFer64978.2025.10975741},
  ISSN={},
  month={March},}@INPROCEEDINGS{10164609,
  author={Zhao, Maomao and Zhang, Shaojie and Jiang, Bin},
  booktitle={2023 6th International Symposium on Autonomous Systems (ISAS)}, 
  title={Multi-Agent Cooperative Attacker-Defender-Target Task Decision Based on PF-MADDPG}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={A novel potential function multi-agent deep deterministic policy gradient (PF-MADDPG) algorithm is proposed for the multi-agent Attacker-Defender-Target (ADT). A multi-agent continuous state space and a continuous action space are established. The potential function rewards of target and defenders are designed to accelerate the game confrontation training speed, and the MADDPG algorithm is utilized to obtain effective strategies, so as to describe the influence of different actions on attackers. Finally, simulations are given to verify the effectiveness of the proposed PF-MADDPG algorithm.},
  keywords={Training;Autonomous systems;Heuristic algorithms;Clustering algorithms;Games;Task analysis;multi-agent system;PF-MADDPG;Attacker-Defender-Target;potential function},
  doi={10.1109/ISAS59543.2023.10164609},
  ISSN={},
  month={June},}@INPROCEEDINGS{9925730,
  author={Fremond, Rodolphe and Xu, Yan and Inalhan, Gokhan},
  booktitle={2022 IEEE/AIAA 41st Digital Avionics Systems Conference (DASC)}, 
  title={Application of an autonomous multi-agent system using Proximal Policy Optimisation for tactical deconfliction within the urban airspace}, 
  year={2022},
  volume={},
  number={},
  pages={1-10},
  abstract={The present paper formalises the development of a Multi-agent Reinforcement Learning (MARL) solver for U-space Service Providers (USSPs) supporting the tactical conflict resolution and exhibited in the Air Mobility Urban - Large Experimental Demonstration (AMU-LED) project. It relies on an Advantage Actor Critic (A2C) model with a Proximal Policy Optimisation (PPO) learning baseline. The application of the autonomous system is demonstrated under a synthetic (with live and virtual) air/unmanned traffic management (ATM/UTM) environment. The Unmanned Aircraft Systems (UASs) are flying in cruise phase at low altitudes, whose respective flight plan generates intersections for enforcing a high collision frequency. The study adopts a step-wise complexity approach of scenarios that confront two agents’ observation methods and showcases a practical case of tactical conflict resolution. The experiments show encouraging deconfliction performance with promising prospects for seeing a such solver deployed.},
  keywords={Training;Uncertainty;Three-dimensional displays;Scalability;Urban areas;Reinforcement learning;Stakeholders;Urban Air Mobility;Reinforcement Learning;Actor Critic;Multi-Agent System;Proximal Policy Optimization;U-space Service Providers},
  doi={10.1109/DASC55683.2022.9925730},
  ISSN={2155-7209},
  month={Sep.},}@INPROCEEDINGS{9455984,
  author={Hu, Siyi and Zhu, Fengda and Chang, Xiaojun and Liang, Xiaodan},
  booktitle={2021 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)}, 
  title={Transformer Based Multi-Agent Framework}, 
  year={2021},
  volume={},
  number={},
  pages={1-2},
  abstract={We present a Transformer-like agent to learn the policy of multi-agent cooperation tasks, which is a breakthrough for traditional RNN-based multi-agent models that need to be retrained for each task. Our model can handle various input and output with strong transferability and can parallel tackle different tasks. Besides, We are the first to successfully utilize transformer into a recurrent architecture, providing insight on stabilizing transformers in recurrent RL tasks.},
  keywords={Conferences;Task analysis;Multi-agent systems;Multi-agent System;Transfer Learning;Zero-shot Generalization},
  doi={10.1109/ICMEW53276.2021.9455984},
  ISSN={},
  month={July},}@INPROCEEDINGS{10015437,
  author={Malhotra, Kanupriya and Lim, Zhi Jun and Alam, Sameer},
  booktitle={2022 Winter Simulation Conference (WSC)}, 
  title={A Multi-Agent Reinforcement Learning Approach for System-Level Flight Delay Absorption}, 
  year={2022},
  volume={},
  number={},
  pages={406-417},
  abstract={With increasing air traffic, there is an ever-growing need for Air Traffic Controllers (ATCO) to efficiently manage traffic and congestion. Congestion often leads to increased delays in the Terminal Maneuvering Area (TMA), causing large amounts of fuel burn and detrimental environmental impacts. Approaches such as the Extended Arrival Manager (E-AMAN) propose solutions to absorb such delays, whereby flights are scheduled much before they enter the TMA. However, such an approach requires a speed management system where flights can coordinate to absorb system-level delays in their en-route phase. This paper proposes a Multi-Agent System (MAS) approach using Deep Reinforcement Learning to model and train flights as agents which can coordinate with each other to effectively absorb system-level delays. The simulations utilize Multi-Agent POsthumous Credit Assignment in Unity and test two reward approaches. Initial findings reveal an average of 3.3 minutes of system-level delay absorptions from a required delay of 4 minutes.},
  keywords={Deep learning;Absorption;Reinforcement learning;Delays;Fuels;Multi-agent systems},
  doi={10.1109/WSC57314.2022.10015437},
  ISSN={1558-4305},
  month={Dec},}@INPROCEEDINGS{10055893,
  author={Li, Kuo and Liu, Siwei and Jia, Qing-Shan},
  booktitle={2022 China Automation Congress (CAC)}, 
  title={Enabling Inter-Agent Transfer for Multi-Agent Learning System by Incorporating Role Reversal}, 
  year={2022},
  volume={},
  number={},
  pages={5667-5672},
  abstract={This work focuses on inter-agent transfer for multi-agent reinforcement learning. For a learning system consisting of multiple agents that cope with similar situations, incorporating inter-agent transfer could significantly improve the learning efficiency since the trajectories from others also benefit local policy iterations. However, experience sharing is typically not straightforward since the agents may have different sensors and observations. In order to transfer trajectories among agents, we propose to incorporate role reversal, which maps the observations of different agents and augments the experiences by exchanging their viewpoints. In this way, each agent learns from the trials of both itself and others. We further conduct simulations to validate the effectiveness, where the proposed algorithms surpass the baseline algorithms by a remarkable margin in all the situations. Importantly, our method can be integrated into all modern off-policy multi-agent reinforcement learning algorithms to pursue better performance.},
  keywords={Training;Learning systems;Automation;Reinforcement learning;Trajectory;Sensors;Optimization;reinforcement learning;multi-agent system;transfer learning;role reversal},
  doi={10.1109/CAC57257.2022.10055893},
  ISSN={2688-0938},
  month={Nov},}@INPROCEEDINGS{11108355,
  author={Ma, Yiheng and Jiang, Feng and Han, Kun and Zhu, Haiqi and Bie, Xiaofeng},
  booktitle={2025 IEEE International Conference on Pattern Recognition, Machine Vision and Artificial Intelligence (PRMVAI)}, 
  title={Multi-Strategy Distillation Based on CTCE and CEDE}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={This study proposes a multi-agent reinforcement learning algorithm that integrates the Centralized Training with Centralized Execution (CTCE) and Centralized Training with Decentralized Execution (CTDE) frameworks to enhance distributed agents' collaboration efficiency and training performance through customized knowledge distillation strategies. To address the challenges of partial observability and credit assignment in multi - agent systems, we design a distillation framework that employs the Multi - Agent Transformer (MAT) as the teacher model and QMIX as the student model, and incorporate dynamic intervention mechanisms (“Early Guidance”, “On Demand Assistance”, and “Key - Point Emphasis” strategies) to optimize the frequency and intensity of teacher guidance during critical decision - making phases. Additionally, we innovatively introduce distributional reinforcement learning methods. We measure the student model's confidence level through the multipeak concentration index (MCI) to facilitate effective teacher intervention. Experimental results demonstrate that our method significantly improves the success rate and reward values of student models in StarCraft II mini - games, validating the effectiveness of dynamic distillation strategies in balancing global collaboration with local adaptability. This approach provides novel insights for multi - agent coordination in complex tasks.},
  keywords={Training;Adaptation models;Collaboration;Reinforcement learning;Dynamic scheduling;Transformers;Robustness;Indexes;Resource management;Observability;CTCE;CTDE;distributional reinforcement learning;knowledge distillation;multi-agent;multi-peak concentration index;QR-QMIX},
  doi={10.1109/PRMVAI65741.2025.11108355},
  ISSN={},
  month={June},}@ARTICLE{9784819,
  author={Zhang, Weijia and Liu, Hao and Xiong, Hui and Xu, Tong and Wang, Fan and Xin, Haoran and Wu, Hua},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={RLCharge: Imitative Multi-Agent Spatiotemporal Reinforcement Learning for Electric Vehicle Charging Station Recommendation}, 
  year={2023},
  volume={35},
  number={6},
  pages={6290-6304},
  abstract={Electric Vehicle (EV) has become a preferable choice in the modern transportation system due to its environmental and energy sustainability. However, in many large cities, EV drivers often fail to find the proper spots for charging, because of the limited charging infrastructures and the spatiotemporally unbalanced charging demands. Indeed, the recent emergence of deep reinforcement learning provides great potential to improve the charging experience from various aspects over a long-term horizon. In this paper, we propose an Imitative Multi-Agent Spatio-Temporal Reinforcement Learning (RlCharge) framework for intelligently recommending public accessible charging stations by jointly considering various long-term spatio-temporal factors. Specifically, by regarding each charging station as an individual agent, we formulate the problem as a multi-objective multi-agent reinforcement learning task. We first develop a multi-agent actor-critic framework with centralized training decentralized execution. Particularly, we propose a tailor-designed centralized attentive critic to coordinate the recommendation between geo-distributed agents, and introduce a delayed access strategy to exploit the knowledge of future charging competition during centralized training. Moreover, to handle the partial observability problem during decentralized execution in the large-scale multi-agent system, we propose the spatio-temporal heterogeneous graph convolution module, including (1) a dynamic graph convolution block to generate real-time representations for observable forthcoming EVs, and (2) a spatial graph convolution block to share the agent observations by message propagation between spatially adjacent agents. After that, to effectively optimize multiple divergent learning objectives, we extend the centralized attentive critic to multi-critics, and develop a dynamic gradient re-weighting strategy to adaptively guide the optimization direction. In addition, we propose an adaptive imitation learning scheme to further accelerate and stabilize the policy convergence. Finally, extensive experiments on two real-world datasets demonstrate that RlCharge achieves the best comprehensive performance compared with ten baseline approaches.},
  keywords={Charging stations;Task analysis;Convolution;Training;Reinforcement learning;Optimization;Continuous wavelet transforms;Electric vehicle charging station recommendation;multi-agent reinforcement learning;multi-objective optimization;graph neural networks;imitation learning},
  doi={10.1109/TKDE.2022.3178819},
  ISSN={1558-2191},
  month={June},}@INPROCEEDINGS{9917038,
  author={Kelker, Michael and Quakernack, Lars and Haubrock, Jens and Westermann, Dirk},
  booktitle={2022 IEEE Power & Energy Society General Meeting (PESGM)}, 
  title={Multi agent double deep Q-network with multiple reward functions for electric vehicle charge control}, 
  year={2022},
  volume={},
  number={},
  pages={01-05},
  abstract={Even today, electric vehicles (EVs) can endanger grid stability by overloading equipment at the low-voltage (LV) level due to high charging power at private charging points and simultaneity. With a high share ofEVs in the future, it is therefore necessary to control their charging power in such a way that congestion of equipment in the LV grid is avoided. In order to increase the user acceptance of EVs, a fast charging time of the EVs has to be guaranteed despite the control of the charging power. For achieving this, an autonomously acting control algorithm using a multi agent double deep Q-network (MADDQN) has been defined and validated in simulation in the following paper. For this purpose, multiple reward functions have been defined. In the validation on the modified CIGRE LV grid with a share of 100% EVs, it has been shown that the MADDQN can reduce the transformer utilization by up to 50 % relative to the uncontrolled case. At the same time, the charging time can be increased by 51 % relative to the minimum EV charging power of 1.4 kW.},
  keywords={Low voltage;Monte Carlo methods;Power system stability;Charging stations;Transformers;Electric vehicle charging;Stability analysis;Electric vehicle;Low voltage;Reinforcement learning},
  doi={10.1109/PESGM48719.2022.9917038},
  ISSN={1944-9933},
  month={July},}@INPROCEEDINGS{9587482,
  author={Shinde, Manish and Chintawar, Ruturaj and Chavan, Raj and Chatnani, Bhavesh and Giri, Dr. Mrs. Nupur},
  booktitle={2021 2nd Global Conference for Advancement in Technology (GCAT)}, 
  title={Multiple Intersection Traffic Control using Reinforcement Learning}, 
  year={2021},
  volume={},
  number={},
  pages={1-4},
  abstract={This paper introduces application of Multi Agent Deep Deterministic Policy Gradients algorithm for multiple traffic intersection problems. The problem of decrease in waiting time at traffic intersections is still unsolved. Reinforcement learning is the recent technique which was introduced in past years. This paper is an attempt to apply Reinforcement Learning for multiple intersections.},
  keywords={Reinforcement learning;Traffic control;Distributed computing;Reinforcement Learning;Multi Agent Deep Deterministic Policy Gradients;Actor;Critic;Machine Learning},
  doi={10.1109/GCAT52182.2021.9587482},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8430473,
  author={Majumdar, Abhijit and Benavidez, Patrick and Jamshidi, Mo},
  booktitle={2018 World Automation Congress (WAC)}, 
  title={Lightweight Multi Car Dynamic Simulator for Reinforcement Learning}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={With improvements in reinforcement learning algorithms, and the demand to implement these algorithms on real systems, the use of a simulator as an intermediate stage is essential to save time, material and financial resources. The lack of particular features in a unified simulator for applications to autonomous cars and robotics, encouraged this research, which produced a simulator capable of simulating multiple car like objects, in either one or several arenas (environments). Being a lightweight application, multiple instances of the simulator can run at the same time, only constrained by the available computational resources.},
  keywords={Automobiles;Graphical user interfaces;Learning (artificial intelligence);Heuristic algorithms;Computational modeling;Testing;Libraries;Multi-agent systems;Unmanned autonomous vehicles;Simulation;Reinforcement learning;Multiple instances;Multi Programming environments},
  doi={10.23919/WAC.2018.8430473},
  ISSN={},
  month={June},}@INPROCEEDINGS{10408816,
  author={Ren, Chongde and Chen, Jinchao and Du, Chenglie and Yu, Wenquan},
  booktitle={2023 IEEE 11th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)}, 
  title={Path Planning Algorithm for Multiple UAVs Based on Artificial Potential Field}, 
  year={2023},
  volume={11},
  number={},
  pages={970-974},
  abstract={Due to the lower cost and higher maneuverability, unmanned aerial vehicles (UAVs) have found extensive use in both the civilian and military worlds. Path planning, as a crucial problem in the process of UAVs flight, aims to determine the optimal routes for multiple UAVs from various starting points to a single destination. However, because of the involvement of complex conditional constraints, path planning becomes a highly challenging problem. The path planning problem involving numerous UAVs is examined in this research, and a SAAPF-MADDPG algorithm based on Artificial Potential Field (APF) is suggested as a solution. First, a SA-greedy algorithm that can change the probability of random exploration by agents based on the number of steps and successful rounds to prevent UAVs from getting trapped in a local optimum. Then, we design complex reward functions based on APF to guide UAVs to destination faster. Finally, SAAPF-MADDPG is evaluated against the MADDPG, DDPG, and MATD3 methods in simulation scenarios to confirm its efficacy.},
  keywords={Training;Navigation;Neural networks;Path planning;Stability analysis;Planning;Information technology;autonomous navigation;deep reinforcement learning;multi-agent systems;path planning},
  doi={10.1109/ITAIC58329.2023.10408816},
  ISSN={2693-2865},
  month={Dec},}@INPROCEEDINGS{11196621,
  author={Devadasu, G. and Nagarjuna, T. and Sarika, Sabavath and Balassem, Zayd and S, Dyana. and Anakath, A.S.},
  booktitle={2025 International Conference on Metaverse and Current Trends in Computing (ICMCTC)}, 
  title={Enhancing Crisis Management in Smart Cities: A Multi-Agent Deep Learning Approach for redictive and Adaptive Urban Resilience}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={With the rapid growth of urban population and ever more complex city infrastructure, urban crisis management becomes a great challenge to manage from natural disasters, traffic accidents and public health emergencies. Current crisis management systems typically involve an unclear central decision making and take too long to respond and do not use resources efficiently. In order to tackle these problems, we present an original solution based on self adaptive smart cities which implement adaptive crisis management capabilities through the use of Multi Agent Deep Learning (MADL). Through deep reinforcement learning, deep autonomous agents monitoring different sectors of an urban area such as transportation, healthcare, law enforcement, in a diverse real-time, this system adapts to and predicts potential crises before it reaches full bloom. In addition to that, every agent works in synergy with each other to ensure that the resources are relocated optimally and conflicts are resolved during emergencies. The system is capable to dynamically adjust its response strategies by integrating real time data from the IoT devices, social media as well as citizen input. Besides, data is secured and transparent during interaction between them using blockchain technology. Predictive crisis detection, proactive resource management and personalized citizen alerts are the string of proposed solution that converts the urban environment into more adaptive resilient ecosystems. This is ultimately a sustainable approach for the crisis management issue that enables the efficiency, acceleration of response time, and urban safety support, as well as long term adaptability and resilience in smart cities.},
  keywords={Crisis management;Smart cities;Transportation;Deep reinforcement learning;Real-time systems;Autonomous agents;Blockchains;Time factors;Resource management;Resilience;MADL;Blockchain;Predictive Crisis;Machine Learning},
  doi={10.1109/ICMCTC62214.2025.11196621},
  ISSN={},
  month={April},}@INPROCEEDINGS{9255795,
  author={Bakakeu, Jupiter and Kisskalt, Dominik and Franke, Joerg and Baer, Shirin and Klos, Hans-Henning and Peschke, Joern},
  booktitle={2020 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)}, 
  title={Multi-Agent Reinforcement Learning for the Energy Optimization of Cyber-Physical Production Systems}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  abstract={The paper proposes an artificial intelligence-based solution for the efficient operation of a heterogeneous cluster of flexible manufacturing machines with energy generation and storage capabilities in an electricity micro-grid featuring high volatility of electricity prices. The problem of finding the optimal control policy is first formulated as a game-theoretic sequential decision-making problem under uncertainty, where at every time step the uncertainty is characterized by future weather-dependent energy prices, high demand fluctuation, as well as random unexpected disturbances on the factory floor. Because of the parallel interaction of the machines with the grid, the local viewpoints of an agent are non-stationary and non-Markovian. Therefore, traditional methods such as standard reinforcement learning approaches that learn a specialized policy for a single machine are not applicable. To address this problem, we propose a multi-agent actor-critic method that takes into account the policies of other participants to achieve explicit coordination between a large numbers of actors. We show the strength of our approach in mixed cooperative and competitive scenarios where different production machines were able to discover different coordination strategies in order to increase the energy efficiency of the whole factory floor.},
  keywords={Optimization;Reinforcement learning;Training;Production facilities;Uncertainty;Renewable energy sources;Task analysis;Flexible Manufacturing System;Load Management;Reinforcement Learning;Proximal Policy Optimization;Actor-Critic;Multi-Agent System;Industrie 4.0;Autocurricula},
  doi={10.1109/CCECE47787.2020.9255795},
  ISSN={2576-7046},
  month={Aug},}@INPROCEEDINGS{11149024,
  author={Zhang, Tianyi and Hu, Yaoguang},
  booktitle={2025 IEEE 20th Conference on Industrial Electronics and Applications (ICIEA)}, 
  title={Multi-Agent reinforcement learning for dynamic flexible job shop scheduling with fuzzy transportation time}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Self-organizing manufacturing systems (SOMS) represent an advanced type of flexible manufacturing system, which introduces variability in material transportation time. Previous research has predominantly treated logistics transportation and maintenance as separate concerns in the flexible job shop problem, often overlooking the impact of uncertain transportation times. This oversight can compromise simulation accuracy and increase tardiness of jobs. To address these challenges, this paper proposes a dynamic flexible job shop scheduling method that integrates machine reliability with transportation time uncertainty in SOMS. The proposed approach formulates a partially observable Markov decision process, incorporating both machine reliability and logistical constraints to minimize average weighted tardiness of jobs. Additionally, the multi-agent soft actor-critic algorithm is employed to improve adaptability to transportation fluctuations. Simulation experiments demonstrate that the approach significantly reduces average tardiness, achieving a 50% reduction compared to rule-based approaches and a 27% reduction relative to reinforcement learning-based methods.},
  keywords={Job shop scheduling;Fluctuations;Uncertainty;Remotely guided vehicles;Transportation;Dynamic scheduling;Numerical simulation;Vehicle dynamics;Optimization;Manufacturing systems;dynamic flexible job shop scheduling;deep reinforcement learning;multi-agent system;self-organizing manufacturing system},
  doi={10.1109/ICIEA65512.2025.11149024},
  ISSN={2158-2297},
  month={Aug},}@INPROCEEDINGS{9838553,
  author={Liu, Jieyan and Liu, Yi and Du, Zhekai and Lu, Ke},
  booktitle={ICC 2022 - IEEE International Conference on Communications}, 
  title={Towards Distributed Communication and Control in Real-World Multi-Agent Reinforcement Learning}, 
  year={2022},
  volume={},
  number={},
  pages={4974-4979},
  abstract={Multi-agent system investigates the problem of designing a complex system composed of multiple autonomous agents with limited ability and partial observability. As a milestone, AlphaStar has achieved remarkable success in StarCraft II, which is a significant breakthrough in the competitive environments with complex strategic spaces and real-time decisions. However, it poses new challenges for deploying these centralized control models in real-world environments because many of them in such competitive environments were not designed to accommodate the requirements of real-world communication networks, e.g., the problems of high latency and large traffic are inevitable when they are actually deployed. To alleviate this issue, we propose a distributed control paradigm that explicitly splits the control power between the centralized meta-agent and agent units through a combination of centralized and decentralized paradigms. The units can autonomously decide to follow the decisions of the meta-agent or adapt to environment variations immediately by themselves in a decentralized manner. We simulate real-world network environments based on the Mininet platform, experiments based on the StarCraft II Learning Environment (SC2LE) show that our approach achieves a better adaptation in real-world network environments.},
  keywords={Adaptation models;Decentralized control;Reinforcement learning;Games;Real-time systems;Communication networks;Observability},
  doi={10.1109/ICC45855.2022.9838553},
  ISSN={1938-1883},
  month={May},}@INPROCEEDINGS{10662338,
  author={Chen, Hao-Xiang and Zhang, Xi-Wen and Shen, Jun-Nan},
  booktitle={2024 43rd Chinese Control Conference (CCC)}, 
  title={Graph-based Selection-Activation Reinforcement Learning for Heterogenous Multi-agent Collaboration}, 
  year={2024},
  volume={},
  number={},
  pages={5835-5840},
  abstract={In practical applications of multi-agent systems, agents are often heterogeneous, and each type of them typically has different task objectives. For heterogeneous multi-agent reinforcement learning (HMARL), the diversity of agent types and the unbalanced agent number of each type can lead to the curse of dimensionality and non-stationary. Moreover, the increase in the number of heterogeneous agents may result in slow convergence during training. This paper proposes a graph-based selection-activation reinforcement learning (GSARL) method for training heterogenous multi-agent collaboration strategies. It first constructs agents based on their types, then extracts the global adjacency matrices and the ally adjacency matrices from the agents’ observations, and calculates the global feature matrices. Afterwards, GSARL utilizes hierarchical graph convolutional network to sequentially convolve the global information and ally information, obtaining action logits based on agent types. By using the neural topology graph and the selection-activation method, the optimal multi-agent collaboration configuration is obtained through combinatorial optimization. Experiments are conducted in an adversarial combat simulation environment involving collaborative Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs). Simulation results show that the proposed method can accelerate convergence while allowing that each type of heterogeneous agents can leverage its unique advantages.},
  keywords={Training;Simulation;Scalability;Collaboration;Reinforcement learning;Feature extraction;Topology;Multi-Agent System;Hierarchical Graph Convolutional Network;Heterogenous Multi-Agent Collaboration},
  doi={10.23919/CCC63176.2024.10662338},
  ISSN={1934-1768},
  month={July},}@INPROCEEDINGS{11108669,
  author={Chen, Yifan and Duan, Minghan and Jiang, Feng and Han, Kun and Zhu, Haiqi and Bie, Xiaofeng},
  booktitle={2025 IEEE International Conference on Pattern Recognition, Machine Vision and Artificial Intelligence (PRMVAI)}, 
  title={MAIDTB: Multi-Agent Intelligent Dynamic Trajectory Backtrack for Sparse Strategic Games}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Multi-agent reinforcement learning (MARL) in sparse strategic games, characterized by intelligent dynamic targets and infrequent feedback, poses significant learning challenges. This paper introduces the Multi-agent Intelligent Dynamic Trajectory Backtrack (MAIDTB) framework, a novel method to augment MARL algorithms for these complex scenarios. MAIDTB employs Bayesian inverse models to predict strategic target behaviors, enabling the generation of valuable ‘imagined’ trajectories from failed attempts. A Bayesian Neural Network (BNN) is integral to this process, assessing the confidence of inverse model predictions to dynamically control trajectory inference step length. This approach enhances learning robustness by mitigating error accumulation from uncertain predictions. Comparative experiments demonstrate that MARL agents augmented with MAIDTB achieve significantly higher learning efficiency and task success rates in sparse strategic games compared to both baseline MARL algorithms and those augmented with other existing trajectory replay techniques.},
  keywords={Inverse problems;Heuristic algorithms;Games;Reinforcement learning;Predictive models;Prediction algorithms;Robustness;Inference algorithms;Trajectory;Bayes methods;multi-agent system;multi-agent learning;reinforcement learning;dynamic goal},
  doi={10.1109/PRMVAI65741.2025.11108669},
  ISSN={},
  month={June},}@INPROCEEDINGS{9905866,
  author={Ikeda, Takuma and Shibuya, Takeshi},
  booktitle={2022 61st Annual Conference of the Society of Instrument and Control Engineers (SICE)}, 
  title={Centralized Training with Decentralized Execution Reinforcement Learning for Cooperative Multi-agent Systems with Communication Delay}, 
  year={2022},
  volume={},
  number={},
  pages={135-140},
  abstract={In cooperative multi-agent systems, efficient coordination among agents is important when accomplishing tasks. VFFAC is a method that learns the communication system between agents and their interactions with the environment to obtain policies with high performance. However, this method results in decreased performance of policy in environments with a delay in communication. Furthermore, there is no formulation of the control problem of a cooperative multi-agent system with communication delays in unknown environments. In this study, we formulated a decision-making problem in a cooperative multi-agent system with an unknown environment model and a certain length delay in communication. We also propose a method to handle communication delays by using the history of information obtained through communication. We demonstrated that the proposed method successfully learns policy with high rewards through simulated experiments in an environment with a communication delay.},
  keywords={Training;Communication systems;Instruments;Decision making;Reinforcement learning;Delays;History;Reinforcement Learning;Delayed Communication;Cooperative Multi-agent System},
  doi={10.23919/SICE56594.2022.9905866},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9632323,
  author={Ebell, Niklas and Pruckner, Marco},
  booktitle={2021 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)}, 
  title={Benchmarking a Decentralized Reinforcement Learning Control Strategy for an Energy Community}, 
  year={2021},
  volume={},
  number={},
  pages={385-390},
  abstract={The energy transition towards a more sustainable, secure and affordable electrical power system consisting of high shares of renewable energy sources increases the energy system's complexity. It creates an energy system in a more decentralized pattern with many more stakeholders involved. In this context, new data-driven operation control strategies play an important role in order to provide fast decision support and a better coordination of electrical assets in the distribution grid. In this paper, we evaluate a novel Multi-Agent Reinforcement Learning approach which focuses on cooperative agents with only local state information and aim to balance the electricity generation and consumption of an energy community consisting of ten households. This approach is compared to a rule-based and an optimal control policy. Results show that independent Q-learner achieve performance 35 % better than rule-based control and compensate high computational effort with adaptability, simplicity in communication requirements and respect of data-privacy.},
  keywords={Computers;Renewable energy sources;Conferences;Optimal control;Reinforcement learning;Benchmark testing;Smart grids;Reinforcement Learning;Smart Grid;Decentralized Learning;Multi-Agent System;Prosumer;Control;Microgrid;Distributed Energy Resources;Optimization},
  doi={10.1109/SmartGridComm51999.2021.9632323},
  ISSN={},
  month={Oct},}@ARTICLE{10623476,
  author={Chen, Jian and Zhao, Jian-Yin and Zhao, Wen-Fei and Qin, Yu-Feng and Ji, Hong},
  journal={IEEE Access}, 
  title={Robust Multi-Agent Communication via Diffusion-Based Message Denoising}, 
  year={2024},
  volume={12},
  number={},
  pages={170437-170450},
  abstract={Multi-agent communication allows agents to share local information or their own intention with other agents, thus enhancing the collaborative performance of the multi-agent system. Despite its significance, previous multi-agent communication methods typically assume agents communicate in a perfect, interference-free environment without any communication disturbance. However, in real-world scenarios, communication within multi-agent systems often faces environmental noise, and even in adversarial settings, agents may encounter communication attacks from adversaries. How to maintain the collaborative stability within the multi-agent system under communication attack remains crucial yet inadequately researched. In this paper, we build a robust communication mechanism,  $DM^{2}$  (diffusion model to denoise the disturbed messages), which utilizes the denoising process of diffusion models to recover the original communicated messages, thus preventing the influence of communication interference. Besides, to reduce the computational burden brought by diffusion models, we design a detection module to recognize when the messages have been distorted, thus adopting the diffusion model only when necessary. The experiments show that, with this diffusion-based message denoising mechanism, our approach shows significantly superior communication robustness than existing baselines. Our approach exhibits excellent communication robustness in the presence of various types and different degrees of noise attacks. Visualization experiments and ablation studies validate the effectiveness of our diffusion model in recovering the original messages.},
  keywords={Diffusion models;Training;Noise reduction;Robustness;Reinforcement learning;Perturbation methods;Multi-agent systems;Multi-agent reinforcement learning;multi-agent communication;communication robustness;diffusion model},
  doi={10.1109/ACCESS.2024.3438803},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9385538,
  author={Le Gléau, Tangui and Marjou, Xavier and Lemlouma, Tayeb and Radier, Benoit},
  booktitle={2021 24th Conference on Innovation in Clouds, Internet and Networks and Workshops (ICIN)}, 
  title={A Multi-agent OpenAI Gym Environment for Telecom Providers Cooperation}, 
  year={2021},
  volume={},
  number={},
  pages={28-32},
  abstract={The ever-increasing use of the Internet (streaming, Internet of things, etc.) constantly demands more connectivity, which incentivises telecommunications providers to collaborate by sharing resources to collectively increase the quality of service without deploying more infrastructure. However, to the best of our knowledge, there is no tool for testing and evaluating participation strategies in such collaborations. This article presents a new adaptable framework, based on the OpenAI Gym toolkit, allowing to generate customisable environments for cooperating on radio resources. This framework facilitates the development and comparison of agents (such as reinforcement learning agents) in a generic way. The main goal of the paper is to detail the available functionalities of our framework. We then focus on game theory aspects as multi-player games induced by these environments can be considered as sequential social dilemmas. We show in particular that although each agent has no incentive to remain cooperative at each step of such iterated games, a mutual cooperation provides better outcomes (in other words, Nash Equilibrium is non optimal)},
  keywords={Technological innovation;Quality of service;Games;Reinforcement learning;Tools;Telecommunications;Testing;Multi-agent system;simulation frameworks;RAN sharing;Game Theory},
  doi={10.1109/ICIN51074.2021.9385538},
  ISSN={2472-8144},
  month={March},}@INPROCEEDINGS{10333236,
  author={Nipu, Ayesha Siddika and Liu, Siming and Harris, Anthony},
  booktitle={2023 IEEE Conference on Games (CoG)}, 
  title={Enabling Multi-Agent Transfer Reinforcement Learning via Scenario Independent Representation}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Multi-Agent Reinforcement Learning (MARL) algorithms are widely adopted in tackling complex tasks that require collaboration and competition among agents in dynamic Multi-Agent Systems (MAS). However, learning such tasks from scratch is arduous and may not always be feasible, particularly for MASs with a large number of interactive agents due to the extensive sample complexity. Therefore, reusing knowledge gained from past experiences or other agents could efficiently accelerate the learning process and upscale MARL algorithms. In this study, we introduce a novel framework that enables transfer learning for MARL through unifying various state spaces into fixed-size inputs that allow one unified deep-learning policy viable in different scenarios within a MAS. We evaluated our approach in a range of scenarios within the StarCraft Multi-Agent Challenge (SMAC) environment, and the findings show significant enhancements in multi-agent learning performance using maneuvering skills learned from other scenarios compared to agents learning from scratch. Furthermore, we adopted Curriculum Transfer Learning (CTL), enabling our deep learning policy to progressively acquire knowledge and skills across pre-designed homogeneous learning scenarios organized by difficulty levels. This process promotes inter- and intra-agent knowledge transfer, leading to high multi-agent learning performance in more complicated heterogeneous scenarios.},
  keywords={Deep learning;Heuristic algorithms;Transfer learning;Collaboration;Reinforcement learning;Games;Complexity theory;Deep reinforcement learning;multi-agent system;transfer learning;curriculum learning;StarCraft II},
  doi={10.1109/CoG57401.2023.10333236},
  ISSN={2325-4289},
  month={Aug},}@INPROCEEDINGS{9728866,
  author={Park, Yong Hee and Choi, Seong Gon},
  booktitle={2022 24th International Conference on Advanced Communication Technology (ICACT)}, 
  title={Reinforcement Learning base DR Method for ESS SoC Optimization and Users Satisfaction}, 
  year={2022},
  volume={},
  number={},
  pages={160-166},
  abstract={We proposed a Demand Response (DR) method to optimize Energy Storage System (ESS) State of Charge (SoC) and prevent user satisfaction decrease using Reinforcement Learning (RL). ESS SoC should be managed as an optimal value for reasons such as peak load responsiveness and battery life. However, the existing method for ESS SoC optimization assumed a fixed environment. In the actual environment, each factor, such as the departure time of the vehicle, is variable. Therefore, there is a need for a DR plan that can adapt to the environment. In addition, since DR reduces supply power for user, there is a problem of decrease user satisfaction. We aim to learn ESS SoC optimization, shifted load minimization, and optimal management of EV SoC. To this end, we formulate each element for RL and design environment to simulate. In the simulation results, we found the optimal policy.},
  keywords={Simulation;Reinforcement learning;Minimization;Demand response;Communications technology;Smart grids;Batteries;Smart Grid;Demand Response;ESS SoC;Peak Shaving;Reinforcement Learning},
  doi={10.23919/ICACT53585.2022.9728866},
  ISSN={1738-9445},
  month={Feb},}@INPROCEEDINGS{11050708,
  author={Somvanshi, Shriyank and Liu, Jinli and Das, Subasish},
  booktitle={2025 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={A Survey on Generative AI in Transportation Systems Management and Operation}, 
  year={2025},
  volume={},
  number={},
  pages={829-832},
  abstract={The integration of Generative AI (Gen-AI) into Transportation Systems Management and Operations (TSMO) offers transformative potential to address persistent knowledge management challenges. This survey study explores how Gen-AI can revolutionize TSMO through enhanced data integration, automated knowledge extraction, and predictive modeling. By synthesizing diverse datasets, Gen-AI facilitates the creation of unified knowledge repositories, improves real-time decision-making, and supports proactive scenario planning. The study introduces a comprehensive framework for embedding Gen-AI into TSMO workflows, enabling streamlined operational efficiency, cross-agency collaboration, and scalable data-driven strategies. Key applications of Gen-AI in TSMO include adaptive traffic control, crash response planning, and predictive modeling for future traffic scenarios. The study highlights innovative Gen-AI models and techniques, such as generative adversarial networks (GANs), large language models (LLMs), and hybrid augmented intelligence frameworks, which collectively enhance TSMO's capacity for resilience and responsiveness. Despite its transformative potential, Gen-AI adoption faces critical challenges, including ethical considerations, computational constraints, and the need for stakeholder trust. The paper emphasizes the importance of responsible AI development, fairness, and explainability to ensure sustainable adoption. By addressing these barriers, the proposed framework sets the stage for a new era of intelligent, adaptive, and community-centered transportation system management,},
  keywords={Surveys;Adaptation models;Computational modeling;Decision making;Transportation;Data integration;Predictive models;Traffic control;Planning;Stakeholders;Gen-AI;TSMO;Transportation Engineering;AI;Survey},
  doi={10.1109/CAI64502.2025.00148},
  ISSN={},
  month={May},}@INPROCEEDINGS{10935070,
  author={Hossini, Anas and Kloul, Leïla and Guiraud, Maël and Boulakia, Benjamin Cohen},
  booktitle={2025 Annual Reliability and Maintainability Symposium (RAMS)}, 
  title={Predictive Maintenance for Smart Buildings: Balancing QoS and Cost Efficiency}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={The rapid advancement of sensing technologies and connectivity has revolutionized predictive maintenance (PdM), particularly for smart buildings (SB). Despite these advancements, implementing data-driven approaches faces challenges, mainly due to scarce failure data and the SB systems complexity. In this paper, we propose a cooperative multi-agent reinforcement learning (RL) based approach to address these challenges in which each agent is responsible for maintaining one subsystem. As a case study, we consider three subsystems within the SB: lighting, network, and IT. The network subsystem ensures power supply, while the IT subsystem controls and shares data. The proposed approach enables the design of comprehensive maintenance decision processes that minimize maintenance costs for these subsystems while satisfying their quality of service (QoS) standards. Our approach's advantage lies in its ability to ensure the effective operation of each SB subsystem and the SB itself, even with limited data. Moreover, the proposed approach is adaptative according to the operating conditions of each SB subsystem.},
  keywords={Smart buildings;Costs;Random access memory;Quality of service;Reinforcement learning;Maintenance;Sensors;Standards;Predictive maintenance;System of systems;Smart Building;System of Systems;Predictive Maintenance;Reinforcement Learning},
  doi={10.1109/RAMS48127.2025.10935070},
  ISSN={2577-0993},
  month={Jan},}@INPROCEEDINGS{10619346,
  author={Ziqing, L. and Liu, Guanlin and Lai, Lifeng and Weiyu, X.},
  booktitle={2024 IEEE International Symposium on Information Theory (ISIT)}, 
  title={Camouflage Adversarial Attacks on Multiple Agent Systems}, 
  year={2024},
  volume={},
  number={},
  pages={7-12},
  abstract={The multi-agent reinforcement learning systems (MARL) based on the Markov decision process (MDP) have emerged in many critical applications. To improve the robust-ness/defense of MARL systems against adversarial attacks, the study of various adversarial attacks on reinforcement learning systems is very important. Previous works on adversarial attacks considered some possible features to attack in MDP, such as the action poisoning attacks, the reward poisoning attacks, and the state perception attacks. In this paper, we propose a brand-new form of attack called the camouflage attack in the MARL systems. In the camouflage attack, the attackers change the appearances of some objects without changing the actual objects themselves; and the camouflaged appearances may look the same to all the targeted recipient (victim) agents. The camouflaged appearances can mislead the recipient agents to misguided actions. We design algorithms that give the optimal camouflage attacks minimizing the rewards of recipient agents. Our numerical and theoretical results show that camouflage attacks can rival the more con-ventional, but likely more difficult state perception attacks. We also investigate cost-constrained camouflage attacks and showed numerically how cost budgets affect the attack performance.},
  keywords={Costs;Markov decision processes;Reinforcement learning;Information theory},
  doi={10.1109/ISIT57864.2024.10619346},
  ISSN={2157-8117},
  month={July},}@ARTICLE{11006503,
  author={Hu, Tianyi and Pu, Zhiqiang and Ai, Xiaolin and Qiu, Tenghai and Liang, Yanyan and Yi, Jianqiang},
  journal={IEEE Transactions on Cognitive and Developmental Systems}, 
  title={Hybrid Actor-Critic for Physically Heterogeneous Multi-Agent Reinforcement Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1-15},
  abstract={This paper focuses on cooperative policy learning for physically heterogeneous multi-agent system (PHet-MAS), where agents have different observation spaces, action spaces and local state transitions. Due to the various input-output structures of agents’ policies in PHet-MAS, it’s difficult to employ parameter sharing techniques for sample efficiency. Moreover, a totally heterogeneous policy design impedes agents from utilizing the training experience of their companions, and increases the risk of environmental non-stationarity. To address the above issues, we propose hybrid heterogeneous actor-critic (HHAC), a method for the policy learning of PHet-MAS. The framework of HHAC consists of a hybrid actor and a hybrid critic, both containing globally shared and locally shared modules. The locally shared modules can be customized according to the actual physical properties of agents, while the globally shared modules can help extract and utilize the common information among agents. In the hybrid critic, a behavioral intention module is designed to alleviate the environmental non-stationary issue caused by evolving heterogeneous policies. Finally, a hybrid network training method is developed to address challenges in sample construction and training stability of hybrid networks. As evidenced by experimental results, HHAC exhibits superior performance enhancements over baseline approaches, and can facilitate PHet-MAS in learning sophisticated and instructive policies.},
  keywords={Training;Multi-agent systems;Data mining;Vectors;Redundancy;Mathematical models;Markov decision processes;Learning systems;Hands;Focusing;Multi-agent systems;deep reinforcement learning;heterogeneity},
  doi={10.1109/TCDS.2025.3570497},
  ISSN={2379-8939},
  month={},}@INPROCEEDINGS{10650164,
  author={Shuvo, Shaon Bhatta and Das, Jyoti and Kobti, Ziad and Kar, Narayan},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Advancing Pandemic Preparedness through a Data-Driven Hybrid Simulation Model}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={The rise of new disease variants, such as COVID-19, influenza, and others, highlights the critical need for advanced epidemiological modeling to guide early-stage outbreak management, especially when vaccine options are not available or reliable. This paper presents a novel, hybrid, data-driven model that integrates Agent-Based Modeling (ABM) with an extended SEIHRD (Susceptible, Exposed, Infectious, Hospitalized, Recovered, and Dead) framework, enhanced by N-step Deep Q Reinforcement Learning (N-Step DQRL). This model merges ABM’s behavioral insights with the SEIHRD model’s progression dynamics, utilizing DQRL for adaptive, data-informed decision-making. It is particularly focused on enhancing non-pharmaceutical interventions, such as lockdown policies, which are crucial in managing outbreaks in the absence of vaccines. This approach strikes a balance between detailed analysis and scalability, vital for policymakers in responding to emerging disease variants. The model’s efficacy, as evidenced by an analysis of recent COVID-19 data, highlights its potential to significantly improve global pandemic preparedness and response, merging behavioral analysis with disease progression trends through the use of advanced deep learning techniques.},
  keywords={COVID-19;Adaptation models;Pandemics;Scalability;Reinforcement learning;Data models;Robustness;agent-based modeling;compartmental model;deep learning;reinforcement learning;optimization},
  doi={10.1109/IJCNN60899.2024.10650164},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{11107498,
  author={Fiscko, Carmel and Yin, Haoyu and Sinopoli, Bruno},
  booktitle={2025 American Control Conference (ACC)}, 
  title={Hierarchical MARL with Stackelberg Games}, 
  year={2025},
  volume={},
  number={},
  pages={4115-4122},
  abstract={We consider a multi-agent system under the control of a central planner (CP). We model the system as a hierarchical multi-agent reinforcement learning (MARL) problem in which a state process is influenced by both the CP’s control and the agents’ actions. Each agent learns a local policy to maximize a local value function, which may or may not be aligned with the CP’s control objective. The CP’s goal is therefore to find a global policy such that the agents learn an equilibrium joint policy that maximizes the CP’s value function. We first show that this problem is equivalent to a Stackelberg game. Given a model, this equivalence can be used to derive game-theoretic properties about the desired Stackelberg equilibrium and can be used to solve for optimal policies directly. If the game model is unknown, then we propose a Monte Carlo (MC)-based reinforcement learning (RL) method based on the hierarchical game structure. We demonstrate that under standard RL assumptions, this method can approximate solutions to the desired Stackelberg game. This procedure is validated in simulations on synthetic games resembling social welfare problems.},
  keywords={Monte Carlo methods;Process control;Optimal control;Finance;Estimation;Games;Reinforcement learning;Standards;Multi-agent systems;Convergence},
  doi={10.23919/ACC63710.2025.11107498},
  ISSN={2378-5861},
  month={July},}@INPROCEEDINGS{10811454,
  author={Jin, Jiayue and Qian, Lang and Song, Liang},
  booktitle={2024 IEEE 10th World Forum on Internet of Things (WF-IoT)}, 
  title={Multi-agent Framework Based on Coordinated Sensing and Control with Online Evolutive Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The integration of Internet of Things (IoT) and Artificial Intelligence (AI) technologies has emerged as a pivotal research focus, fostering the development of diverse intelligent systems. These systems usually require autonomous decision-making and real-time feedback, and the multi-agent system (MAS) is one of the main paradigms for realizing such systems. Within MAS, collaborative perception and control are key components. Although existing research has made progress in these two aspects, most of them study the multi-agent perception and control algorithms separately in MAS, which may be insufficient when facing the non-stationary environment. In this paper, we propose a multi-agent coordinated sensing and control framework based on online evolutive learning. Specifically, we present a framework consisting of multiple sensors and controllers, where the sensors are based on target detection models, and the controllers are based on multi-agent reinforcement learning algorithms. Coordinated optimization from the controllers to the sensors, along with a generative model providing prior knowledge, enables agents to achieve online evolutive learning. To validate the effectiveness of the proposed framework, we construct a multi-agent pathfinding simulation environment, and the experiments demonstrate both the superiority and the necessity of our approach.},
  keywords={Heuristic algorithms;Reinforcement learning;Object detection;Real-time systems;Sensors;Safety;Internet of Things;Reliability;Optimization;Multi-agent systems;Online Evolutive Learning;Multi-agent System;Sensing and Control;Supervised Learning;Reinforcement Learning},
  doi={10.1109/WF-IoT62078.2024.10811454},
  ISSN={2768-1734},
  month={Nov},}@ARTICLE{9716772,
  author={Pu, Zhiqiang and Wang, Huimu and Liu, Zhen and Yi, Jianqiang and Wu, Shiguang},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Attention Enhanced Reinforcement Learning for Multi agent Cooperation}, 
  year={2023},
  volume={34},
  number={11},
  pages={8235-8249},
  abstract={In this article, a novel method, called attention enhanced reinforcement learning (AERL), is proposed to address issues including complex interaction, limited communication range, and time-varying communication topology for multi agent cooperation. AERL includes a communication enhanced network (CEN), a graph spatiotemporal long short-term memory network (GST-LSTM), and parameters sharing multi-pseudo critic proximal policy optimization (PS-MPC-PPO). Specifically, CEN based on graph attention mechanism is designed to enlarge the agents’ communication range and to deal with complex interaction among the agents. GST-LSTM, which replaces the standard fully connected (FC) operator in LSTM with graph attention operator, is designed to capture the temporal dependence while maintaining the spatial structure learned by CEN. PS-MPC-PPO, which extends proximal policy optimization (PPO) in multi agent systems with parameters’ sharing to scale to environments with a large number of agents in training, is designed with multi-pseudo critics to mitigate the bias problem in training and accelerate the convergence process. Simulation results for three groups of representative scenarios including formation control, group containment, and predator–prey games demonstrate the effectiveness and robustness of AERL.},
  keywords={Training;Reinforcement learning;Games;Scalability;Task analysis;Standards;Optimization;Attention mechanism;deep reinforcement learning (DRL);graph convolutional networks;multi agent systems},
  doi={10.1109/TNNLS.2022.3146858},
  ISSN={2162-2388},
  month={Nov},}@INPROCEEDINGS{8430409,
  author={Majumdar, Abhijit and Benavidez, Patrick and Jamshidi, Mo},
  booktitle={2018 World Automation Congress (WAC)}, 
  title={Multi-Agent Exploration for Faster and Reliable Deep Q-Learning Convergence in Reinforcement Learning}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={Function approximation based Q-learning, using deep q-learning has had recent extraordinary developments applicable to generalized applications. Many techniques have been introduced to counter the inherent caveats in using a deep neural network in reinforcement learning. We demonstrate the use of multi-agent virtual exploration integrated into existing algorithms to show better convergence property, and show how they can be applied as extensions to provide faster and better converged values.},
  keywords={Neural networks;Learning (artificial intelligence);Convergence;Mathematical model;Training;Optimization;Approximation algorithms;Reinforcement Learning;Q-learning;DQN;Parallel processing;Unmanned autonomous vehicles;Simulation;Multi-agent;Exploration},
  doi={10.23919/WAC.2018.8430409},
  ISSN={},
  month={June},}@INPROCEEDINGS{10881535,
  author={Umar, Sani and Mohammed, Shahmir Khan and Alkaabi, Nouf and Muhaidat, Sami},
  booktitle={2024 IEEE Middle East Conference on Communications and Networking (MECOM)}, 
  title={Autonomous Mobile Inspection of Process Parameters using Reinforcement Learning in Industrial IoT Environment}, 
  year={2024},
  volume={},
  number={},
  pages={241-246},
  abstract={Due to their error-prone and time-consuming nature, conventional manual inspection techniques are inadequate in the Industrial Internet of Things (IIoT) environment. This paper presents an approach to resolve these challenges by applying Reinforcement Learning (RL) for autonomous mobile inspection of the industrial process parameters. Our system optimizes the navigation and inspection strategies of mobile robots in industrial settings with the Deep Deterministic Policy Gradient (DDPG) algorithm. The mobile robot is equipped with sensors to inspect parameters and support decisions on where the most important inspections need to be conducted. Our method provides improved navigation efficiency, avoid human interventions, and increases overall productivity. This paper highlights the potential of RL to revolutionize industrial inspections while ensuring optimal performance and safety. Simulation results show the validation of the proposed approach concerning autonomous navigation in the industrial environment, reaching target locations, and successful inspection of the industrial parameters. video of the test cases demonstrating the system can be found in this link: [https://youtu.be/XG2ivEbs5jc]},
  keywords={Productivity;Navigation;Service robots;Simulation;Reinforcement learning;Inspection;Safety;Sensors;Mobile robots;Industrial Internet of Things;Reinforcement Learning;Autonomous mobile robots;Industrial processes;IIoT;DDPG},
  doi={10.1109/MECOM61498.2024.10881535},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9893711,
  author={Nipu, Ayesha Siddika and Liu, Siming and Harris, Anthony},
  booktitle={2022 IEEE Conference on Games (CoG)}, 
  title={MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement Learning}, 
  year={2022},
  volume={},
  number={},
  pages={512-515},
  abstract={Distributed decision-making in multi-agent systems presents difficult challenges for interactive behavior learning in both cooperative and competitive systems. To mitigate this complexity, MAIDRL presents a semi-centralized Dense Reinforcement Learning algorithm enhanced by agent influence maps (AIMs), for learning effective multi-agent control on StarCraft Multi-Agent Challenge (SMAC) scenarios. In this paper, we extend the DenseNet in MAIDRL and introduce semi-centralized Multi-Agent Dense-CNN Reinforcement Learning, MAIDCRL, by incorporating convolutional layers into the deep model architecture, and evaluate the performance on both homogeneous and heterogeneous scenarios. The results show that the CNN-enabled MAIDCRL significantly improved the learning performance and achieved a faster learning rate compared to the existing MAIDRL, especially on more complicated heterogeneous SMAC scenarios. We further investigate the stability and robustness of our model. The statistics reflect that our model not only achieves higher winning rate in all the given scenarios but also boosts the agent’s learning process in fine-grained decision-making.},
  keywords={Decision making;Reinforcement learning;Games;Stability analysis;Robustness;Complexity theory;Behavioral sciences;Deep reinforcement learning;convolutional neural network;multi-agent system;StarCraft II;MAIDRL},
  doi={10.1109/CoG51982.2022.9893711},
  ISSN={2325-4289},
  month={Aug},}@ARTICLE{9745398,
  author={Dudukovich, Rachel and Gormley, Dylan and Kancharla, Shilpa and Wagner, Katherine and Short, Robert and Brooks, David and Fantl, Jason and Janardhanan, Shruti and Fung, Alexander},
  journal={IEEE Journal of Radio Frequency Identification}, 
  title={Toward the Development of a Multi-Agent Cognitive Networking System for the Lunar Environment}, 
  year={2022},
  volume={6},
  number={},
  pages={269-283},
  abstract={This paper details the development of a multi-agent cognitive system intended to optimize networking performance in the lunar environment. One concept of the future of lunar communication, LunaNet, outlines a complex network of networks. Challenges such as scalability, interoperability, and reliability must first be addressed to successfully fulfill this vision. Machine intelligence can greatly reduce the reliance on human operators and enable efficient operations for tasks such as scheduling and network management. Machine learning, artificial intelligence, and other automated decision-making techniques can be used to allow network nodes to intelligently sense and adapt to changes in the environment such as link disruptions, new nodes joining the network, and support for a diverse range of protocols. Cognitive networking seeks to evolve these technologies into an autonomous system with improved science data return, reliability, and scalability. In this paper, we study four main areas as a means to further develop cognitive networking capabilities: networking protocol development, analysis of wireless data for modeling and simulation, development of algorithms for a multi-agent system, and spectrum sensing technology.},
  keywords={Moon;Protocols;Routing;Orbits;NASA;Small satellites;Radiofrequency identification;Cognitive networking;cognitive radio;delay-tolerant networking;multi-agent reinforcement learning;spectrum sensing;cross-layer optimization;link quality},
  doi={10.1109/JRFID.2022.3162952},
  ISSN={2469-7281},
  month={},}@INPROCEEDINGS{9412176,
  author={Leung, Chin-Wing and Hu, Shuyue and Leung, Ho-Fung},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, 
  title={Self-Play or Group Practice: Learning to Play Alternating Markov Game in Multi-Agent System}, 
  year={2021},
  volume={},
  number={},
  pages={9234-9241},
  abstract={The research in reinforcement learning has achieved great success in strategic game playing. These successes are thanks to the incorporation of deep reinforcement learning (DRL) and Monte Carlo Tree Search (MCTS) to the agent trained under the self-play (SP) environment. By self-play, agents are provided with an incrementally more difficult curriculum which in turn facilitates learning. However, recent research suggests that agents trained via self-play may easily lead to getting stuck in local equilibria. In this paper, we consider a population of agents each independently learns to play an alternating Markov game (AMG). We propose a new training framework-group practice- for a population of decentralized RL agents. By group practice (GP), agents are assigned into multiple learning groups during training, for every episode of games, an agent is randomly paired up and practices with another agent in the learning group. The convergence result to the optimal value function and the Nash equilibrium are proved under the GP framework. Experimental study is conducted by applying GP to Q-learning algorithm and the deep Q-learning with Monte-Carlo tree search on the game of Connect Four and the game of Hex. We verify that GP is the more efficient training scheme than SP given the same amount of training. We also show that the learning effectiveness can even be improved when applying local grouping to agents.},
  keywords={Training;Monte Carlo methods;Sociology;Games;Reinforcement learning;Markov processes;Nash equilibrium},
  doi={10.1109/ICPR48806.2021.9412176},
  ISSN={1051-4651},
  month={Jan},}@INPROCEEDINGS{10831294,
  author={Aschu, Demetros and Peter, Robinroy and Karaf, Sausar and Fedoseev, Aleksey and Tsetserukou, Dzmitry},
  booktitle={2024 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={MARLander: A Local Path Planning for Drone Swarms using Multiagent Deep Reinforcement Learning}, 
  year={2024},
  volume={},
  number={},
  pages={2943-2948},
  abstract={Achieving safe and precise landings for a swarm of drones poses a significant challenge, primarily attributed to conventional control and planning methods. This paper presents the implementation of multi-agent deep reinforcement learning (MADRL) techniques for the precise landing of a drone swarm at relocated target locations. The system is trained in a realistic simulated environment with a maximum linear velocity of 3 m/s in training spaces of 4 m3and deployed utilizing Crazyflie drones with a Vicon indoor localization system. The experimental results revealed that the proposed approach achieved a landing accuracy of 2.26 cm on stationary and 3.93 cm on moving platforms surpassing a baseline method used with a Proportional-integral-derivative (PID) controller with an Artificial Potential Field (APF). This research high-lights drone landing technologies that eliminate the need for analytical centralized systems, potentially offering scalability and revolutionizing applications in logistics, safety, and rescue missions.},
  keywords={Training;Location awareness;Scalability;Deep reinforcement learning;Path planning;Safety;Planning;PD control;Drones;Logistics;Swarm of Drones;Multi-agent system;Deep Reinforcement Learning;Collision Avoidances;Planner},
  doi={10.1109/SMC54092.2024.10831294},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10610578,
  author={Chen, Ruiqing and Song, Wenbin and Zu, Weiqin and Dong, ZiXin and Guo, Ze and Sun, Fanglei and Tian, Zheng and Wang, Jun},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={An LLM-driven Framework for Multiple-Vehicle Dispatching and Navigation in Smart City Landscapes}, 
  year={2024},
  volume={},
  number={},
  pages={2147-2153},
  abstract={In the context of smart cities, autonomous vehicles, such as unmanned delivery vehicles and taxis are gradually gaining acceptance. However, their application scenarios remain significantly fragmented. Typically, an Autonomous Multi-Functional Vehicle (AMFV) is not engaged in other scenarios when idle in a specific one. Currently, a unified system capable of coordinating and using these resources efficiently is lacking. Moreover, there is an absence of an advanced navigation algorithm for facilitating coordinated navigation among Heterogeneous Vehicles (HVs). To address these issues, we propose the LLM-driven Multi-vehicle Dispatching and navigation (LiMeda) framework. It comprises an LLM-driven scheduling module that facilitates efficient allocation considering task scenarios and vehicle information, which addresses the issue of incompatible vehicle resources across various smart city scenarios. And the other is a navigation module, founded on the Heterogeneous Agent Reinforcement Learning (HARL) framework we previously proposed, which can effectively perform cooperative navigation tasks among heterogeneous agents, assisting the cooperative task completion by HVs in a smart city. Experimental results show our method outperforms both traditional scheduling algorithms and Reinforcement Learning navigation algorithms in metric terms. Additionally, it shows remarkable scalability and generalization under varying city scales, vehicle numbers, and task numbers.},
  keywords={Navigation;Smart cities;Scheduling algorithms;Scalability;Robot kinematics;Reinforcement learning;Dispatching},
  doi={10.1109/ICRA57147.2024.10610578},
  ISSN={},
  month={May},}@INPROCEEDINGS{9527913,
  author={Bellini, Emanuele and Bagnoli, Franco and Caporuscio, Mauro and Damiani, Ernesto and Flammini, Francesco and Linkov, Igor and Liò, Pietro and Marrone, Stefano},
  booktitle={2021 IEEE International Conference on Cyber Security and Resilience (CSR)}, 
  title={Resilience learning through self adaptation in digital twins of human-cyber-physical systems}, 
  year={2021},
  volume={},
  number={},
  pages={168-173},
  abstract={Human-Cyber-Physical-Systems (HPCS), such as critical infrastructures in modern society, are subject to several systemic threats due to their complex interconnections and interdependencies. Management of systemic threats requires a paradigm shift from static risk assessment to holistic resilience modeling and evaluation using intelligent, data-driven and run-time approaches. In fact, the complexity and criticality of HCPS requires timely decisions considering many parameters and implications, which in turn require the adoption of advanced monitoring frameworks and evaluation tools. In order to tackle such challenge, we introduce those new paradigms in a framework named RESILTRON, envisioning Digital Twins (DT) to support decision making and improve resilience in HCPS under systemic stress. In order to represent possibly complex and heterogeneous HCPS, together with their environment and stressors, we leverage on multi-simulation approaches, combining multiple formalisms, data-driven approaches and Artificial Intelligence (AI) modelling paradigms, through a structured, modular and compositional framework. DT are used to provide an adaptive abstract representation of the system in terms of multi-layered spatially-embedded dynamic networks, and to apply self-adaptation to time-warped What-If analyses, in order to find the best sequence of decisions to ensure resilience under uncertainty and continuous HPCS evolution.},
  keywords={Adaptation models;Uncertainty;Digital twin;Decision making;Tools;Time factors;Artificial intelligence},
  doi={10.1109/CSR51186.2021.9527913},
  ISSN={},
  month={July},}@INPROCEEDINGS{9298050,
  author={Perin, Giovanni and Nophut, David and Badia, Leonardo and Fitzek, Frank H.P.},
  booktitle={2020 11th IEEE Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)}, 
  title={Maximizing Airtime Efficiency for Reliable Broadcast Streams in WMNs with Multi-Armed Bandits}, 
  year={2020},
  volume={},
  number={},
  pages={0472-0478},
  abstract={Wireless broadcast routing is a complex problem, shown in the literature to be NP-complete. Current protocols implement either heuristics to find solutions that are not guaranteed to be optimal or classic flooding. However, many future use cases, like automotive applications, industrial robotics, and multimedia broadcast, will require efficient yet reliable methods. In this work, we use contextual multi-armed bandits together with opportunistic routing (OR) and network coding (NC) to find approximately optimal solutions to the problem of broadcast routing in a distributed fashion. Each router independently learns its own transmission credit, i.e., the number of packets to forward for each innovative packet received, so that the airtime cost, subject to latency constraints, is minimized. Results show that the proposed solutions, particularly the deep learning based one, vastly improve the overall reliability, while performing close to MORE multicast in terms of airtime and to B.A.T.M.A.N. in latency, both being the best candidates in the respective discipline among the tested ones.},
  keywords={Routing;Unicast;Reliability;Optimization;Protocols;Proposals;Artificial neural networks;Reinforcement learning;multi-armed bandits;wireless mesh networks;routing;broadcast},
  doi={10.1109/UEMCON51285.2020.9298050},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10865466,
  author={Wu, Hongqian and Deng, Hongzhong and Li, Jichao},
  booktitle={2024 China Automation Congress (CAC)}, 
  title={Methods for Collaborative Combat Capabilities Analysis: An Overview}, 
  year={2024},
  volume={},
  number={},
  pages={3866-3872},
  abstract={Collaborative combat is the foundational organizational form of intelligent, informatized, and systematic combat. With the rapid development of unmanned combat system, collaborative combat becomes the trend of future combat forms. Based on the network information sharing, the key elements to decide the victory or defeat of war are no longer limited to the advanced level of individual platforms, but also depends on the collaboration level among the components of the system. Thus, analyzing the system's collaborative capabilities is a core aspect of current combat assessment and optimization. Owing to the fog of war, the analysis of collaborative combat capabilities is not only limited by the acquisition of data and the complexity of the combat system, but is also influenced by the heterogeneity, hierarchy, coupling, dynamics, and multi-domain characteristics of the collaborative relationships between equipment, making it difficult to obtain an analytical method that considers various dimensions. We comb through the main research methods used for collaborative combat capability analysis, categorizing the research methods into four perspectives: analysis based on network structure, modeling and simulation, capability aggregation, and hybrid approaches. Furthermore, we provide a forward-looking perspective on the future trajectory of collaborative combat capability analysis. This overview can serve as a theoretical and technical foundation for research into countermeasures for systemic confrontation.},
  keywords={Couplings;Analytical models;Systematics;Automation;Collaboration;Information sharing;Market research;Trajectory;Complexity theory;Optimization;collaborative combat;capability analysis;capability aggregation;network structure;modeling & simulation},
  doi={10.1109/CAC63892.2024.10865466},
  ISSN={2688-0938},
  month={Nov},}@INPROCEEDINGS{10295504,
  author={Jagadish, Lakshya and Sarma, Banashree and Manivasakan, R.},
  booktitle={2023 European Conference on Communication Systems (ECCS)}, 
  title={Multi Agent DeepRL Based Joint Power and Subchannel Allocation in IAB networks}, 
  year={2023},
  volume={},
  number={},
  pages={55-61},
  abstract={Integrated Access and Backhauling (IAB) is a viable approach for meeting the unprecedented need for higher data rates of future generations, acting as a cost-effective alternative to dense fiber-wired links. The design of such networks with constraints usually results in an optimization problem of non-convex and combinatorial nature. Under those situations, it is challenging to obtain an optimal strategy for the joint Subchannel Allocation and Power Allocation (SAPA) problem. In this paper, we develop a multi-agent Deep Reinforcement Learning (DeepRL) based framework for joint optimization of power and subchannel allocation in an IAB network to maximize the downlink data rate. SAPA using DDQN (Double Deep Q-Learning Network) can handle computationally expensive problems with huge action spaces associated with multiple users and nodes. Unlike the conventional methods such as game theory, fractional programming, and convex optimization, which in practice demand more and more accurate network information, the multi-agent DeepRL approach requires less environment network information. Simulation results show the proposed scheme's promising performance when compared with baseline (Deep Q-Learning Network and Random) schemes.},
  keywords={Training;Q-learning;Simulation;Programming;Downlink;Optical fiber theory;Numerical models;Integrated Access and Backhaul;Deep Reinforcement Learning;Power Allocation;Subchannel Allocation},
  doi={10.1109/ECCS58882.2023.00019},
  ISSN={},
  month={May},}@INPROCEEDINGS{10349880,
  author={Chandrasekaran, Geetha and De Veciana, Gustavo},
  booktitle={2023 21st International Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks (WiOpt)}, 
  title={Distributed Reinforcement Learning Based Delay Sensitive Decentralized Resource Scheduling}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={We address the problem of distributed resource allocation in wireless systems in the presence of dynamic user traffic and coupling resulting from interference. We propose a Reinforcement Learning (RL) framework based on a separation of concerns between frequency reuse for interference mitigation and opportunistic user scheduling. In particular we explore a setting where a stochastic game is set up among base stations to learn frequency reuse patterns and solved using multi-agent RL given an underlying choice for user scheduling. We establish the existence and convergence to a Nash equilibrium of the proposed setting. The performance of our framework and theoretical findings are evaluated through simulation and compared to more aggressive oracle-aided centralized baselines. The resulting frequency reuse policy is shown to achieve 5–25% improvements in capacity and associated delay performance over a centralized interference aware max weight scheduling policy across BSs. Furthermore, a reduced physical resource utilization on the order of 9–34% leads to a higher energy efficiency as compared to the centralized benchmark.},
  keywords={Couplings;Training;Base stations;Heuristic algorithms;Wireless networks;Interference;Reinforcement learning},
  doi={10.23919/WiOpt58741.2023.10349880},
  ISSN={2690-3342},
  month={Aug},}@ARTICLE{10820999,
  author={Shi, Juan and Chu, Chen and Fan, Guoxi and Hu, Die and Liu, Jinzhuo and Wang, Zhen and Hu, Shuyue},
  journal={IEEE Transactions on Cybernetics}, 
  title={Payoff Control in Multichannel Games: Influencing Opponent Learning Evolution}, 
  year={2025},
  volume={55},
  number={2},
  pages={776-785},
  abstract={In this article, we introduce a new theory for payoff control in multichannel learning environments, where agents interact with each other over multiple channels and each channel is a repeated normal form game. We propose two payoff control strategies—partial control and full control—that allow a single agent to set an upper bound to the opponent’s expected payoffs summed across all channels, even if the opponent is a reinforcement learning agent. We prove that a partial (or full) control strategy can be obtained by solving a system of inequalities, and characterize the conditions under which such a partial (or full) control strategy exists. We show that by utilizing these control strategies, the agent can influence the opponent’s learning evolution and direct it toward a desired viable equilibrium. Our experiments confirm the effectiveness of our theory for payoff control in a wide range of multichannel learning environments.},
  keywords={Games;Upper bound;Thin film transistors;Q-learning;Game theory;Fans;Cybernetics;Computer security;Technological innovation;Software;Multichannel games;payoff control;reinforcement learning},
  doi={10.1109/TCYB.2024.3507830},
  ISSN={2168-2275},
  month={Feb},}@INPROCEEDINGS{10711748,
  author={Menegatti, Danilo and Wrona, Andrea and Di Paola, Antonio and Gentile, Simone and Giuseppi, Alessandro},
  booktitle={2024 IEEE 20th International Conference on Automation Science and Engineering (CASE)}, 
  title={Deep Reinforcement Learning Platooning Control of Non-Cooperative Autonomous Vehicles in a Mixed Traffic Environment}, 
  year={2024},
  volume={},
  number={},
  pages={108-113},
  abstract={Ensuring secure spacing between vehicles is vital for road safety, efficient traffic flow, and system stability in autonomous driving. While traditional cooperative platooning approach, relying on centralized coordination exploiting wireless network, faces practical implementation challenges due to communication constraints and diverse driving behaviors, this work introduces a scalable non-cooperative multi-agent platooning strategy based on Deep Reinforcement Learning, leveraging on decentralized decision-making principles. The agents’ aim is to adjust their velocities dynamically to ensure safe following distances and adapt to surrounding vehicle behavior, without the possibility of exchanging information over a wireless network. Extensive simulations validate the effectiveness and robustness of the proposed approach, making it suitable for real-world autonomous driving scenarios.},
  keywords={Wireless networks;Deep reinforcement learning;Control systems;Aerodynamics;Robustness;Stability analysis;Sensors;Vehicle dynamics;Autonomous vehicles;Optimization;Autonomous Vehicles;Multi-Agent Systems;Platooning Control;Non-Cooperative Control;Deep Reinforcement Learning;Decentralized Decision-Making;Traffic Flow Optimization},
  doi={10.1109/CASE59546.2024.10711748},
  ISSN={2161-8089},
  month={Aug},}@INPROCEEDINGS{10684921,
  author={Fan, Xiangrui and Ren, Yuan and Lin, Jinyong},
  booktitle={2024 12th International Conference on Intelligent Computing and Wireless Optical Communications (ICWOC)}, 
  title={Design of Network Topology Control Algorithm for Multi-agent Systems}, 
  year={2024},
  volume={},
  number={},
  pages={93-96},
  abstract={In order to meet the requirements of efficient and reliable network communication in cooperative multi-agent systems, this paper discusses the multi-agent autonomous network topology control algorithm which can adapt to the dynamic changing environment and task. It optimizes the system organizational structure by updating the cluster topology on demand to meet the task requirements. The research focuses on intelligent network topology control method based on power control, channel switching and routing optimization, so as to overcome the influence of complex environment interference, intragroup interference and adapt to changing tasks. The main theoretical achievements are: (1) Topology control algorithm based on power control; (2) Topology control algorithm based on channel switching; (3) Topology control algorithm based on routing optimization. The above research explores new methods and ideas for reliable networking of multi-agent systems, and support the diverse cooperative tasks of multi-agent systems.},
  keywords={Intelligent networks;Network topology;Optical switches;Power control;Interference;Control systems;Routing;multi-agent system;network topology control;power control;routing optimization;reinforcement learning},
  doi={10.1109/ICWOC62055.2024.10684921},
  ISSN={},
  month={June},}@ARTICLE{9740162,
  author={Shi, Yiming and Rong, Zhihai},
  journal={IEEE Transactions on Circuits and Systems II: Express Briefs}, 
  title={Analysis of Q-Learning Like Algorithms Through Evolutionary Game Dynamics}, 
  year={2022},
  volume={69},
  number={5},
  pages={2463-2467},
  abstract={Based on two-player two-action and three-action game models, this brief studies the dynamics of Q-learning and Frequency Adjusted Q-(FAQ-) learning algorithms in multi-agent systems, and discloses the underlying mechanisms of these algorithms through the perspective of evolutionary dynamics. It is showed that the dynamics of FAQ-learning or Q-learning with Boltzmann exploration mechanism corresponds to the evolutionary dynamics of selection mechanism with the linear or super-exponential growth, respectively. Hence, FAQ-learning algorithm can converge to the equilibrium state of a game model, whereas, the convergence of Q-learning algorithm is related with the initial states of the population. Therefore, the continuous evolutionary dynamics with selection mechanism can predict the learning process of discrete Q-learning like algorithms well.},
  keywords={Games;Heuristic algorithms;Q-learning;Statistics;Sociology;Prediction algorithms;Game theory;Game theory;evolutionary dynamics;multi-agent system;learning dynamics;Q-learning},
  doi={10.1109/TCSII.2022.3161655},
  ISSN={1558-3791},
  month={May},}@INPROCEEDINGS{9613839,
  author={Dudukovich, Rachel and Wagner, Katherine and Kancharla, Shilpa and Fantl, Jason and Fung, Alex},
  booktitle={2021 IEEE International Conference on Wireless for Space and Extreme Environments (WiSEE)}, 
  title={Towards the Development of a Multi-Agent Cognitive Networking System for the Lunar Environment}, 
  year={2021},
  volume={},
  number={},
  pages={7-13},
  abstract={This paper details the development of a multi-agent cognitive system intended to optimize networking performance in the lunar environment. NASA’s current concept of the future of lunar communication, LunaNet [1], outlines a complex network of networks. Challenges such as scalability, interoperability and reliability must first be addressed to successfully fulfill this vision. Machine intelligence can greatly reduce the reliance on human operators and enable efficient operations for tasks such as scheduling and network management. The application of machine learning, artificial intelligence, and other automated decision-making techniques can be used to allow network nodes to intelligently sense and adapt to changes in the environment such as link disruptions, new nodes joining the network, and support for a diverse range of protocols. Cognitive networking seeks to evolve these technologies into an autonomous system with improved science data return, reliability, and scalability. In this paper, we study three main areas a means to further develop cognitive networking capabilities: networking and flight software development, analysis of wireless data for modeling and simulation, and development of algorithms for a multi-agent system.},
  keywords={Wireless communication;Wireless sensor networks;Analytical models;Scalability;Moon;Software algorithms;Data models;Cognitive Networking;Delay Tolerant Networking;Multi-Agent Reinforcement Learning},
  doi={10.1109/WiSEE50203.2021.9613839},
  ISSN={2380-7636},
  month={Oct},}@ARTICLE{9994770,
  author={Wang, Yuhang and He, Ying and Yu, F. Richard and Lin, Qiuzhen and Leung, Victor C. M.},
  journal={IEEE Transactions on Wireless Communications}, 
  title={Efficient Resource Allocation in Multi-UAV Assisted Vehicular Networks With Security Constraint and Attention Mechanism}, 
  year={2023},
  volume={22},
  number={7},
  pages={4802-4813},
  abstract={With the rapid development of intelligent transportation systems, there is an increasingly strong demand for low-latency and high-bandwidth vehicular services. Unmanned aerial vehicles (UAVs) can be used as a supplement to the ground networks, to relieve the communication pressure on ground facilities, such as base stations. In this paper, we use multiple UAVs to provide services for vehicles and model the multi-UAV scenario as a collaborative multi-agent system. In addition, we take vehicle safety as the top priority and the delay requirement as the constraints. Then we exploit the Lagrange multiplier to combine the constraint function and cost function, so as to reduce the resource consumption as much as possible on the premise of ensuring the safety of the vehicles. The influence of spectrum efficiency and computing power should also be taken into account when allocating resources. We adopt the multi-agent reinforcement learning to train the UAVs, and meanwhile introduce the attention mechanism so that each UAV can optimize itself better with the information of other UAVs. Through extensive simulations, the effectiveness of our proposed method is verified. Particularly, the limited resources can allocated efficiently according to the vehicle’s needs under the premise of ensuring vehicle safety.},
  keywords={Reinforcement learning;Safety;Autonomous aerial vehicles;Wireless communication;Delays;Resource management;Task analysis;Multi-UAV;security constraints;multi-agent reinforcement learning;resources allocation;attention mechanism},
  doi={10.1109/TWC.2022.3229013},
  ISSN={1558-2248},
  month={July},}@ARTICLE{10549978,
  author={Sun, Youbang and Liu, Tao and Kumar, P. R. and Shahrampour, Shahin},
  journal={IEEE Control Systems Letters}, 
  title={Linear Convergence of Independent Natural Policy Gradient in Games With Entropy Regularization}, 
  year={2024},
  volume={8},
  number={},
  pages={1217-1222},
  abstract={This letter focuses on the entropy-regularized independent natural policy gradient (NPG) algorithm in multi-agent reinforcement learning. In this letter, agents are assumed to have access to an oracle with exact policy evaluation and seek to maximize their respective independent rewards. Each individual’s reward is assumed to depend on the actions of all agents in the multi-agent system, leading to a game between agents. All agents make decisions under a policy with bounded rationality, which is enforced by the introduction of entropy regularization. In practice, a smaller regularization implies that agents are more rational and behave closer to Nash policies. On the other hand, with larger regularization agents tend to act randomly, which ensures more exploration. We show that, under sufficient entropy regularization, the dynamics of this system converge at a linear rate to the quantal response equilibrium (QRE). Although regularization assumptions prevent the QRE from approximating a Nash equilibrium (NE), our findings apply to a wide range of games, including cooperative, potential, and two-player matrix games. We also provide extensive empirical results on multiple games (including Markov games) as a verification of our theoretical analysis.},
  keywords={Games;Entropy;Convergence;Nash equilibrium;Reinforcement learning;Gradient methods;Approximation algorithms;Game theory;multi-agent reinforcement learning;natural policy gradient;quantal response equilibrium},
  doi={10.1109/LCSYS.2024.3410149},
  ISSN={2475-1456},
  month={},}@INPROCEEDINGS{9651916,
  author={Wang, Yuhang and He, Ying and Dong, Minhui},
  booktitle={2021 IEEE 29th International Conference on Network Protocols (ICNP)}, 
  title={Resource Allocation in Vehicular Networks with Multi-UAV Served Edge Computing}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={With the rapid development of intelligent transportation systems, there is an increasingly strong demand for low-latency and high-bandwidth vehicular services, such as automatic driving assistance, emergency alarm, and infotainment. However, in some cases (e.g., traffic congestion, remote areas), the ground communication networks alone cannot meet the vast needs of vehicles. Unmanned aerial vehicles (UAVs) are flexible and deployable, which can be used as a supplement to the ground networks, to relieve the communication pressure on ground facilities, such as base stations. In this paper, we use multiple UAVs to provide services for vehicles and model the multi-UAV scenario as a collaborative multi-agent system. All UAVs share limited bandwidth resources and equip with edge computing servers to serve the vehicles. In addition, serious consequences may be caused if the delay requirements of vehicles are not satisfied. Therefore, we take vehicle safety as the top priority and the delay requirement as the constraints. Then we exploit the Lagrange multiplier to combine the constraint function and cost function, so as to reduce the resource consumption as much as possible on the premise of ensuring the safety of the vehicles. The influence of channel efficiency and computing power should also be taken into account when allocating resources. We adopt the multi-agent reinforcement learning to train the UAVs, and meanwhile introduce the attention mechanism so that each UAV can optimize itself better with the information of other UAVs. Through a large number of experiments, the effectiveness of our proposed method is verified. Particularly, in the case of strictly limiting bandwidth resources, resources can still be allocated according to vehicle needs under the premise of ensuring vehicle safety.},
  keywords={Base stations;Computational modeling;Bandwidth;Reinforcement learning;Autonomous aerial vehicles;Safety;Delays;multi-UAV;security constraints;multi-agent reinforcement learning;resources allocation;attention mechanism},
  doi={10.1109/ICNP52444.2021.9651916},
  ISSN={2643-3303},
  month={Nov},}@INPROCEEDINGS{8929148,
  author={Wang, Chang and Chen, Hao and Yan, Chao and Xiang, Xiaojia},
  booktitle={2019 IEEE International Conference on Agents (ICA)}, 
  title={Reinforcement Learning with an Extended Classifier System in Zero-sum Markov Games}, 
  year={2019},
  volume={},
  number={},
  pages={44-49},
  abstract={A reinforcement learning (RL) agent can learn how to win against an opponent agent in zero-sum Markov Games after episodes of training. However, it is still challenging for the RL agent to acquire the optimal policy if the opponent agent is also able to learn concurrently. In this paper, we propose a new RL algorithm based on the eXtended Classifier System (XCS) that maintains a population of competing rules for action selection and uses the genetic algorithm (GA) to evolve the rules for searching the optimal policy. The RL agent can learn from scratch by observing the behaviors of the opponent agent without making any assumptions about the policy of the RL agent or the opponent agent. In addition, we use eligibility trace to further speed up the learning process. We demonstrate the performance of the proposed algorithm by comparing it with several benchmark algorithms in an adversarial soccer game against the same deterministic policy learner.},
  keywords={Games;Markov processes;Genetic algorithms;Task analysis;Prediction algorithms;Learning (artificial intelligence);Game theory;reinforcement learning;learning classifier system;multi-agent system;markov games;eligibility trace},
  doi={10.1109/AGENTS.2019.8929148},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10293832,
  author={Iturria-Rivera, Pedro Enrique and Chenier, Marcel and Herscovici, Bernard and Kantarci, Burak and Erol-Kantarci, Melike},
  booktitle={2023 IEEE 34th Annual International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC)}, 
  title={Channel Selection for Wi-Fi 7 Multi-Link Operation via Optimistic-Weighted VDN and Parallel Transfer Reinforcement Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Dense and unplanned IEEE 802.11 Wireless Fidelity (Wi-Fi) deployments and the continuous increase of throughput and latency stringent services for users have led to machine learning algorithms to be considered as promising techniques in the industry and the academia. Specifically, the ongoing IEEE 802.11be EHT —Extremely High Throughput, known as Wi-Fi 7— amendment propose, for the first time, Multi-Link Operation (MLO). Among others, this new feature will increase the complexity of channel selection due the novel multiple interfaces proposal. In this paper, we present a Parallel Transfer Reinforcement Learning (PTRL)-based cooperative Multi-Agent Reinforcement Learning (MARL) algorithm named Parallel Transfer Reinforcement Learning Optimistic-Weighted Value Decomposition Networks (oVDN) to improve intelligent channel selection in IEEE 802.11be MLO-capable networks. Additionally, we compare the impact of different parallel transfer learning alternatives and a centralized non-transfer MARL baseline. Two PTRL methods are presented: Multi-Agent System (MAS) Joint Q-function Transfer, where the joint Q-function is transferred and MAS Best/Worst Experience Transfer where the best and worst experiences are transferred among MASs. Simulation results show that oVDNg–only the best experiences are utilized– is the best algorithm variant. Moreover, oVDNg offers a gain up to 3%, 7.2% and 11% when compared with VDN, VDN-nonQ and non-PTRL baselines. Furthermore, oVDNg experienced a reward convergence gain in the 5 GHz interface of 33.3% over oVDNb and oVDN where only worst and both types of experiences are considered, respectively. Finally, our best PTRL alternative showed an improvement over the non-PTRL baseline in terms of speed of convergence up to 40 episodes and reward up to 135%.},
  keywords={Measurement;Simulation;Transfer learning;Modulation;Reinforcement learning;Throughput;Proposals;Multi-Link Operation;Channel Selection;Reinforcement Learning;Wi-Fi 7;VDN},
  doi={10.1109/PIMRC56721.2023.10293832},
  ISSN={2166-9589},
  month={Sep.},}@ARTICLE{10547029,
  author={Lan, Xi and Qiao, Yuansong and Lee, Brian},
  journal={IEEE Access}, 
  title={Multiagent Hierarchical Reinforcement Learning With Asynchronous Termination Applied to Robotic Pick and Place}, 
  year={2024},
  volume={12},
  number={},
  pages={78988-79002},
  abstract={Recent breakthroughs in hierarchical multi-agent deep reinforcement learning (HMADRL) are propelling the development of sophisticated multi-robot systems, particularly in the realm of complex coordination tasks. These advancements hold significant potential for addressing the intricate challenges inherent in fast-evolving sectors such as intelligent manufacturing. In this study, we introduce an innovative simulator tailored for a multi-robot pick-and-place (PnP) operation, built upon the OpenAI Gym framework. Our aim is to demonstrate the efficacy of HMADRL algorithms for multi robot coordination in a manufacturing setting, concentrating on their influence on the gripping rate, a crucial indicator for gauging system performance and operational efficiency.},
  keywords={Robots;Robot kinematics;Task analysis;Training;Multi-robot systems;Productivity;Navigation;Multi-agent system;pick and place;multi-agent-hierarchical reinforcement learning;multi-robot system;asynchronous termination},
  doi={10.1109/ACCESS.2024.3409076},
  ISSN={2169-3536},
  month={},}@ARTICLE{11048550,
  author={Stai, Eleni and Gotzias, Georgios and Papadostefanaki, Marianna and Papavassiliou, Symeon},
  journal={IEEE Networking Letters}, 
  title={Distributed Constrained Real-Time Control of Multiple Batteries in Green Networks}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Energy sustainability is endeavored in all networked systems and renewable energy sources (RESs) will play a fundamental role towards its achievement. Due to their uncertainties, RESs integration for powering any type of network requires support for handling energy surplus or shortage due to e.g., unexpected weather conditions or generation-demand lags. Such support can be offered by batteries that have become crucial for a large scale integration of RESs to networked systems and when multiple batteries serve one network, their coordinated control will allow for maximizing the benefits of their operation. In this paper, we propose a real-time control scheme for multiple batteries based on a multi-agent reinforcement learning scheme that is distributed in execution but centralized in training so as to achieve a high level of coordination but in a scalable way. Most importantly, it is safe in the sense that it accounts for binding lookahead constraints on the batteries state-of-energy, which express long-term feasibility and cost efficiency goals in the network operation. We have performed a case study in smart grids with the network goal of following a dispatch plan in real-time. Numerical results showcase the effectiveness of the proposed scheme both in terms of cost performance and of constraints violation avoidance versus baseline approaches.},
  keywords={Batteries;Real-time systems;Costs;Training;Smart grids;Toy manufacturing industry;Sustainable development;Uncertainty;Renewable energy sources;Mathematical models},
  doi={10.1109/LNET.2025.3582414},
  ISSN={2576-3156},
  month={},}@INPROCEEDINGS{9411153,
  author={Omi, Saki and Shin, Hyo-Sang and Tsourdos, Antonios and Espeland, Joakim and Buchi, Andrian},
  booktitle={2021 15th European Conference on Antennas and Propagation (EuCAP)}, 
  title={Introduction to UAV swarm utilization for communication on the move terminals tracking evaluation with reinforcement learning technique}, 
  year={2021},
  volume={},
  number={},
  pages={1-5},
  abstract={As the growth of communication and satellite industry, the demand of satellite antenna evaluation is increasing. Particularly Communication On The Move (COTM) terminal antenna, including electronically steerable antennas (ESA) and for the communication between new constellations on LEO and MEO, requires tracking accuracy test for the communication on moving vehicles. The measurement capability of conventional methodologies have been limited due to their location fixed facilities and non-adjustable sensor’s positions during the measurement. To overcome this drawbacks, we will present how multi-agent system of UAVs could be utilized for COTM tracking accuracy evaluation. This measurement needs instant actions for UAVs to keep them navigating in order to achieve accurate and stable measurement. Reinforcement learning (RL) techniques are investigated for this purpose in this paper. The performance improvement is demonstrated with the system using RL technique to adjust UAVs with sensors during the measurement.},
  keywords={Antenna measurements;Training;Satellite antennas;Satellites;Tracking;Navigation;Reinforcement learning;Communication On The Move;de-pointing;antenna measurements;UAV;multi-agent reinforcement learning},
  doi={10.23919/EuCAP51087.2021.9411153},
  ISSN={},
  month={March},}
